{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, StackingClassifier, VotingClassifier, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: \n",
    "#### Aim 1: Validating the original classifier - Elastic Net\n",
    "#### Aim 2: Training new classifiers\n",
    "\n",
    "### For Aim 2:\n",
    "### Preprocessing:\n",
    "#### - Missing values in both training and testing datasets are replaced by mean; \n",
    "#### - Invalid samples and family member samples are removed\n",
    "#### - Merge 4 datasets as training and the left 1 as testing (repeated 5 times)\n",
    "### Data Engineering:\n",
    "#### - Variable selection method: Logistic Regression / Random Forest;\n",
    "### Models:\n",
    "#### Stacking: SVM, LR, RF, GB, MNB\n",
    "#### Voting: SVM, LR, RF, GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Datasets\n",
    "\n",
    "# 1658 * 834\n",
    "data_ERISK = pd.read_csv('ERisk_data.csv')\n",
    "# 1464 * 834\n",
    "data_ERISK = data_ERISK.dropna()\n",
    "\n",
    "# 614 rows × 834 columns\n",
    "data_BSGS = pd.read_csv('BSGS_data.csv')\n",
    "# 358 rows × 834 columns\n",
    "data_BSGS = data_BSGS[data_BSGS['label']!='0']\n",
    "data_BSGS['label'] = data_BSGS['label'].replace(['MZ','DZ'],[1,0])\n",
    "data_BSGS = data_BSGS.fillna(data_BSGS.mean())\n",
    "\n",
    "# 180 * 834\n",
    "data_DENMARK = pd.read_csv('DENMARK_data.csv')\n",
    "data_DENMARK['label'] = data_DENMARK['label'].replace([2],[0])\n",
    "\n",
    "# 479 * 832\n",
    "data_AMDTSS = pd.read_csv('AMDTSS_data.csv')\n",
    "# 264 * 832 - removing family members\n",
    "data_AMDTSS = data_AMDTSS[data_AMDTSS['label']!='Sister']\n",
    "data_AMDTSS['label'] = data_AMDTSS['label'].replace(['MZ','DZ'],[1,0])\n",
    "\n",
    "# 648 * 834\n",
    "data_EMTAB = pd.read_csv('EMTAB_data.csv')\n",
    "data_EMTAB['label'] = data_EMTAB['label'].replace(['dizygotic', 'monozygotic'],[0, 1])\n",
    "data_EMTAB = data_EMTAB.fillna(data_EMTAB.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a stacking ensemble of models\n",
    "def get_stacking():\n",
    "\t# define the base models\n",
    "\tlevel0 = list()\n",
    "\tlevel0.append(('lr', LogisticRegression(tol = 0.001, solver = 'sag', penalty = 'l2', C = 30)))\n",
    "\tlevel0.append(('rf', RandomForestClassifier(n_estimators = 500, min_impurity_decrease = 1e-06, max_depth = 50, criterion = 'gini')))\n",
    "\tlevel0.append(('gb', GradientBoostingClassifier(n_estimators = 300, max_depth = 5, learning_rate = 0.5)))\n",
    "\tlevel0.append(('svm', SVC(kernel = 'rbf', gamma = 'scale', degree = 1, decision_function_shape = 'ovr', C = 20)))\n",
    "\tlevel0.append(('mnb', GaussianNB()))\n",
    "\t# define meta learner model\n",
    "\tlevel1 = LogisticRegression(tol = 0.001, solver = 'sag', penalty = 'l2', C = 30)\n",
    "\t# define the stacking ensemble\n",
    "\tmodel = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training: E-Risk, BSGS, Denmark, AMDTSS\n",
    "# Testing: E-MTAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cg22695986</th>\n",
       "      <th>cg01193368</th>\n",
       "      <th>cg22056094</th>\n",
       "      <th>cg06098368</th>\n",
       "      <th>cg08690094</th>\n",
       "      <th>cg11236452</th>\n",
       "      <th>cg26916862</th>\n",
       "      <th>cg03124146</th>\n",
       "      <th>cg14770527</th>\n",
       "      <th>...</th>\n",
       "      <th>cg11174855</th>\n",
       "      <th>cg04524933</th>\n",
       "      <th>cg19418458</th>\n",
       "      <th>cg21808635</th>\n",
       "      <th>cg11359720</th>\n",
       "      <th>cg07635017</th>\n",
       "      <th>cg08641118</th>\n",
       "      <th>cg09166085</th>\n",
       "      <th>cg22034735</th>\n",
       "      <th>cg17805624</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.233353</td>\n",
       "      <td>0.606732</td>\n",
       "      <td>0.730107</td>\n",
       "      <td>0.240143</td>\n",
       "      <td>0.561082</td>\n",
       "      <td>0.533287</td>\n",
       "      <td>0.194961</td>\n",
       "      <td>0.144218</td>\n",
       "      <td>0.597436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327951</td>\n",
       "      <td>0.919347</td>\n",
       "      <td>0.365084</td>\n",
       "      <td>0.780742</td>\n",
       "      <td>0.731394</td>\n",
       "      <td>0.532118</td>\n",
       "      <td>0.081238</td>\n",
       "      <td>0.108676</td>\n",
       "      <td>0.508394</td>\n",
       "      <td>0.279428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.284813</td>\n",
       "      <td>0.599726</td>\n",
       "      <td>0.715363</td>\n",
       "      <td>0.242588</td>\n",
       "      <td>0.564277</td>\n",
       "      <td>0.578224</td>\n",
       "      <td>0.192382</td>\n",
       "      <td>0.139766</td>\n",
       "      <td>0.570267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195741</td>\n",
       "      <td>0.945236</td>\n",
       "      <td>0.337817</td>\n",
       "      <td>0.696424</td>\n",
       "      <td>0.745175</td>\n",
       "      <td>0.546033</td>\n",
       "      <td>0.099777</td>\n",
       "      <td>0.128168</td>\n",
       "      <td>0.498426</td>\n",
       "      <td>0.360946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.206618</td>\n",
       "      <td>0.552816</td>\n",
       "      <td>0.572559</td>\n",
       "      <td>0.169127</td>\n",
       "      <td>0.541453</td>\n",
       "      <td>0.509944</td>\n",
       "      <td>0.197505</td>\n",
       "      <td>0.193932</td>\n",
       "      <td>0.510173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083195</td>\n",
       "      <td>0.906838</td>\n",
       "      <td>0.357911</td>\n",
       "      <td>0.606163</td>\n",
       "      <td>0.773520</td>\n",
       "      <td>0.536528</td>\n",
       "      <td>0.079384</td>\n",
       "      <td>0.091236</td>\n",
       "      <td>0.204013</td>\n",
       "      <td>0.296357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.203151</td>\n",
       "      <td>0.655871</td>\n",
       "      <td>0.391728</td>\n",
       "      <td>0.224729</td>\n",
       "      <td>0.480992</td>\n",
       "      <td>0.421599</td>\n",
       "      <td>0.178132</td>\n",
       "      <td>0.181609</td>\n",
       "      <td>0.481978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134511</td>\n",
       "      <td>0.956986</td>\n",
       "      <td>0.583261</td>\n",
       "      <td>0.680310</td>\n",
       "      <td>0.753129</td>\n",
       "      <td>0.557829</td>\n",
       "      <td>0.067174</td>\n",
       "      <td>0.158550</td>\n",
       "      <td>0.297127</td>\n",
       "      <td>0.292479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.266709</td>\n",
       "      <td>0.493554</td>\n",
       "      <td>0.395203</td>\n",
       "      <td>0.231550</td>\n",
       "      <td>0.474545</td>\n",
       "      <td>0.381759</td>\n",
       "      <td>0.194651</td>\n",
       "      <td>0.170328</td>\n",
       "      <td>0.549200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069399</td>\n",
       "      <td>0.921778</td>\n",
       "      <td>0.333275</td>\n",
       "      <td>0.722998</td>\n",
       "      <td>0.824286</td>\n",
       "      <td>0.546425</td>\n",
       "      <td>0.075612</td>\n",
       "      <td>0.178922</td>\n",
       "      <td>0.526409</td>\n",
       "      <td>0.278850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.233895</td>\n",
       "      <td>0.732645</td>\n",
       "      <td>0.750664</td>\n",
       "      <td>0.149491</td>\n",
       "      <td>0.524413</td>\n",
       "      <td>0.618680</td>\n",
       "      <td>0.234315</td>\n",
       "      <td>0.236759</td>\n",
       "      <td>0.890437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174961</td>\n",
       "      <td>0.906643</td>\n",
       "      <td>0.581697</td>\n",
       "      <td>0.863217</td>\n",
       "      <td>0.798059</td>\n",
       "      <td>0.723334</td>\n",
       "      <td>0.130358</td>\n",
       "      <td>0.177975</td>\n",
       "      <td>0.821848</td>\n",
       "      <td>0.391814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.247867</td>\n",
       "      <td>0.744726</td>\n",
       "      <td>0.477351</td>\n",
       "      <td>0.211166</td>\n",
       "      <td>0.500541</td>\n",
       "      <td>0.604047</td>\n",
       "      <td>0.259998</td>\n",
       "      <td>0.343025</td>\n",
       "      <td>0.705436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165785</td>\n",
       "      <td>0.913656</td>\n",
       "      <td>0.615752</td>\n",
       "      <td>0.832152</td>\n",
       "      <td>0.712137</td>\n",
       "      <td>0.582116</td>\n",
       "      <td>0.300884</td>\n",
       "      <td>0.234928</td>\n",
       "      <td>0.620441</td>\n",
       "      <td>0.187204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.247103</td>\n",
       "      <td>0.768668</td>\n",
       "      <td>0.489964</td>\n",
       "      <td>0.235903</td>\n",
       "      <td>0.515031</td>\n",
       "      <td>0.663927</td>\n",
       "      <td>0.295938</td>\n",
       "      <td>0.331253</td>\n",
       "      <td>0.679972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180183</td>\n",
       "      <td>0.936972</td>\n",
       "      <td>0.580143</td>\n",
       "      <td>0.852484</td>\n",
       "      <td>0.725882</td>\n",
       "      <td>0.614776</td>\n",
       "      <td>0.291890</td>\n",
       "      <td>0.213946</td>\n",
       "      <td>0.573433</td>\n",
       "      <td>0.174113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.525080</td>\n",
       "      <td>0.812421</td>\n",
       "      <td>0.533161</td>\n",
       "      <td>0.398285</td>\n",
       "      <td>0.603025</td>\n",
       "      <td>0.545732</td>\n",
       "      <td>0.494836</td>\n",
       "      <td>0.403702</td>\n",
       "      <td>0.749127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182134</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.435156</td>\n",
       "      <td>0.783916</td>\n",
       "      <td>0.682231</td>\n",
       "      <td>0.671497</td>\n",
       "      <td>0.108399</td>\n",
       "      <td>0.149804</td>\n",
       "      <td>0.330827</td>\n",
       "      <td>0.346468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.511652</td>\n",
       "      <td>0.817238</td>\n",
       "      <td>0.563878</td>\n",
       "      <td>0.421862</td>\n",
       "      <td>0.633747</td>\n",
       "      <td>0.593015</td>\n",
       "      <td>0.570494</td>\n",
       "      <td>0.464229</td>\n",
       "      <td>0.791174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220152</td>\n",
       "      <td>0.924519</td>\n",
       "      <td>0.446466</td>\n",
       "      <td>0.771333</td>\n",
       "      <td>0.657411</td>\n",
       "      <td>0.628458</td>\n",
       "      <td>0.121865</td>\n",
       "      <td>0.195369</td>\n",
       "      <td>0.398596</td>\n",
       "      <td>0.367197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1982 rows × 834 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  cg22695986  cg01193368  cg22056094  cg06098368  cg08690094  \\\n",
       "0      1.0    0.233353    0.606732    0.730107    0.240143    0.561082   \n",
       "1      1.0    0.284813    0.599726    0.715363    0.242588    0.564277   \n",
       "2      1.0    0.206618    0.552816    0.572559    0.169127    0.541453   \n",
       "3      1.0    0.203151    0.655871    0.391728    0.224729    0.480992   \n",
       "4      0.0    0.266709    0.493554    0.395203    0.231550    0.474545   \n",
       "..     ...         ...         ...         ...         ...         ...   \n",
       "155    0.0    0.233895    0.732645    0.750664    0.149491    0.524413   \n",
       "156    0.0    0.247867    0.744726    0.477351    0.211166    0.500541   \n",
       "157    0.0    0.247103    0.768668    0.489964    0.235903    0.515031   \n",
       "158    0.0    0.525080    0.812421    0.533161    0.398285    0.603025   \n",
       "159    0.0    0.511652    0.817238    0.563878    0.421862    0.633747   \n",
       "\n",
       "     cg11236452  cg26916862  cg03124146  cg14770527  ...  cg11174855  \\\n",
       "0      0.533287    0.194961    0.144218    0.597436  ...    0.327951   \n",
       "1      0.578224    0.192382    0.139766    0.570267  ...    0.195741   \n",
       "2      0.509944    0.197505    0.193932    0.510173  ...    0.083195   \n",
       "3      0.421599    0.178132    0.181609    0.481978  ...    0.134511   \n",
       "4      0.381759    0.194651    0.170328    0.549200  ...    0.069399   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "155    0.618680    0.234315    0.236759    0.890437  ...    0.174961   \n",
       "156    0.604047    0.259998    0.343025    0.705436  ...    0.165785   \n",
       "157    0.663927    0.295938    0.331253    0.679972  ...    0.180183   \n",
       "158    0.545732    0.494836    0.403702    0.749127  ...    0.182134   \n",
       "159    0.593015    0.570494    0.464229    0.791174  ...    0.220152   \n",
       "\n",
       "     cg04524933  cg19418458  cg21808635  cg11359720  cg07635017  cg08641118  \\\n",
       "0      0.919347    0.365084    0.780742    0.731394    0.532118    0.081238   \n",
       "1      0.945236    0.337817    0.696424    0.745175    0.546033    0.099777   \n",
       "2      0.906838    0.357911    0.606163    0.773520    0.536528    0.079384   \n",
       "3      0.956986    0.583261    0.680310    0.753129    0.557829    0.067174   \n",
       "4      0.921778    0.333275    0.722998    0.824286    0.546425    0.075612   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "155    0.906643    0.581697    0.863217    0.798059    0.723334    0.130358   \n",
       "156    0.913656    0.615752    0.832152    0.712137    0.582116    0.300884   \n",
       "157    0.936972    0.580143    0.852484    0.725882    0.614776    0.291890   \n",
       "158    0.935897    0.435156    0.783916    0.682231    0.671497    0.108399   \n",
       "159    0.924519    0.446466    0.771333    0.657411    0.628458    0.121865   \n",
       "\n",
       "     cg09166085  cg22034735  cg17805624  \n",
       "0      0.108676    0.508394    0.279428  \n",
       "1      0.128168    0.498426    0.360946  \n",
       "2      0.091236    0.204013    0.296357  \n",
       "3      0.158550    0.297127    0.292479  \n",
       "4      0.178922    0.526409    0.278850  \n",
       "..          ...         ...         ...  \n",
       "155    0.177975    0.821848    0.391814  \n",
       "156    0.234928    0.620441    0.187204  \n",
       "157    0.213946    0.573433    0.174113  \n",
       "158    0.149804    0.330827    0.346468  \n",
       "159    0.195369    0.398596    0.367197  \n",
       "\n",
       "[1982 rows x 834 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2202 * 834\n",
    "train_data1 = pd.concat([data_ERISK, data_BSGS, data_DENMARK])\n",
    "train_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cg22695986</th>\n",
       "      <th>cg01193368</th>\n",
       "      <th>cg22056094</th>\n",
       "      <th>cg06098368</th>\n",
       "      <th>cg08690094</th>\n",
       "      <th>cg11236452</th>\n",
       "      <th>cg26916862</th>\n",
       "      <th>cg03124146</th>\n",
       "      <th>cg14770527</th>\n",
       "      <th>...</th>\n",
       "      <th>cg11174855</th>\n",
       "      <th>cg04524933</th>\n",
       "      <th>cg19418458</th>\n",
       "      <th>cg21808635</th>\n",
       "      <th>cg11359720</th>\n",
       "      <th>cg07635017</th>\n",
       "      <th>cg08641118</th>\n",
       "      <th>cg09166085</th>\n",
       "      <th>cg22034735</th>\n",
       "      <th>cg17805624</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.233353</td>\n",
       "      <td>0.606732</td>\n",
       "      <td>0.730107</td>\n",
       "      <td>0.240143</td>\n",
       "      <td>0.561082</td>\n",
       "      <td>0.533287</td>\n",
       "      <td>0.194961</td>\n",
       "      <td>0.144218</td>\n",
       "      <td>0.597436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327951</td>\n",
       "      <td>0.919347</td>\n",
       "      <td>0.365084</td>\n",
       "      <td>0.780742</td>\n",
       "      <td>0.731394</td>\n",
       "      <td>0.532118</td>\n",
       "      <td>0.081238</td>\n",
       "      <td>0.108676</td>\n",
       "      <td>0.508394</td>\n",
       "      <td>0.279428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.284813</td>\n",
       "      <td>0.599726</td>\n",
       "      <td>0.715363</td>\n",
       "      <td>0.242588</td>\n",
       "      <td>0.564277</td>\n",
       "      <td>0.578224</td>\n",
       "      <td>0.192382</td>\n",
       "      <td>0.139766</td>\n",
       "      <td>0.570267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195741</td>\n",
       "      <td>0.945236</td>\n",
       "      <td>0.337817</td>\n",
       "      <td>0.696424</td>\n",
       "      <td>0.745175</td>\n",
       "      <td>0.546033</td>\n",
       "      <td>0.099777</td>\n",
       "      <td>0.128168</td>\n",
       "      <td>0.498426</td>\n",
       "      <td>0.360946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.206618</td>\n",
       "      <td>0.552816</td>\n",
       "      <td>0.572559</td>\n",
       "      <td>0.169127</td>\n",
       "      <td>0.541453</td>\n",
       "      <td>0.509944</td>\n",
       "      <td>0.197505</td>\n",
       "      <td>0.193932</td>\n",
       "      <td>0.510173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083195</td>\n",
       "      <td>0.906838</td>\n",
       "      <td>0.357911</td>\n",
       "      <td>0.606163</td>\n",
       "      <td>0.773520</td>\n",
       "      <td>0.536528</td>\n",
       "      <td>0.079384</td>\n",
       "      <td>0.091236</td>\n",
       "      <td>0.204013</td>\n",
       "      <td>0.296357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.203151</td>\n",
       "      <td>0.655871</td>\n",
       "      <td>0.391728</td>\n",
       "      <td>0.224729</td>\n",
       "      <td>0.480992</td>\n",
       "      <td>0.421599</td>\n",
       "      <td>0.178132</td>\n",
       "      <td>0.181609</td>\n",
       "      <td>0.481978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134511</td>\n",
       "      <td>0.956986</td>\n",
       "      <td>0.583261</td>\n",
       "      <td>0.680310</td>\n",
       "      <td>0.753129</td>\n",
       "      <td>0.557829</td>\n",
       "      <td>0.067174</td>\n",
       "      <td>0.158550</td>\n",
       "      <td>0.297127</td>\n",
       "      <td>0.292479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.266709</td>\n",
       "      <td>0.493554</td>\n",
       "      <td>0.395203</td>\n",
       "      <td>0.231550</td>\n",
       "      <td>0.474545</td>\n",
       "      <td>0.381759</td>\n",
       "      <td>0.194651</td>\n",
       "      <td>0.170328</td>\n",
       "      <td>0.549200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069399</td>\n",
       "      <td>0.921778</td>\n",
       "      <td>0.333275</td>\n",
       "      <td>0.722998</td>\n",
       "      <td>0.824286</td>\n",
       "      <td>0.546425</td>\n",
       "      <td>0.075612</td>\n",
       "      <td>0.178922</td>\n",
       "      <td>0.526409</td>\n",
       "      <td>0.278850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.233895</td>\n",
       "      <td>0.732645</td>\n",
       "      <td>0.750664</td>\n",
       "      <td>0.149491</td>\n",
       "      <td>0.524413</td>\n",
       "      <td>0.618680</td>\n",
       "      <td>0.234315</td>\n",
       "      <td>0.236759</td>\n",
       "      <td>0.890437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174961</td>\n",
       "      <td>0.906643</td>\n",
       "      <td>0.581697</td>\n",
       "      <td>0.863217</td>\n",
       "      <td>0.798059</td>\n",
       "      <td>0.723334</td>\n",
       "      <td>0.130358</td>\n",
       "      <td>0.177975</td>\n",
       "      <td>0.821848</td>\n",
       "      <td>0.391814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.247867</td>\n",
       "      <td>0.744726</td>\n",
       "      <td>0.477351</td>\n",
       "      <td>0.211166</td>\n",
       "      <td>0.500541</td>\n",
       "      <td>0.604047</td>\n",
       "      <td>0.259998</td>\n",
       "      <td>0.343025</td>\n",
       "      <td>0.705436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165785</td>\n",
       "      <td>0.913656</td>\n",
       "      <td>0.615752</td>\n",
       "      <td>0.832152</td>\n",
       "      <td>0.712137</td>\n",
       "      <td>0.582116</td>\n",
       "      <td>0.300884</td>\n",
       "      <td>0.234928</td>\n",
       "      <td>0.620441</td>\n",
       "      <td>0.187204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.247103</td>\n",
       "      <td>0.768668</td>\n",
       "      <td>0.489964</td>\n",
       "      <td>0.235903</td>\n",
       "      <td>0.515031</td>\n",
       "      <td>0.663927</td>\n",
       "      <td>0.295938</td>\n",
       "      <td>0.331253</td>\n",
       "      <td>0.679972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180183</td>\n",
       "      <td>0.936972</td>\n",
       "      <td>0.580143</td>\n",
       "      <td>0.852484</td>\n",
       "      <td>0.725882</td>\n",
       "      <td>0.614776</td>\n",
       "      <td>0.291890</td>\n",
       "      <td>0.213946</td>\n",
       "      <td>0.573433</td>\n",
       "      <td>0.174113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.525080</td>\n",
       "      <td>0.812421</td>\n",
       "      <td>0.533161</td>\n",
       "      <td>0.398285</td>\n",
       "      <td>0.603025</td>\n",
       "      <td>0.545732</td>\n",
       "      <td>0.494836</td>\n",
       "      <td>0.403702</td>\n",
       "      <td>0.749127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182134</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.435156</td>\n",
       "      <td>0.783916</td>\n",
       "      <td>0.682231</td>\n",
       "      <td>0.671497</td>\n",
       "      <td>0.108399</td>\n",
       "      <td>0.149804</td>\n",
       "      <td>0.330827</td>\n",
       "      <td>0.346468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.511652</td>\n",
       "      <td>0.817238</td>\n",
       "      <td>0.563878</td>\n",
       "      <td>0.421862</td>\n",
       "      <td>0.633747</td>\n",
       "      <td>0.593015</td>\n",
       "      <td>0.570494</td>\n",
       "      <td>0.464229</td>\n",
       "      <td>0.791174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220152</td>\n",
       "      <td>0.924519</td>\n",
       "      <td>0.446466</td>\n",
       "      <td>0.771333</td>\n",
       "      <td>0.657411</td>\n",
       "      <td>0.628458</td>\n",
       "      <td>0.121865</td>\n",
       "      <td>0.195369</td>\n",
       "      <td>0.398596</td>\n",
       "      <td>0.367197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1982 rows × 832 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  cg22695986  cg01193368  cg22056094  cg06098368  cg08690094  \\\n",
       "0      1.0    0.233353    0.606732    0.730107    0.240143    0.561082   \n",
       "1      1.0    0.284813    0.599726    0.715363    0.242588    0.564277   \n",
       "2      1.0    0.206618    0.552816    0.572559    0.169127    0.541453   \n",
       "3      1.0    0.203151    0.655871    0.391728    0.224729    0.480992   \n",
       "4      0.0    0.266709    0.493554    0.395203    0.231550    0.474545   \n",
       "..     ...         ...         ...         ...         ...         ...   \n",
       "155    0.0    0.233895    0.732645    0.750664    0.149491    0.524413   \n",
       "156    0.0    0.247867    0.744726    0.477351    0.211166    0.500541   \n",
       "157    0.0    0.247103    0.768668    0.489964    0.235903    0.515031   \n",
       "158    0.0    0.525080    0.812421    0.533161    0.398285    0.603025   \n",
       "159    0.0    0.511652    0.817238    0.563878    0.421862    0.633747   \n",
       "\n",
       "     cg11236452  cg26916862  cg03124146  cg14770527  ...  cg11174855  \\\n",
       "0      0.533287    0.194961    0.144218    0.597436  ...    0.327951   \n",
       "1      0.578224    0.192382    0.139766    0.570267  ...    0.195741   \n",
       "2      0.509944    0.197505    0.193932    0.510173  ...    0.083195   \n",
       "3      0.421599    0.178132    0.181609    0.481978  ...    0.134511   \n",
       "4      0.381759    0.194651    0.170328    0.549200  ...    0.069399   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "155    0.618680    0.234315    0.236759    0.890437  ...    0.174961   \n",
       "156    0.604047    0.259998    0.343025    0.705436  ...    0.165785   \n",
       "157    0.663927    0.295938    0.331253    0.679972  ...    0.180183   \n",
       "158    0.545732    0.494836    0.403702    0.749127  ...    0.182134   \n",
       "159    0.593015    0.570494    0.464229    0.791174  ...    0.220152   \n",
       "\n",
       "     cg04524933  cg19418458  cg21808635  cg11359720  cg07635017  cg08641118  \\\n",
       "0      0.919347    0.365084    0.780742    0.731394    0.532118    0.081238   \n",
       "1      0.945236    0.337817    0.696424    0.745175    0.546033    0.099777   \n",
       "2      0.906838    0.357911    0.606163    0.773520    0.536528    0.079384   \n",
       "3      0.956986    0.583261    0.680310    0.753129    0.557829    0.067174   \n",
       "4      0.921778    0.333275    0.722998    0.824286    0.546425    0.075612   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "155    0.906643    0.581697    0.863217    0.798059    0.723334    0.130358   \n",
       "156    0.913656    0.615752    0.832152    0.712137    0.582116    0.300884   \n",
       "157    0.936972    0.580143    0.852484    0.725882    0.614776    0.291890   \n",
       "158    0.935897    0.435156    0.783916    0.682231    0.671497    0.108399   \n",
       "159    0.924519    0.446466    0.771333    0.657411    0.628458    0.121865   \n",
       "\n",
       "     cg09166085  cg22034735  cg17805624  \n",
       "0      0.108676    0.508394    0.279428  \n",
       "1      0.128168    0.498426    0.360946  \n",
       "2      0.091236    0.204013    0.296357  \n",
       "3      0.158550    0.297127    0.292479  \n",
       "4      0.178922    0.526409    0.278850  \n",
       "..          ...         ...         ...  \n",
       "155    0.177975    0.821848    0.391814  \n",
       "156    0.234928    0.620441    0.187204  \n",
       "157    0.213946    0.573433    0.174113  \n",
       "158    0.149804    0.330827    0.346468  \n",
       "159    0.195369    0.398596    0.367197  \n",
       "\n",
       "[1982 rows x 832 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2002 * 832\n",
    "train_data1 = train_data1.loc[:,[i for i in data_AMDTSS.columns]]\n",
    "train_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cg22695986</th>\n",
       "      <th>cg01193368</th>\n",
       "      <th>cg22056094</th>\n",
       "      <th>cg06098368</th>\n",
       "      <th>cg08690094</th>\n",
       "      <th>cg11236452</th>\n",
       "      <th>cg26916862</th>\n",
       "      <th>cg03124146</th>\n",
       "      <th>cg14770527</th>\n",
       "      <th>...</th>\n",
       "      <th>cg11174855</th>\n",
       "      <th>cg04524933</th>\n",
       "      <th>cg19418458</th>\n",
       "      <th>cg21808635</th>\n",
       "      <th>cg11359720</th>\n",
       "      <th>cg07635017</th>\n",
       "      <th>cg08641118</th>\n",
       "      <th>cg09166085</th>\n",
       "      <th>cg22034735</th>\n",
       "      <th>cg17805624</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.233353</td>\n",
       "      <td>0.606732</td>\n",
       "      <td>0.730107</td>\n",
       "      <td>0.240143</td>\n",
       "      <td>0.561082</td>\n",
       "      <td>0.533287</td>\n",
       "      <td>0.194961</td>\n",
       "      <td>0.144218</td>\n",
       "      <td>0.597436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327951</td>\n",
       "      <td>0.919347</td>\n",
       "      <td>0.365084</td>\n",
       "      <td>0.780742</td>\n",
       "      <td>0.731394</td>\n",
       "      <td>0.532118</td>\n",
       "      <td>0.081238</td>\n",
       "      <td>0.108676</td>\n",
       "      <td>0.508394</td>\n",
       "      <td>0.279428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.284813</td>\n",
       "      <td>0.599726</td>\n",
       "      <td>0.715363</td>\n",
       "      <td>0.242588</td>\n",
       "      <td>0.564277</td>\n",
       "      <td>0.578224</td>\n",
       "      <td>0.192382</td>\n",
       "      <td>0.139766</td>\n",
       "      <td>0.570267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195741</td>\n",
       "      <td>0.945236</td>\n",
       "      <td>0.337817</td>\n",
       "      <td>0.696424</td>\n",
       "      <td>0.745175</td>\n",
       "      <td>0.546033</td>\n",
       "      <td>0.099777</td>\n",
       "      <td>0.128168</td>\n",
       "      <td>0.498426</td>\n",
       "      <td>0.360946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.206618</td>\n",
       "      <td>0.552816</td>\n",
       "      <td>0.572559</td>\n",
       "      <td>0.169127</td>\n",
       "      <td>0.541453</td>\n",
       "      <td>0.509944</td>\n",
       "      <td>0.197505</td>\n",
       "      <td>0.193932</td>\n",
       "      <td>0.510173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083195</td>\n",
       "      <td>0.906838</td>\n",
       "      <td>0.357911</td>\n",
       "      <td>0.606163</td>\n",
       "      <td>0.773520</td>\n",
       "      <td>0.536528</td>\n",
       "      <td>0.079384</td>\n",
       "      <td>0.091236</td>\n",
       "      <td>0.204013</td>\n",
       "      <td>0.296357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.203151</td>\n",
       "      <td>0.655871</td>\n",
       "      <td>0.391728</td>\n",
       "      <td>0.224729</td>\n",
       "      <td>0.480992</td>\n",
       "      <td>0.421599</td>\n",
       "      <td>0.178132</td>\n",
       "      <td>0.181609</td>\n",
       "      <td>0.481978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134511</td>\n",
       "      <td>0.956986</td>\n",
       "      <td>0.583261</td>\n",
       "      <td>0.680310</td>\n",
       "      <td>0.753129</td>\n",
       "      <td>0.557829</td>\n",
       "      <td>0.067174</td>\n",
       "      <td>0.158550</td>\n",
       "      <td>0.297127</td>\n",
       "      <td>0.292479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.266709</td>\n",
       "      <td>0.493554</td>\n",
       "      <td>0.395203</td>\n",
       "      <td>0.231550</td>\n",
       "      <td>0.474545</td>\n",
       "      <td>0.381759</td>\n",
       "      <td>0.194651</td>\n",
       "      <td>0.170328</td>\n",
       "      <td>0.549200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069399</td>\n",
       "      <td>0.921778</td>\n",
       "      <td>0.333275</td>\n",
       "      <td>0.722998</td>\n",
       "      <td>0.824286</td>\n",
       "      <td>0.546425</td>\n",
       "      <td>0.075612</td>\n",
       "      <td>0.178922</td>\n",
       "      <td>0.526409</td>\n",
       "      <td>0.278850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.421692</td>\n",
       "      <td>0.819136</td>\n",
       "      <td>0.889120</td>\n",
       "      <td>0.274042</td>\n",
       "      <td>0.525204</td>\n",
       "      <td>0.698463</td>\n",
       "      <td>0.415842</td>\n",
       "      <td>0.387231</td>\n",
       "      <td>0.759353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285387</td>\n",
       "      <td>0.953237</td>\n",
       "      <td>0.542180</td>\n",
       "      <td>0.869079</td>\n",
       "      <td>0.713411</td>\n",
       "      <td>0.647941</td>\n",
       "      <td>0.176504</td>\n",
       "      <td>0.168747</td>\n",
       "      <td>0.529537</td>\n",
       "      <td>0.275265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.155194</td>\n",
       "      <td>0.650936</td>\n",
       "      <td>0.436741</td>\n",
       "      <td>0.127649</td>\n",
       "      <td>0.458996</td>\n",
       "      <td>0.414456</td>\n",
       "      <td>0.214664</td>\n",
       "      <td>0.116075</td>\n",
       "      <td>0.700554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098905</td>\n",
       "      <td>0.945503</td>\n",
       "      <td>0.361169</td>\n",
       "      <td>0.766231</td>\n",
       "      <td>0.652946</td>\n",
       "      <td>0.603185</td>\n",
       "      <td>0.143384</td>\n",
       "      <td>0.172315</td>\n",
       "      <td>0.522733</td>\n",
       "      <td>0.230949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.170119</td>\n",
       "      <td>0.668992</td>\n",
       "      <td>0.532885</td>\n",
       "      <td>0.141059</td>\n",
       "      <td>0.522190</td>\n",
       "      <td>0.469892</td>\n",
       "      <td>0.177975</td>\n",
       "      <td>0.141652</td>\n",
       "      <td>0.686412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208649</td>\n",
       "      <td>0.962932</td>\n",
       "      <td>0.609414</td>\n",
       "      <td>0.827691</td>\n",
       "      <td>0.850505</td>\n",
       "      <td>0.631754</td>\n",
       "      <td>0.077538</td>\n",
       "      <td>0.129992</td>\n",
       "      <td>0.617378</td>\n",
       "      <td>0.242766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.316241</td>\n",
       "      <td>0.642261</td>\n",
       "      <td>0.906532</td>\n",
       "      <td>0.303791</td>\n",
       "      <td>0.403003</td>\n",
       "      <td>0.830418</td>\n",
       "      <td>0.242378</td>\n",
       "      <td>0.278751</td>\n",
       "      <td>0.764943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223300</td>\n",
       "      <td>0.917177</td>\n",
       "      <td>0.357777</td>\n",
       "      <td>0.776557</td>\n",
       "      <td>0.607103</td>\n",
       "      <td>0.595563</td>\n",
       "      <td>0.108415</td>\n",
       "      <td>0.338011</td>\n",
       "      <td>0.444886</td>\n",
       "      <td>0.302313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.356544</td>\n",
       "      <td>0.780978</td>\n",
       "      <td>0.565044</td>\n",
       "      <td>0.301901</td>\n",
       "      <td>0.552029</td>\n",
       "      <td>0.542632</td>\n",
       "      <td>0.365609</td>\n",
       "      <td>0.374958</td>\n",
       "      <td>0.796539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231661</td>\n",
       "      <td>0.958034</td>\n",
       "      <td>0.734586</td>\n",
       "      <td>0.737711</td>\n",
       "      <td>0.774517</td>\n",
       "      <td>0.587225</td>\n",
       "      <td>0.164567</td>\n",
       "      <td>0.167376</td>\n",
       "      <td>0.740924</td>\n",
       "      <td>0.223064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2246 rows × 832 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  cg22695986  cg01193368  cg22056094  cg06098368  cg08690094  \\\n",
       "0      1.0    0.233353    0.606732    0.730107    0.240143    0.561082   \n",
       "1      1.0    0.284813    0.599726    0.715363    0.242588    0.564277   \n",
       "2      1.0    0.206618    0.552816    0.572559    0.169127    0.541453   \n",
       "3      1.0    0.203151    0.655871    0.391728    0.224729    0.480992   \n",
       "4      0.0    0.266709    0.493554    0.395203    0.231550    0.474545   \n",
       "..     ...         ...         ...         ...         ...         ...   \n",
       "471    0.0    0.421692    0.819136    0.889120    0.274042    0.525204   \n",
       "472    1.0    0.155194    0.650936    0.436741    0.127649    0.458996   \n",
       "475    1.0    0.170119    0.668992    0.532885    0.141059    0.522190   \n",
       "477    0.0    0.316241    0.642261    0.906532    0.303791    0.403003   \n",
       "478    1.0    0.356544    0.780978    0.565044    0.301901    0.552029   \n",
       "\n",
       "     cg11236452  cg26916862  cg03124146  cg14770527  ...  cg11174855  \\\n",
       "0      0.533287    0.194961    0.144218    0.597436  ...    0.327951   \n",
       "1      0.578224    0.192382    0.139766    0.570267  ...    0.195741   \n",
       "2      0.509944    0.197505    0.193932    0.510173  ...    0.083195   \n",
       "3      0.421599    0.178132    0.181609    0.481978  ...    0.134511   \n",
       "4      0.381759    0.194651    0.170328    0.549200  ...    0.069399   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "471    0.698463    0.415842    0.387231    0.759353  ...    0.285387   \n",
       "472    0.414456    0.214664    0.116075    0.700554  ...    0.098905   \n",
       "475    0.469892    0.177975    0.141652    0.686412  ...    0.208649   \n",
       "477    0.830418    0.242378    0.278751    0.764943  ...    0.223300   \n",
       "478    0.542632    0.365609    0.374958    0.796539  ...    0.231661   \n",
       "\n",
       "     cg04524933  cg19418458  cg21808635  cg11359720  cg07635017  cg08641118  \\\n",
       "0      0.919347    0.365084    0.780742    0.731394    0.532118    0.081238   \n",
       "1      0.945236    0.337817    0.696424    0.745175    0.546033    0.099777   \n",
       "2      0.906838    0.357911    0.606163    0.773520    0.536528    0.079384   \n",
       "3      0.956986    0.583261    0.680310    0.753129    0.557829    0.067174   \n",
       "4      0.921778    0.333275    0.722998    0.824286    0.546425    0.075612   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "471    0.953237    0.542180    0.869079    0.713411    0.647941    0.176504   \n",
       "472    0.945503    0.361169    0.766231    0.652946    0.603185    0.143384   \n",
       "475    0.962932    0.609414    0.827691    0.850505    0.631754    0.077538   \n",
       "477    0.917177    0.357777    0.776557    0.607103    0.595563    0.108415   \n",
       "478    0.958034    0.734586    0.737711    0.774517    0.587225    0.164567   \n",
       "\n",
       "     cg09166085  cg22034735  cg17805624  \n",
       "0      0.108676    0.508394    0.279428  \n",
       "1      0.128168    0.498426    0.360946  \n",
       "2      0.091236    0.204013    0.296357  \n",
       "3      0.158550    0.297127    0.292479  \n",
       "4      0.178922    0.526409    0.278850  \n",
       "..          ...         ...         ...  \n",
       "471    0.168747    0.529537    0.275265  \n",
       "472    0.172315    0.522733    0.230949  \n",
       "475    0.129992    0.617378    0.242766  \n",
       "477    0.338011    0.444886    0.302313  \n",
       "478    0.167376    0.740924    0.223064  \n",
       "\n",
       "[2246 rows x 832 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2530 * 832\n",
    "train_data1 = pd.concat([train_data1, data_AMDTSS])\n",
    "train_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1684, 831), (562, 831), (1684,), (562,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training 75% developing 25%\n",
    "x_train1, x_dev1, y_train1, y_dev1 = train_test_split(train_data1.drop(columns=['label']), train_data1['label'])\n",
    "x_train1.shape, x_dev1.shape, y_train1.shape, y_dev1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Original Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7223243464052287"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en1 = LogisticRegression(penalty = \"elasticnet\", solver = \"saga\", l1_ratio = 0.5)\n",
    "en1.fit(x_train1, y_train1)\n",
    "y_EMTAB = data_EMTAB['label']\n",
    "x_EMTAB = data_EMTAB.loc[:,[i for i in data_AMDTSS.columns]]\n",
    "x_EMTAB = x_EMTAB.drop(['label'], axis = 1)\n",
    "en_auc1 = roc_auc_score(y_EMTAB, en1.predict_proba(x_EMTAB)[:, 1])\n",
    "en_auc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train New Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Selection by RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=500, min_impurity_decrease=1e-05,\n",
       "                       n_estimators=500)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid search for rf\n",
    "# The number of trees in the forest.\n",
    "n_estimators = [50, 100, 200, 300, 500]\n",
    "# The function to measure the quality of a split\n",
    "criterion = [\"gini\", \"entropy\"]\n",
    "# A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "min_impurity_decrease = [0.1, 0.000001, 0.00001]\n",
    "# The maximum depth of the tree.\n",
    "max_depth = [20, 50, 100, 500, 1000]\n",
    "\n",
    "param_distributions = dict(n_estimators = n_estimators, criterion = criterion, min_impurity_decrease = min_impurity_decrease, max_depth = max_depth)\n",
    "rf = RandomForestClassifier()\n",
    "grid_rf = RandomizedSearchCV(estimator = rf, param_distributions = param_distributions, scoring = \"roc_auc\",\n",
    "                          verbose = 1, n_jobs = -1) \n",
    "grid_result_rf = grid_rf.fit(x_train1, y_train1) \n",
    "grid_result_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=RandomForestClassifier(max_depth=500,\n",
       "                                                 min_impurity_decrease=1e-05,\n",
       "                                                 n_estimators=500))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variable selection by random forest\n",
    "rf_selection1 = SelectFromModel(grid_result_rf.best_estimator_)\n",
    "rf_selection1.fit(x_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selected variables\n",
    "selected_feat_rf1 = x_train1.columns[(rf_selection1.get_support())]\n",
    "len(selected_feat_rf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cg22695986</th>\n",
       "      <th>cg01193368</th>\n",
       "      <th>cg22056094</th>\n",
       "      <th>cg06098368</th>\n",
       "      <th>cg08690094</th>\n",
       "      <th>cg11236452</th>\n",
       "      <th>cg14770527</th>\n",
       "      <th>cg23316599</th>\n",
       "      <th>cg10933186</th>\n",
       "      <th>cg11108474</th>\n",
       "      <th>...</th>\n",
       "      <th>cg09243445</th>\n",
       "      <th>cg14692950</th>\n",
       "      <th>cg23604683</th>\n",
       "      <th>cg09009380</th>\n",
       "      <th>cg09990584</th>\n",
       "      <th>cg07903023</th>\n",
       "      <th>cg07635017</th>\n",
       "      <th>cg08641118</th>\n",
       "      <th>cg17805624</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.233353</td>\n",
       "      <td>0.606732</td>\n",
       "      <td>0.730107</td>\n",
       "      <td>0.240143</td>\n",
       "      <td>0.561082</td>\n",
       "      <td>0.533287</td>\n",
       "      <td>0.597436</td>\n",
       "      <td>0.396608</td>\n",
       "      <td>0.850368</td>\n",
       "      <td>0.479727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306735</td>\n",
       "      <td>0.672161</td>\n",
       "      <td>0.596974</td>\n",
       "      <td>0.323892</td>\n",
       "      <td>0.342347</td>\n",
       "      <td>0.641294</td>\n",
       "      <td>0.532118</td>\n",
       "      <td>0.081238</td>\n",
       "      <td>0.279428</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.284813</td>\n",
       "      <td>0.599726</td>\n",
       "      <td>0.715363</td>\n",
       "      <td>0.242588</td>\n",
       "      <td>0.564277</td>\n",
       "      <td>0.578224</td>\n",
       "      <td>0.570267</td>\n",
       "      <td>0.419441</td>\n",
       "      <td>0.770310</td>\n",
       "      <td>0.483118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.316390</td>\n",
       "      <td>0.660461</td>\n",
       "      <td>0.648905</td>\n",
       "      <td>0.315845</td>\n",
       "      <td>0.321944</td>\n",
       "      <td>0.668892</td>\n",
       "      <td>0.546033</td>\n",
       "      <td>0.099777</td>\n",
       "      <td>0.360946</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.206618</td>\n",
       "      <td>0.552816</td>\n",
       "      <td>0.572559</td>\n",
       "      <td>0.169127</td>\n",
       "      <td>0.541453</td>\n",
       "      <td>0.509944</td>\n",
       "      <td>0.510173</td>\n",
       "      <td>0.479463</td>\n",
       "      <td>0.741130</td>\n",
       "      <td>0.368669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335758</td>\n",
       "      <td>0.661402</td>\n",
       "      <td>0.671847</td>\n",
       "      <td>0.313867</td>\n",
       "      <td>0.327677</td>\n",
       "      <td>0.433934</td>\n",
       "      <td>0.536528</td>\n",
       "      <td>0.079384</td>\n",
       "      <td>0.296357</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.203151</td>\n",
       "      <td>0.655871</td>\n",
       "      <td>0.391728</td>\n",
       "      <td>0.224729</td>\n",
       "      <td>0.480992</td>\n",
       "      <td>0.421599</td>\n",
       "      <td>0.481978</td>\n",
       "      <td>0.510254</td>\n",
       "      <td>0.667161</td>\n",
       "      <td>0.360383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355541</td>\n",
       "      <td>0.620736</td>\n",
       "      <td>0.654326</td>\n",
       "      <td>0.300121</td>\n",
       "      <td>0.303911</td>\n",
       "      <td>0.416025</td>\n",
       "      <td>0.557829</td>\n",
       "      <td>0.067174</td>\n",
       "      <td>0.292479</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.266709</td>\n",
       "      <td>0.493554</td>\n",
       "      <td>0.395203</td>\n",
       "      <td>0.231550</td>\n",
       "      <td>0.474545</td>\n",
       "      <td>0.381759</td>\n",
       "      <td>0.549200</td>\n",
       "      <td>0.506738</td>\n",
       "      <td>0.731597</td>\n",
       "      <td>0.401675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359560</td>\n",
       "      <td>0.675501</td>\n",
       "      <td>0.630504</td>\n",
       "      <td>0.313593</td>\n",
       "      <td>0.333914</td>\n",
       "      <td>0.593111</td>\n",
       "      <td>0.546425</td>\n",
       "      <td>0.075612</td>\n",
       "      <td>0.278850</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>0.421692</td>\n",
       "      <td>0.819136</td>\n",
       "      <td>0.889120</td>\n",
       "      <td>0.274042</td>\n",
       "      <td>0.525204</td>\n",
       "      <td>0.698463</td>\n",
       "      <td>0.759353</td>\n",
       "      <td>0.573460</td>\n",
       "      <td>0.799908</td>\n",
       "      <td>0.401122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.365249</td>\n",
       "      <td>0.789125</td>\n",
       "      <td>0.759704</td>\n",
       "      <td>0.389640</td>\n",
       "      <td>0.385300</td>\n",
       "      <td>0.699565</td>\n",
       "      <td>0.647941</td>\n",
       "      <td>0.176504</td>\n",
       "      <td>0.275265</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>0.155194</td>\n",
       "      <td>0.650936</td>\n",
       "      <td>0.436741</td>\n",
       "      <td>0.127649</td>\n",
       "      <td>0.458996</td>\n",
       "      <td>0.414456</td>\n",
       "      <td>0.700554</td>\n",
       "      <td>0.475778</td>\n",
       "      <td>0.679567</td>\n",
       "      <td>0.298102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362257</td>\n",
       "      <td>0.730544</td>\n",
       "      <td>0.710898</td>\n",
       "      <td>0.337200</td>\n",
       "      <td>0.347106</td>\n",
       "      <td>0.598367</td>\n",
       "      <td>0.603185</td>\n",
       "      <td>0.143384</td>\n",
       "      <td>0.230949</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0.170119</td>\n",
       "      <td>0.668992</td>\n",
       "      <td>0.532885</td>\n",
       "      <td>0.141059</td>\n",
       "      <td>0.522190</td>\n",
       "      <td>0.469892</td>\n",
       "      <td>0.686412</td>\n",
       "      <td>0.454601</td>\n",
       "      <td>0.703902</td>\n",
       "      <td>0.322749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403914</td>\n",
       "      <td>0.841245</td>\n",
       "      <td>0.780795</td>\n",
       "      <td>0.410973</td>\n",
       "      <td>0.507703</td>\n",
       "      <td>0.744434</td>\n",
       "      <td>0.631754</td>\n",
       "      <td>0.077538</td>\n",
       "      <td>0.242766</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0.316241</td>\n",
       "      <td>0.642261</td>\n",
       "      <td>0.906532</td>\n",
       "      <td>0.303791</td>\n",
       "      <td>0.403003</td>\n",
       "      <td>0.830418</td>\n",
       "      <td>0.764943</td>\n",
       "      <td>0.376560</td>\n",
       "      <td>0.921208</td>\n",
       "      <td>0.445176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.381835</td>\n",
       "      <td>0.819452</td>\n",
       "      <td>0.786505</td>\n",
       "      <td>0.340245</td>\n",
       "      <td>0.452558</td>\n",
       "      <td>0.637866</td>\n",
       "      <td>0.595563</td>\n",
       "      <td>0.108415</td>\n",
       "      <td>0.302313</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0.356544</td>\n",
       "      <td>0.780978</td>\n",
       "      <td>0.565044</td>\n",
       "      <td>0.301901</td>\n",
       "      <td>0.552029</td>\n",
       "      <td>0.542632</td>\n",
       "      <td>0.796539</td>\n",
       "      <td>0.529338</td>\n",
       "      <td>0.782587</td>\n",
       "      <td>0.463919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.358353</td>\n",
       "      <td>0.800519</td>\n",
       "      <td>0.794745</td>\n",
       "      <td>0.378964</td>\n",
       "      <td>0.379746</td>\n",
       "      <td>0.654000</td>\n",
       "      <td>0.587225</td>\n",
       "      <td>0.164567</td>\n",
       "      <td>0.223064</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2246 rows × 281 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cg22695986  cg01193368  cg22056094  cg06098368  cg08690094  cg11236452  \\\n",
       "0      0.233353    0.606732    0.730107    0.240143    0.561082    0.533287   \n",
       "1      0.284813    0.599726    0.715363    0.242588    0.564277    0.578224   \n",
       "2      0.206618    0.552816    0.572559    0.169127    0.541453    0.509944   \n",
       "3      0.203151    0.655871    0.391728    0.224729    0.480992    0.421599   \n",
       "4      0.266709    0.493554    0.395203    0.231550    0.474545    0.381759   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "471    0.421692    0.819136    0.889120    0.274042    0.525204    0.698463   \n",
       "472    0.155194    0.650936    0.436741    0.127649    0.458996    0.414456   \n",
       "475    0.170119    0.668992    0.532885    0.141059    0.522190    0.469892   \n",
       "477    0.316241    0.642261    0.906532    0.303791    0.403003    0.830418   \n",
       "478    0.356544    0.780978    0.565044    0.301901    0.552029    0.542632   \n",
       "\n",
       "     cg14770527  cg23316599  cg10933186  cg11108474  ...  cg09243445  \\\n",
       "0      0.597436    0.396608    0.850368    0.479727  ...    0.306735   \n",
       "1      0.570267    0.419441    0.770310    0.483118  ...    0.316390   \n",
       "2      0.510173    0.479463    0.741130    0.368669  ...    0.335758   \n",
       "3      0.481978    0.510254    0.667161    0.360383  ...    0.355541   \n",
       "4      0.549200    0.506738    0.731597    0.401675  ...    0.359560   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "471    0.759353    0.573460    0.799908    0.401122  ...    0.365249   \n",
       "472    0.700554    0.475778    0.679567    0.298102  ...    0.362257   \n",
       "475    0.686412    0.454601    0.703902    0.322749  ...    0.403914   \n",
       "477    0.764943    0.376560    0.921208    0.445176  ...    0.381835   \n",
       "478    0.796539    0.529338    0.782587    0.463919  ...    0.358353   \n",
       "\n",
       "     cg14692950  cg23604683  cg09009380  cg09990584  cg07903023  cg07635017  \\\n",
       "0      0.672161    0.596974    0.323892    0.342347    0.641294    0.532118   \n",
       "1      0.660461    0.648905    0.315845    0.321944    0.668892    0.546033   \n",
       "2      0.661402    0.671847    0.313867    0.327677    0.433934    0.536528   \n",
       "3      0.620736    0.654326    0.300121    0.303911    0.416025    0.557829   \n",
       "4      0.675501    0.630504    0.313593    0.333914    0.593111    0.546425   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "471    0.789125    0.759704    0.389640    0.385300    0.699565    0.647941   \n",
       "472    0.730544    0.710898    0.337200    0.347106    0.598367    0.603185   \n",
       "475    0.841245    0.780795    0.410973    0.507703    0.744434    0.631754   \n",
       "477    0.819452    0.786505    0.340245    0.452558    0.637866    0.595563   \n",
       "478    0.800519    0.794745    0.378964    0.379746    0.654000    0.587225   \n",
       "\n",
       "     cg08641118  cg17805624  label  \n",
       "0      0.081238    0.279428    1.0  \n",
       "1      0.099777    0.360946    1.0  \n",
       "2      0.079384    0.296357    1.0  \n",
       "3      0.067174    0.292479    1.0  \n",
       "4      0.075612    0.278850    0.0  \n",
       "..          ...         ...    ...  \n",
       "471    0.176504    0.275265    0.0  \n",
       "472    0.143384    0.230949    1.0  \n",
       "475    0.077538    0.242766    1.0  \n",
       "477    0.108415    0.302313    0.0  \n",
       "478    0.164567    0.223064    1.0  \n",
       "\n",
       "[2246 rows x 281 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_selected_train_data1 = train_data1.loc[:,[i for i in selected_feat_rf1]]\n",
    "rf_selected_train_data1['label'] = train_data1['label'] \n",
    "rf_selected_train_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1684, 280), (562, 280), (1684,), (562,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training 75% developing 25%\n",
    "x_train_rf1, x_dev_rf1, y_train_rf1, y_dev_rf1 = train_test_split(rf_selected_train_data1.drop(columns=['label']), rf_selected_train_data1['label'])\n",
    "x_train_rf1.shape, x_dev_rf1.shape, y_train_rf1.shape, y_dev_rf1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8345204391437465"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_selected1 = grid_result_rf.best_estimator_\n",
    "rf_selected1.fit(x_train_rf1, y_train_rf1)\n",
    "rf_selected_auc1 = roc_auc_score(y_dev_rf1, rf_selected1.predict_proba(x_dev_rf1)[:, 1])\n",
    "rf_selected_auc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_EMTAB_rf_selected = data_EMTAB['label']\n",
    "x_EMTAB_rf_selected = data_EMTAB.loc[:,[i for i in selected_feat_rf1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.641452205882353"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_selected_auc_EMTAB = roc_auc_score(y_EMTAB_rf_selected, rf_selected1.predict_proba(x_EMTAB_rf_selected)[:, 1])\n",
    "rf_selected_auc_EMTAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6255718954248366"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking_rf_EMTAB = get_stacking()\n",
    "stacking_rf_EMTAB.fit(x_train_rf1, y_train_rf1)\n",
    "stacking_rf_EMTAB_auc = roc_auc_score(y_EMTAB_rf_selected, stacking_rf_EMTAB.predict_proba(x_EMTAB_rf_selected)[:, 1])\n",
    "stacking_rf_EMTAB_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(tol = 0.001, solver = 'sag', penalty = 'l2', C = 30)\n",
    "clf2 = RandomForestClassifier(n_estimators = 500, min_impurity_decrease = 1e-06, max_depth = 50, criterion = 'gini')\n",
    "clf3 = GradientBoostingClassifier(n_estimators = 300, max_depth = 5, learning_rate = 0.5)\n",
    "clf4 = SVC(kernel = 'rbf', gamma = 'scale', degree = 1, decision_function_shape = 'ovr', C = 20, probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6876633986928105"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_rf_EMTAB = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gb', clf3), ('svc', clf4)], voting='soft')\n",
    "voting_rf_EMTAB.fit(x_train_rf1, y_train_rf1)\n",
    "voting__rf_EMTAB_auc = roc_auc_score(y_EMTAB_rf_selected, voting_rf_EMTAB.predict_proba(x_EMTAB_rf_selected)[:, 1])\n",
    "voting__rf_EMTAB_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Selection by LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 450, in _check_solver\n",
      "    raise ValueError(\"Only 'saga' solver supports elasticnet penalty,\"\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 450, in _check_solver\n",
      "    raise ValueError(\"Only 'saga' solver supports elasticnet penalty,\"\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 450, in _check_solver\n",
      "    raise ValueError(\"Only 'saga' solver supports elasticnet penalty,\"\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 450, in _check_solver\n",
      "    raise ValueError(\"Only 'saga' solver supports elasticnet penalty,\"\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 450, in _check_solver\n",
      "    raise ValueError(\"Only 'saga' solver supports elasticnet penalty,\"\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      " 0.79044013 0.78696934        nan 0.74636943]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, solver='newton-cg', tol=1e-05)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid search for lr\n",
    "penalty = [\"l1\", \"l2\", \"elasticnet\"]\n",
    "# Tolerance for stopping criteria.\n",
    "tol = [0.00001, 0.001, 0.0000001]\n",
    "# Inverse of regularization strength\n",
    "C = [0,1, 0.5, 1, 10, 20, 30, 50, 100]\n",
    "# Algorithm to use in the optimization problem\n",
    "solver = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "\n",
    "\n",
    "param_distributions = dict(penalty = penalty, tol = tol, C = C, solver = solver)\n",
    "lr = LogisticRegression()\n",
    "grid_lr = RandomizedSearchCV(estimator = lr, param_distributions = param_distributions, scoring = \"roc_auc\",\n",
    "                          verbose = 1, n_jobs = -1) \n",
    "grid_result_lr = grid_lr.fit(x_train1, y_train1) \n",
    "grid_result_lr.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=LogisticRegression(C=1, solver='newton-cg',\n",
       "                                             tol=1e-05))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_selection1 = SelectFromModel(grid_result_lr.best_estimator_)\n",
    "lr_selection1.fit(x_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "358"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_feat_lr1 = x_train1.columns[(lr_selection1.get_support())]\n",
    "len(selected_feat_lr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cg22695986</th>\n",
       "      <th>cg01193368</th>\n",
       "      <th>cg22056094</th>\n",
       "      <th>cg06098368</th>\n",
       "      <th>cg26916862</th>\n",
       "      <th>cg03124146</th>\n",
       "      <th>cg14770527</th>\n",
       "      <th>cg23316599</th>\n",
       "      <th>cg11108474</th>\n",
       "      <th>cg16340103</th>\n",
       "      <th>...</th>\n",
       "      <th>cg06495631</th>\n",
       "      <th>cg07903023</th>\n",
       "      <th>cg14466863</th>\n",
       "      <th>cg15236528</th>\n",
       "      <th>cg07291889</th>\n",
       "      <th>cg11174855</th>\n",
       "      <th>cg11359720</th>\n",
       "      <th>cg08641118</th>\n",
       "      <th>cg17805624</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.233353</td>\n",
       "      <td>0.606732</td>\n",
       "      <td>0.730107</td>\n",
       "      <td>0.240143</td>\n",
       "      <td>0.194961</td>\n",
       "      <td>0.144218</td>\n",
       "      <td>0.597436</td>\n",
       "      <td>0.396608</td>\n",
       "      <td>0.479727</td>\n",
       "      <td>0.556758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570667</td>\n",
       "      <td>0.641294</td>\n",
       "      <td>0.825537</td>\n",
       "      <td>0.156715</td>\n",
       "      <td>0.706627</td>\n",
       "      <td>0.327951</td>\n",
       "      <td>0.731394</td>\n",
       "      <td>0.081238</td>\n",
       "      <td>0.279428</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.284813</td>\n",
       "      <td>0.599726</td>\n",
       "      <td>0.715363</td>\n",
       "      <td>0.242588</td>\n",
       "      <td>0.192382</td>\n",
       "      <td>0.139766</td>\n",
       "      <td>0.570267</td>\n",
       "      <td>0.419441</td>\n",
       "      <td>0.483118</td>\n",
       "      <td>0.481381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.589975</td>\n",
       "      <td>0.668892</td>\n",
       "      <td>0.823126</td>\n",
       "      <td>0.220170</td>\n",
       "      <td>0.740209</td>\n",
       "      <td>0.195741</td>\n",
       "      <td>0.745175</td>\n",
       "      <td>0.099777</td>\n",
       "      <td>0.360946</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.206618</td>\n",
       "      <td>0.552816</td>\n",
       "      <td>0.572559</td>\n",
       "      <td>0.169127</td>\n",
       "      <td>0.197505</td>\n",
       "      <td>0.193932</td>\n",
       "      <td>0.510173</td>\n",
       "      <td>0.479463</td>\n",
       "      <td>0.368669</td>\n",
       "      <td>0.513161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507627</td>\n",
       "      <td>0.433934</td>\n",
       "      <td>0.748225</td>\n",
       "      <td>0.178511</td>\n",
       "      <td>0.643731</td>\n",
       "      <td>0.083195</td>\n",
       "      <td>0.773520</td>\n",
       "      <td>0.079384</td>\n",
       "      <td>0.296357</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.203151</td>\n",
       "      <td>0.655871</td>\n",
       "      <td>0.391728</td>\n",
       "      <td>0.224729</td>\n",
       "      <td>0.178132</td>\n",
       "      <td>0.181609</td>\n",
       "      <td>0.481978</td>\n",
       "      <td>0.510254</td>\n",
       "      <td>0.360383</td>\n",
       "      <td>0.432874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507012</td>\n",
       "      <td>0.416025</td>\n",
       "      <td>0.761062</td>\n",
       "      <td>0.086153</td>\n",
       "      <td>0.709617</td>\n",
       "      <td>0.134511</td>\n",
       "      <td>0.753129</td>\n",
       "      <td>0.067174</td>\n",
       "      <td>0.292479</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.266709</td>\n",
       "      <td>0.493554</td>\n",
       "      <td>0.395203</td>\n",
       "      <td>0.231550</td>\n",
       "      <td>0.194651</td>\n",
       "      <td>0.170328</td>\n",
       "      <td>0.549200</td>\n",
       "      <td>0.506738</td>\n",
       "      <td>0.401675</td>\n",
       "      <td>0.475677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673096</td>\n",
       "      <td>0.593111</td>\n",
       "      <td>0.805394</td>\n",
       "      <td>0.137047</td>\n",
       "      <td>0.727944</td>\n",
       "      <td>0.069399</td>\n",
       "      <td>0.824286</td>\n",
       "      <td>0.075612</td>\n",
       "      <td>0.278850</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>0.421692</td>\n",
       "      <td>0.819136</td>\n",
       "      <td>0.889120</td>\n",
       "      <td>0.274042</td>\n",
       "      <td>0.415842</td>\n",
       "      <td>0.387231</td>\n",
       "      <td>0.759353</td>\n",
       "      <td>0.573460</td>\n",
       "      <td>0.401122</td>\n",
       "      <td>0.677112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645043</td>\n",
       "      <td>0.699565</td>\n",
       "      <td>0.780383</td>\n",
       "      <td>0.128186</td>\n",
       "      <td>0.855644</td>\n",
       "      <td>0.285387</td>\n",
       "      <td>0.713411</td>\n",
       "      <td>0.176504</td>\n",
       "      <td>0.275265</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>0.155194</td>\n",
       "      <td>0.650936</td>\n",
       "      <td>0.436741</td>\n",
       "      <td>0.127649</td>\n",
       "      <td>0.214664</td>\n",
       "      <td>0.116075</td>\n",
       "      <td>0.700554</td>\n",
       "      <td>0.475778</td>\n",
       "      <td>0.298102</td>\n",
       "      <td>0.585688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.487461</td>\n",
       "      <td>0.598367</td>\n",
       "      <td>0.734427</td>\n",
       "      <td>0.134853</td>\n",
       "      <td>0.777445</td>\n",
       "      <td>0.098905</td>\n",
       "      <td>0.652946</td>\n",
       "      <td>0.143384</td>\n",
       "      <td>0.230949</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0.170119</td>\n",
       "      <td>0.668992</td>\n",
       "      <td>0.532885</td>\n",
       "      <td>0.141059</td>\n",
       "      <td>0.177975</td>\n",
       "      <td>0.141652</td>\n",
       "      <td>0.686412</td>\n",
       "      <td>0.454601</td>\n",
       "      <td>0.322749</td>\n",
       "      <td>0.536903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561416</td>\n",
       "      <td>0.744434</td>\n",
       "      <td>0.805425</td>\n",
       "      <td>0.146198</td>\n",
       "      <td>0.836003</td>\n",
       "      <td>0.208649</td>\n",
       "      <td>0.850505</td>\n",
       "      <td>0.077538</td>\n",
       "      <td>0.242766</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0.316241</td>\n",
       "      <td>0.642261</td>\n",
       "      <td>0.906532</td>\n",
       "      <td>0.303791</td>\n",
       "      <td>0.242378</td>\n",
       "      <td>0.278751</td>\n",
       "      <td>0.764943</td>\n",
       "      <td>0.376560</td>\n",
       "      <td>0.445176</td>\n",
       "      <td>0.485595</td>\n",
       "      <td>...</td>\n",
       "      <td>0.566642</td>\n",
       "      <td>0.637866</td>\n",
       "      <td>0.760405</td>\n",
       "      <td>0.130743</td>\n",
       "      <td>0.857884</td>\n",
       "      <td>0.223300</td>\n",
       "      <td>0.607103</td>\n",
       "      <td>0.108415</td>\n",
       "      <td>0.302313</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0.356544</td>\n",
       "      <td>0.780978</td>\n",
       "      <td>0.565044</td>\n",
       "      <td>0.301901</td>\n",
       "      <td>0.365609</td>\n",
       "      <td>0.374958</td>\n",
       "      <td>0.796539</td>\n",
       "      <td>0.529338</td>\n",
       "      <td>0.463919</td>\n",
       "      <td>0.555628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650711</td>\n",
       "      <td>0.654000</td>\n",
       "      <td>0.777106</td>\n",
       "      <td>0.173264</td>\n",
       "      <td>0.828490</td>\n",
       "      <td>0.231661</td>\n",
       "      <td>0.774517</td>\n",
       "      <td>0.164567</td>\n",
       "      <td>0.223064</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2246 rows × 359 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cg22695986  cg01193368  cg22056094  cg06098368  cg26916862  cg03124146  \\\n",
       "0      0.233353    0.606732    0.730107    0.240143    0.194961    0.144218   \n",
       "1      0.284813    0.599726    0.715363    0.242588    0.192382    0.139766   \n",
       "2      0.206618    0.552816    0.572559    0.169127    0.197505    0.193932   \n",
       "3      0.203151    0.655871    0.391728    0.224729    0.178132    0.181609   \n",
       "4      0.266709    0.493554    0.395203    0.231550    0.194651    0.170328   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "471    0.421692    0.819136    0.889120    0.274042    0.415842    0.387231   \n",
       "472    0.155194    0.650936    0.436741    0.127649    0.214664    0.116075   \n",
       "475    0.170119    0.668992    0.532885    0.141059    0.177975    0.141652   \n",
       "477    0.316241    0.642261    0.906532    0.303791    0.242378    0.278751   \n",
       "478    0.356544    0.780978    0.565044    0.301901    0.365609    0.374958   \n",
       "\n",
       "     cg14770527  cg23316599  cg11108474  cg16340103  ...  cg06495631  \\\n",
       "0      0.597436    0.396608    0.479727    0.556758  ...    0.570667   \n",
       "1      0.570267    0.419441    0.483118    0.481381  ...    0.589975   \n",
       "2      0.510173    0.479463    0.368669    0.513161  ...    0.507627   \n",
       "3      0.481978    0.510254    0.360383    0.432874  ...    0.507012   \n",
       "4      0.549200    0.506738    0.401675    0.475677  ...    0.673096   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "471    0.759353    0.573460    0.401122    0.677112  ...    0.645043   \n",
       "472    0.700554    0.475778    0.298102    0.585688  ...    0.487461   \n",
       "475    0.686412    0.454601    0.322749    0.536903  ...    0.561416   \n",
       "477    0.764943    0.376560    0.445176    0.485595  ...    0.566642   \n",
       "478    0.796539    0.529338    0.463919    0.555628  ...    0.650711   \n",
       "\n",
       "     cg07903023  cg14466863  cg15236528  cg07291889  cg11174855  cg11359720  \\\n",
       "0      0.641294    0.825537    0.156715    0.706627    0.327951    0.731394   \n",
       "1      0.668892    0.823126    0.220170    0.740209    0.195741    0.745175   \n",
       "2      0.433934    0.748225    0.178511    0.643731    0.083195    0.773520   \n",
       "3      0.416025    0.761062    0.086153    0.709617    0.134511    0.753129   \n",
       "4      0.593111    0.805394    0.137047    0.727944    0.069399    0.824286   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "471    0.699565    0.780383    0.128186    0.855644    0.285387    0.713411   \n",
       "472    0.598367    0.734427    0.134853    0.777445    0.098905    0.652946   \n",
       "475    0.744434    0.805425    0.146198    0.836003    0.208649    0.850505   \n",
       "477    0.637866    0.760405    0.130743    0.857884    0.223300    0.607103   \n",
       "478    0.654000    0.777106    0.173264    0.828490    0.231661    0.774517   \n",
       "\n",
       "     cg08641118  cg17805624  label  \n",
       "0      0.081238    0.279428    1.0  \n",
       "1      0.099777    0.360946    1.0  \n",
       "2      0.079384    0.296357    1.0  \n",
       "3      0.067174    0.292479    1.0  \n",
       "4      0.075612    0.278850    0.0  \n",
       "..          ...         ...    ...  \n",
       "471    0.176504    0.275265    0.0  \n",
       "472    0.143384    0.230949    1.0  \n",
       "475    0.077538    0.242766    1.0  \n",
       "477    0.108415    0.302313    0.0  \n",
       "478    0.164567    0.223064    1.0  \n",
       "\n",
       "[2246 rows x 359 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_selected_train_data1 = train_data1.loc[:,[i for i in selected_feat_lr1]]\n",
    "lr_selected_train_data1['label'] = train_data1['label'] \n",
    "lr_selected_train_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1684, 358), (562, 358), (1684,), (562,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training 75% developing 25%\n",
    "x_train_lr1, x_dev_lr1, y_train_lr1, y_dev_lr1 = train_test_split(lr_selected_train_data1.drop(columns=['label']), lr_selected_train_data1['label'])\n",
    "x_train_lr1.shape, x_dev_lr1.shape, y_train_lr1.shape, y_dev_lr1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.841263808655113"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_selected1 = grid_result_lr.best_estimator_\n",
    "lr_selected1.fit(x_train_lr1, y_train_lr1)\n",
    "lr_selected_auc1 = roc_auc_score(y_dev_lr1, lr_selected1.predict_proba(x_dev_lr1)[:, 1])\n",
    "lr_selected_auc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_EMTAB_lr_selected = data_EMTAB['label']\n",
    "x_EMTAB_lr_selected = data_EMTAB.loc[:,[i for i in selected_feat_lr1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7087724673202613"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_selected_auc_EMTAB = roc_auc_score(y_EMTAB_lr_selected, lr_selected1.predict_proba(x_EMTAB_lr_selected)[:, 1])\n",
    "lr_selected_auc_EMTAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.658935866013072"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking_lr_EMTAB = get_stacking()\n",
    "stacking_lr_EMTAB.fit(x_train_lr1, y_train_lr1)\n",
    "stacking_lr_EMTAB_auc = roc_auc_score(y_EMTAB_lr_selected, stacking_lr_EMTAB.predict_proba(x_EMTAB_lr_selected)[:, 1])\n",
    "stacking_lr_EMTAB_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6998672385620915"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_lr_EMTAB = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gb', clf3), ('svc', clf4)], voting='soft')\n",
    "voting_lr_EMTAB.fit(x_train_lr1, y_train_lr1)\n",
    "voting__lr_EMTAB_auc = roc_auc_score(y_EMTAB_lr_selected, voting_lr_EMTAB.predict_proba(x_EMTAB_lr_selected)[:, 1])\n",
    "voting__lr_EMTAB_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training: E-Risk, BSGS, Denmark, E-MTAB\n",
    "# Testing: AMDTSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cg22695986</th>\n",
       "      <th>cg01193368</th>\n",
       "      <th>cg22056094</th>\n",
       "      <th>cg06098368</th>\n",
       "      <th>cg08690094</th>\n",
       "      <th>cg11236452</th>\n",
       "      <th>cg26916862</th>\n",
       "      <th>cg03124146</th>\n",
       "      <th>cg14770527</th>\n",
       "      <th>...</th>\n",
       "      <th>cg11174855</th>\n",
       "      <th>cg04524933</th>\n",
       "      <th>cg19418458</th>\n",
       "      <th>cg21808635</th>\n",
       "      <th>cg11359720</th>\n",
       "      <th>cg07635017</th>\n",
       "      <th>cg08641118</th>\n",
       "      <th>cg09166085</th>\n",
       "      <th>cg22034735</th>\n",
       "      <th>cg17805624</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.233353</td>\n",
       "      <td>0.606732</td>\n",
       "      <td>0.730107</td>\n",
       "      <td>0.240143</td>\n",
       "      <td>0.561082</td>\n",
       "      <td>0.533287</td>\n",
       "      <td>0.194961</td>\n",
       "      <td>0.144218</td>\n",
       "      <td>0.597436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327951</td>\n",
       "      <td>0.919347</td>\n",
       "      <td>0.365084</td>\n",
       "      <td>0.780742</td>\n",
       "      <td>0.731394</td>\n",
       "      <td>0.532118</td>\n",
       "      <td>0.081238</td>\n",
       "      <td>0.108676</td>\n",
       "      <td>0.508394</td>\n",
       "      <td>0.279428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.284813</td>\n",
       "      <td>0.599726</td>\n",
       "      <td>0.715363</td>\n",
       "      <td>0.242588</td>\n",
       "      <td>0.564277</td>\n",
       "      <td>0.578224</td>\n",
       "      <td>0.192382</td>\n",
       "      <td>0.139766</td>\n",
       "      <td>0.570267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195741</td>\n",
       "      <td>0.945236</td>\n",
       "      <td>0.337817</td>\n",
       "      <td>0.696424</td>\n",
       "      <td>0.745175</td>\n",
       "      <td>0.546033</td>\n",
       "      <td>0.099777</td>\n",
       "      <td>0.128168</td>\n",
       "      <td>0.498426</td>\n",
       "      <td>0.360946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.206618</td>\n",
       "      <td>0.552816</td>\n",
       "      <td>0.572559</td>\n",
       "      <td>0.169127</td>\n",
       "      <td>0.541453</td>\n",
       "      <td>0.509944</td>\n",
       "      <td>0.197505</td>\n",
       "      <td>0.193932</td>\n",
       "      <td>0.510173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083195</td>\n",
       "      <td>0.906838</td>\n",
       "      <td>0.357911</td>\n",
       "      <td>0.606163</td>\n",
       "      <td>0.773520</td>\n",
       "      <td>0.536528</td>\n",
       "      <td>0.079384</td>\n",
       "      <td>0.091236</td>\n",
       "      <td>0.204013</td>\n",
       "      <td>0.296357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.203151</td>\n",
       "      <td>0.655871</td>\n",
       "      <td>0.391728</td>\n",
       "      <td>0.224729</td>\n",
       "      <td>0.480992</td>\n",
       "      <td>0.421599</td>\n",
       "      <td>0.178132</td>\n",
       "      <td>0.181609</td>\n",
       "      <td>0.481978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134511</td>\n",
       "      <td>0.956986</td>\n",
       "      <td>0.583261</td>\n",
       "      <td>0.680310</td>\n",
       "      <td>0.753129</td>\n",
       "      <td>0.557829</td>\n",
       "      <td>0.067174</td>\n",
       "      <td>0.158550</td>\n",
       "      <td>0.297127</td>\n",
       "      <td>0.292479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.266709</td>\n",
       "      <td>0.493554</td>\n",
       "      <td>0.395203</td>\n",
       "      <td>0.231550</td>\n",
       "      <td>0.474545</td>\n",
       "      <td>0.381759</td>\n",
       "      <td>0.194651</td>\n",
       "      <td>0.170328</td>\n",
       "      <td>0.549200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069399</td>\n",
       "      <td>0.921778</td>\n",
       "      <td>0.333275</td>\n",
       "      <td>0.722998</td>\n",
       "      <td>0.824286</td>\n",
       "      <td>0.546425</td>\n",
       "      <td>0.075612</td>\n",
       "      <td>0.178922</td>\n",
       "      <td>0.526409</td>\n",
       "      <td>0.278850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.265920</td>\n",
       "      <td>0.668810</td>\n",
       "      <td>0.538530</td>\n",
       "      <td>0.221750</td>\n",
       "      <td>0.450940</td>\n",
       "      <td>0.409590</td>\n",
       "      <td>0.261340</td>\n",
       "      <td>0.202900</td>\n",
       "      <td>0.703690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074460</td>\n",
       "      <td>0.932110</td>\n",
       "      <td>0.309700</td>\n",
       "      <td>0.811640</td>\n",
       "      <td>0.651310</td>\n",
       "      <td>0.426310</td>\n",
       "      <td>0.106370</td>\n",
       "      <td>0.128560</td>\n",
       "      <td>0.381660</td>\n",
       "      <td>0.148490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.264220</td>\n",
       "      <td>0.673100</td>\n",
       "      <td>0.552430</td>\n",
       "      <td>0.211480</td>\n",
       "      <td>0.484940</td>\n",
       "      <td>0.468880</td>\n",
       "      <td>0.259820</td>\n",
       "      <td>0.240040</td>\n",
       "      <td>0.703450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033430</td>\n",
       "      <td>0.918230</td>\n",
       "      <td>0.273310</td>\n",
       "      <td>0.824450</td>\n",
       "      <td>0.609600</td>\n",
       "      <td>0.465400</td>\n",
       "      <td>0.121100</td>\n",
       "      <td>0.089490</td>\n",
       "      <td>0.315100</td>\n",
       "      <td>0.114360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.300860</td>\n",
       "      <td>0.586160</td>\n",
       "      <td>0.691140</td>\n",
       "      <td>0.240640</td>\n",
       "      <td>0.481040</td>\n",
       "      <td>0.579180</td>\n",
       "      <td>0.275920</td>\n",
       "      <td>0.234960</td>\n",
       "      <td>0.633500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117130</td>\n",
       "      <td>0.936460</td>\n",
       "      <td>0.436270</td>\n",
       "      <td>0.820790</td>\n",
       "      <td>0.597970</td>\n",
       "      <td>0.438150</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>0.214790</td>\n",
       "      <td>0.261410</td>\n",
       "      <td>0.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.289540</td>\n",
       "      <td>0.679960</td>\n",
       "      <td>0.553150</td>\n",
       "      <td>0.286020</td>\n",
       "      <td>0.315550</td>\n",
       "      <td>0.421130</td>\n",
       "      <td>0.231630</td>\n",
       "      <td>0.297830</td>\n",
       "      <td>0.680370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043240</td>\n",
       "      <td>0.931800</td>\n",
       "      <td>0.296000</td>\n",
       "      <td>0.798780</td>\n",
       "      <td>0.609940</td>\n",
       "      <td>0.394220</td>\n",
       "      <td>0.143910</td>\n",
       "      <td>0.143810</td>\n",
       "      <td>0.566440</td>\n",
       "      <td>0.156560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.255850</td>\n",
       "      <td>0.591080</td>\n",
       "      <td>0.525270</td>\n",
       "      <td>0.268260</td>\n",
       "      <td>0.343080</td>\n",
       "      <td>0.478540</td>\n",
       "      <td>0.231610</td>\n",
       "      <td>0.272100</td>\n",
       "      <td>0.707310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094330</td>\n",
       "      <td>0.924410</td>\n",
       "      <td>0.299930</td>\n",
       "      <td>0.865710</td>\n",
       "      <td>0.703540</td>\n",
       "      <td>0.399760</td>\n",
       "      <td>0.157860</td>\n",
       "      <td>0.165840</td>\n",
       "      <td>0.629880</td>\n",
       "      <td>0.119080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2630 rows × 834 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  cg22695986  cg01193368  cg22056094  cg06098368  cg08690094  \\\n",
       "0      1.0    0.233353    0.606732    0.730107    0.240143    0.561082   \n",
       "1      1.0    0.284813    0.599726    0.715363    0.242588    0.564277   \n",
       "2      1.0    0.206618    0.552816    0.572559    0.169127    0.541453   \n",
       "3      1.0    0.203151    0.655871    0.391728    0.224729    0.480992   \n",
       "4      0.0    0.266709    0.493554    0.395203    0.231550    0.474545   \n",
       "..     ...         ...         ...         ...         ...         ...   \n",
       "643    1.0    0.265920    0.668810    0.538530    0.221750    0.450940   \n",
       "644    1.0    0.264220    0.673100    0.552430    0.211480    0.484940   \n",
       "645    0.0    0.300860    0.586160    0.691140    0.240640    0.481040   \n",
       "646    0.0    0.289540    0.679960    0.553150    0.286020    0.315550   \n",
       "647    0.0    0.255850    0.591080    0.525270    0.268260    0.343080   \n",
       "\n",
       "     cg11236452  cg26916862  cg03124146  cg14770527  ...  cg11174855  \\\n",
       "0      0.533287    0.194961    0.144218    0.597436  ...    0.327951   \n",
       "1      0.578224    0.192382    0.139766    0.570267  ...    0.195741   \n",
       "2      0.509944    0.197505    0.193932    0.510173  ...    0.083195   \n",
       "3      0.421599    0.178132    0.181609    0.481978  ...    0.134511   \n",
       "4      0.381759    0.194651    0.170328    0.549200  ...    0.069399   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "643    0.409590    0.261340    0.202900    0.703690  ...    0.074460   \n",
       "644    0.468880    0.259820    0.240040    0.703450  ...    0.033430   \n",
       "645    0.579180    0.275920    0.234960    0.633500  ...    0.117130   \n",
       "646    0.421130    0.231630    0.297830    0.680370  ...    0.043240   \n",
       "647    0.478540    0.231610    0.272100    0.707310  ...    0.094330   \n",
       "\n",
       "     cg04524933  cg19418458  cg21808635  cg11359720  cg07635017  cg08641118  \\\n",
       "0      0.919347    0.365084    0.780742    0.731394    0.532118    0.081238   \n",
       "1      0.945236    0.337817    0.696424    0.745175    0.546033    0.099777   \n",
       "2      0.906838    0.357911    0.606163    0.773520    0.536528    0.079384   \n",
       "3      0.956986    0.583261    0.680310    0.753129    0.557829    0.067174   \n",
       "4      0.921778    0.333275    0.722998    0.824286    0.546425    0.075612   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "643    0.932110    0.309700    0.811640    0.651310    0.426310    0.106370   \n",
       "644    0.918230    0.273310    0.824450    0.609600    0.465400    0.121100   \n",
       "645    0.936460    0.436270    0.820790    0.597970    0.438150    0.134000   \n",
       "646    0.931800    0.296000    0.798780    0.609940    0.394220    0.143910   \n",
       "647    0.924410    0.299930    0.865710    0.703540    0.399760    0.157860   \n",
       "\n",
       "     cg09166085  cg22034735  cg17805624  \n",
       "0      0.108676    0.508394    0.279428  \n",
       "1      0.128168    0.498426    0.360946  \n",
       "2      0.091236    0.204013    0.296357  \n",
       "3      0.158550    0.297127    0.292479  \n",
       "4      0.178922    0.526409    0.278850  \n",
       "..          ...         ...         ...  \n",
       "643    0.128560    0.381660    0.148490  \n",
       "644    0.089490    0.315100    0.114360  \n",
       "645    0.214790    0.261410    0.127200  \n",
       "646    0.143810    0.566440    0.156560  \n",
       "647    0.165840    0.629880    0.119080  \n",
       "\n",
       "[2630 rows x 834 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2650 * 834\n",
    "train_data2 = pd.concat([data_ERISK, data_BSGS, data_DENMARK, data_EMTAB])\n",
    "train_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cg22695986</th>\n",
       "      <th>cg01193368</th>\n",
       "      <th>cg22056094</th>\n",
       "      <th>cg06098368</th>\n",
       "      <th>cg08690094</th>\n",
       "      <th>cg11236452</th>\n",
       "      <th>cg26916862</th>\n",
       "      <th>cg03124146</th>\n",
       "      <th>cg14770527</th>\n",
       "      <th>...</th>\n",
       "      <th>cg11174855</th>\n",
       "      <th>cg04524933</th>\n",
       "      <th>cg19418458</th>\n",
       "      <th>cg21808635</th>\n",
       "      <th>cg11359720</th>\n",
       "      <th>cg07635017</th>\n",
       "      <th>cg08641118</th>\n",
       "      <th>cg09166085</th>\n",
       "      <th>cg22034735</th>\n",
       "      <th>cg17805624</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.233353</td>\n",
       "      <td>0.606732</td>\n",
       "      <td>0.730107</td>\n",
       "      <td>0.240143</td>\n",
       "      <td>0.561082</td>\n",
       "      <td>0.533287</td>\n",
       "      <td>0.194961</td>\n",
       "      <td>0.144218</td>\n",
       "      <td>0.597436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327951</td>\n",
       "      <td>0.919347</td>\n",
       "      <td>0.365084</td>\n",
       "      <td>0.780742</td>\n",
       "      <td>0.731394</td>\n",
       "      <td>0.532118</td>\n",
       "      <td>0.081238</td>\n",
       "      <td>0.108676</td>\n",
       "      <td>0.508394</td>\n",
       "      <td>0.279428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.284813</td>\n",
       "      <td>0.599726</td>\n",
       "      <td>0.715363</td>\n",
       "      <td>0.242588</td>\n",
       "      <td>0.564277</td>\n",
       "      <td>0.578224</td>\n",
       "      <td>0.192382</td>\n",
       "      <td>0.139766</td>\n",
       "      <td>0.570267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195741</td>\n",
       "      <td>0.945236</td>\n",
       "      <td>0.337817</td>\n",
       "      <td>0.696424</td>\n",
       "      <td>0.745175</td>\n",
       "      <td>0.546033</td>\n",
       "      <td>0.099777</td>\n",
       "      <td>0.128168</td>\n",
       "      <td>0.498426</td>\n",
       "      <td>0.360946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.206618</td>\n",
       "      <td>0.552816</td>\n",
       "      <td>0.572559</td>\n",
       "      <td>0.169127</td>\n",
       "      <td>0.541453</td>\n",
       "      <td>0.509944</td>\n",
       "      <td>0.197505</td>\n",
       "      <td>0.193932</td>\n",
       "      <td>0.510173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083195</td>\n",
       "      <td>0.906838</td>\n",
       "      <td>0.357911</td>\n",
       "      <td>0.606163</td>\n",
       "      <td>0.773520</td>\n",
       "      <td>0.536528</td>\n",
       "      <td>0.079384</td>\n",
       "      <td>0.091236</td>\n",
       "      <td>0.204013</td>\n",
       "      <td>0.296357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.203151</td>\n",
       "      <td>0.655871</td>\n",
       "      <td>0.391728</td>\n",
       "      <td>0.224729</td>\n",
       "      <td>0.480992</td>\n",
       "      <td>0.421599</td>\n",
       "      <td>0.178132</td>\n",
       "      <td>0.181609</td>\n",
       "      <td>0.481978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134511</td>\n",
       "      <td>0.956986</td>\n",
       "      <td>0.583261</td>\n",
       "      <td>0.680310</td>\n",
       "      <td>0.753129</td>\n",
       "      <td>0.557829</td>\n",
       "      <td>0.067174</td>\n",
       "      <td>0.158550</td>\n",
       "      <td>0.297127</td>\n",
       "      <td>0.292479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.266709</td>\n",
       "      <td>0.493554</td>\n",
       "      <td>0.395203</td>\n",
       "      <td>0.231550</td>\n",
       "      <td>0.474545</td>\n",
       "      <td>0.381759</td>\n",
       "      <td>0.194651</td>\n",
       "      <td>0.170328</td>\n",
       "      <td>0.549200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069399</td>\n",
       "      <td>0.921778</td>\n",
       "      <td>0.333275</td>\n",
       "      <td>0.722998</td>\n",
       "      <td>0.824286</td>\n",
       "      <td>0.546425</td>\n",
       "      <td>0.075612</td>\n",
       "      <td>0.178922</td>\n",
       "      <td>0.526409</td>\n",
       "      <td>0.278850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.265920</td>\n",
       "      <td>0.668810</td>\n",
       "      <td>0.538530</td>\n",
       "      <td>0.221750</td>\n",
       "      <td>0.450940</td>\n",
       "      <td>0.409590</td>\n",
       "      <td>0.261340</td>\n",
       "      <td>0.202900</td>\n",
       "      <td>0.703690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074460</td>\n",
       "      <td>0.932110</td>\n",
       "      <td>0.309700</td>\n",
       "      <td>0.811640</td>\n",
       "      <td>0.651310</td>\n",
       "      <td>0.426310</td>\n",
       "      <td>0.106370</td>\n",
       "      <td>0.128560</td>\n",
       "      <td>0.381660</td>\n",
       "      <td>0.148490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.264220</td>\n",
       "      <td>0.673100</td>\n",
       "      <td>0.552430</td>\n",
       "      <td>0.211480</td>\n",
       "      <td>0.484940</td>\n",
       "      <td>0.468880</td>\n",
       "      <td>0.259820</td>\n",
       "      <td>0.240040</td>\n",
       "      <td>0.703450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033430</td>\n",
       "      <td>0.918230</td>\n",
       "      <td>0.273310</td>\n",
       "      <td>0.824450</td>\n",
       "      <td>0.609600</td>\n",
       "      <td>0.465400</td>\n",
       "      <td>0.121100</td>\n",
       "      <td>0.089490</td>\n",
       "      <td>0.315100</td>\n",
       "      <td>0.114360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.300860</td>\n",
       "      <td>0.586160</td>\n",
       "      <td>0.691140</td>\n",
       "      <td>0.240640</td>\n",
       "      <td>0.481040</td>\n",
       "      <td>0.579180</td>\n",
       "      <td>0.275920</td>\n",
       "      <td>0.234960</td>\n",
       "      <td>0.633500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117130</td>\n",
       "      <td>0.936460</td>\n",
       "      <td>0.436270</td>\n",
       "      <td>0.820790</td>\n",
       "      <td>0.597970</td>\n",
       "      <td>0.438150</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>0.214790</td>\n",
       "      <td>0.261410</td>\n",
       "      <td>0.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.289540</td>\n",
       "      <td>0.679960</td>\n",
       "      <td>0.553150</td>\n",
       "      <td>0.286020</td>\n",
       "      <td>0.315550</td>\n",
       "      <td>0.421130</td>\n",
       "      <td>0.231630</td>\n",
       "      <td>0.297830</td>\n",
       "      <td>0.680370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043240</td>\n",
       "      <td>0.931800</td>\n",
       "      <td>0.296000</td>\n",
       "      <td>0.798780</td>\n",
       "      <td>0.609940</td>\n",
       "      <td>0.394220</td>\n",
       "      <td>0.143910</td>\n",
       "      <td>0.143810</td>\n",
       "      <td>0.566440</td>\n",
       "      <td>0.156560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.255850</td>\n",
       "      <td>0.591080</td>\n",
       "      <td>0.525270</td>\n",
       "      <td>0.268260</td>\n",
       "      <td>0.343080</td>\n",
       "      <td>0.478540</td>\n",
       "      <td>0.231610</td>\n",
       "      <td>0.272100</td>\n",
       "      <td>0.707310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094330</td>\n",
       "      <td>0.924410</td>\n",
       "      <td>0.299930</td>\n",
       "      <td>0.865710</td>\n",
       "      <td>0.703540</td>\n",
       "      <td>0.399760</td>\n",
       "      <td>0.157860</td>\n",
       "      <td>0.165840</td>\n",
       "      <td>0.629880</td>\n",
       "      <td>0.119080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2630 rows × 832 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  cg22695986  cg01193368  cg22056094  cg06098368  cg08690094  \\\n",
       "0      1.0    0.233353    0.606732    0.730107    0.240143    0.561082   \n",
       "1      1.0    0.284813    0.599726    0.715363    0.242588    0.564277   \n",
       "2      1.0    0.206618    0.552816    0.572559    0.169127    0.541453   \n",
       "3      1.0    0.203151    0.655871    0.391728    0.224729    0.480992   \n",
       "4      0.0    0.266709    0.493554    0.395203    0.231550    0.474545   \n",
       "..     ...         ...         ...         ...         ...         ...   \n",
       "643    1.0    0.265920    0.668810    0.538530    0.221750    0.450940   \n",
       "644    1.0    0.264220    0.673100    0.552430    0.211480    0.484940   \n",
       "645    0.0    0.300860    0.586160    0.691140    0.240640    0.481040   \n",
       "646    0.0    0.289540    0.679960    0.553150    0.286020    0.315550   \n",
       "647    0.0    0.255850    0.591080    0.525270    0.268260    0.343080   \n",
       "\n",
       "     cg11236452  cg26916862  cg03124146  cg14770527  ...  cg11174855  \\\n",
       "0      0.533287    0.194961    0.144218    0.597436  ...    0.327951   \n",
       "1      0.578224    0.192382    0.139766    0.570267  ...    0.195741   \n",
       "2      0.509944    0.197505    0.193932    0.510173  ...    0.083195   \n",
       "3      0.421599    0.178132    0.181609    0.481978  ...    0.134511   \n",
       "4      0.381759    0.194651    0.170328    0.549200  ...    0.069399   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "643    0.409590    0.261340    0.202900    0.703690  ...    0.074460   \n",
       "644    0.468880    0.259820    0.240040    0.703450  ...    0.033430   \n",
       "645    0.579180    0.275920    0.234960    0.633500  ...    0.117130   \n",
       "646    0.421130    0.231630    0.297830    0.680370  ...    0.043240   \n",
       "647    0.478540    0.231610    0.272100    0.707310  ...    0.094330   \n",
       "\n",
       "     cg04524933  cg19418458  cg21808635  cg11359720  cg07635017  cg08641118  \\\n",
       "0      0.919347    0.365084    0.780742    0.731394    0.532118    0.081238   \n",
       "1      0.945236    0.337817    0.696424    0.745175    0.546033    0.099777   \n",
       "2      0.906838    0.357911    0.606163    0.773520    0.536528    0.079384   \n",
       "3      0.956986    0.583261    0.680310    0.753129    0.557829    0.067174   \n",
       "4      0.921778    0.333275    0.722998    0.824286    0.546425    0.075612   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "643    0.932110    0.309700    0.811640    0.651310    0.426310    0.106370   \n",
       "644    0.918230    0.273310    0.824450    0.609600    0.465400    0.121100   \n",
       "645    0.936460    0.436270    0.820790    0.597970    0.438150    0.134000   \n",
       "646    0.931800    0.296000    0.798780    0.609940    0.394220    0.143910   \n",
       "647    0.924410    0.299930    0.865710    0.703540    0.399760    0.157860   \n",
       "\n",
       "     cg09166085  cg22034735  cg17805624  \n",
       "0      0.108676    0.508394    0.279428  \n",
       "1      0.128168    0.498426    0.360946  \n",
       "2      0.091236    0.204013    0.296357  \n",
       "3      0.158550    0.297127    0.292479  \n",
       "4      0.178922    0.526409    0.278850  \n",
       "..          ...         ...         ...  \n",
       "643    0.128560    0.381660    0.148490  \n",
       "644    0.089490    0.315100    0.114360  \n",
       "645    0.214790    0.261410    0.127200  \n",
       "646    0.143810    0.566440    0.156560  \n",
       "647    0.165840    0.629880    0.119080  \n",
       "\n",
       "[2630 rows x 832 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2650 * 832\n",
    "train_data2 = train_data2.loc[:,[i for i in data_AMDTSS.columns]]\n",
    "train_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1972, 831), (658, 831), (1972,), (658,))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training 75% developing 25%\n",
    "x_train2, x_dev2, y_train2, y_dev2 = train_test_split(train_data2.drop(columns=['label']), train_data2['label'])\n",
    "x_train2.shape, x_dev2.shape, y_train2.shape, y_dev2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Original Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7252640036730945"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en2 = LogisticRegression(penalty = \"elasticnet\", solver = \"saga\", l1_ratio = 0.5)\n",
    "en2.fit(x_train2, y_train2)\n",
    "y_AMDTSS = data_AMDTSS['label']\n",
    "x_AMDTSS = data_AMDTSS.drop(['label'], axis = 1)\n",
    "en_auc2 = roc_auc_score(y_AMDTSS, en2.predict_proba(x_AMDTSS)[:, 1])\n",
    "en_auc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train New Clssifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Selection by RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=50, min_impurity_decrease=1e-05,\n",
       "                       n_estimators=300)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid search for rf\n",
    "# The number of trees in the forest.\n",
    "n_estimators = [50, 100, 200, 300, 500]\n",
    "# The function to measure the quality of a split\n",
    "criterion = [\"gini\", \"entropy\"]\n",
    "# A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "min_impurity_decrease = [0.1, 0.000001, 0.00001]\n",
    "# The maximum depth of the tree.\n",
    "max_depth = [20, 50, 100, 500, 1000]\n",
    "\n",
    "param_distributions = dict(n_estimators = n_estimators, criterion = criterion, min_impurity_decrease = min_impurity_decrease, max_depth = max_depth)\n",
    "rf = RandomForestClassifier()\n",
    "grid_rf2 = RandomizedSearchCV(estimator = rf, param_distributions = param_distributions, scoring = \"roc_auc\",\n",
    "                          verbose = 1, n_jobs = -1) \n",
    "grid_result_rf2 = grid_rf2.fit(x_train2, y_train2) \n",
    "\n",
    "grid_result_rf2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=RandomForestClassifier(max_depth=50,\n",
       "                                                 min_impurity_decrease=1e-05,\n",
       "                                                 n_estimators=300))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variable selection by random forest\n",
    "rf_selection2 = SelectFromModel(grid_result_rf2.best_estimator_)\n",
    "rf_selection2.fit(x_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selected variables\n",
    "selected_feat_rf2 = x_train2.columns[(rf_selection2.get_support())]\n",
    "len(selected_feat_rf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cg22695986</th>\n",
       "      <th>cg01193368</th>\n",
       "      <th>cg22056094</th>\n",
       "      <th>cg06098368</th>\n",
       "      <th>cg08690094</th>\n",
       "      <th>cg11236452</th>\n",
       "      <th>cg26916862</th>\n",
       "      <th>cg03124146</th>\n",
       "      <th>cg14770527</th>\n",
       "      <th>cg23316599</th>\n",
       "      <th>...</th>\n",
       "      <th>cg25787956</th>\n",
       "      <th>cg04182912</th>\n",
       "      <th>cg09009380</th>\n",
       "      <th>cg09990584</th>\n",
       "      <th>cg07291889</th>\n",
       "      <th>cg21808635</th>\n",
       "      <th>cg07635017</th>\n",
       "      <th>cg08641118</th>\n",
       "      <th>cg17805624</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.233353</td>\n",
       "      <td>0.606732</td>\n",
       "      <td>0.730107</td>\n",
       "      <td>0.240143</td>\n",
       "      <td>0.561082</td>\n",
       "      <td>0.533287</td>\n",
       "      <td>0.194961</td>\n",
       "      <td>0.144218</td>\n",
       "      <td>0.597436</td>\n",
       "      <td>0.396608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.503054</td>\n",
       "      <td>0.588122</td>\n",
       "      <td>0.323892</td>\n",
       "      <td>0.342347</td>\n",
       "      <td>0.706627</td>\n",
       "      <td>0.780742</td>\n",
       "      <td>0.532118</td>\n",
       "      <td>0.081238</td>\n",
       "      <td>0.279428</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.284813</td>\n",
       "      <td>0.599726</td>\n",
       "      <td>0.715363</td>\n",
       "      <td>0.242588</td>\n",
       "      <td>0.564277</td>\n",
       "      <td>0.578224</td>\n",
       "      <td>0.192382</td>\n",
       "      <td>0.139766</td>\n",
       "      <td>0.570267</td>\n",
       "      <td>0.419441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.466924</td>\n",
       "      <td>0.645375</td>\n",
       "      <td>0.315845</td>\n",
       "      <td>0.321944</td>\n",
       "      <td>0.740209</td>\n",
       "      <td>0.696424</td>\n",
       "      <td>0.546033</td>\n",
       "      <td>0.099777</td>\n",
       "      <td>0.360946</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.206618</td>\n",
       "      <td>0.552816</td>\n",
       "      <td>0.572559</td>\n",
       "      <td>0.169127</td>\n",
       "      <td>0.541453</td>\n",
       "      <td>0.509944</td>\n",
       "      <td>0.197505</td>\n",
       "      <td>0.193932</td>\n",
       "      <td>0.510173</td>\n",
       "      <td>0.479463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.441580</td>\n",
       "      <td>0.689910</td>\n",
       "      <td>0.313867</td>\n",
       "      <td>0.327677</td>\n",
       "      <td>0.643731</td>\n",
       "      <td>0.606163</td>\n",
       "      <td>0.536528</td>\n",
       "      <td>0.079384</td>\n",
       "      <td>0.296357</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.203151</td>\n",
       "      <td>0.655871</td>\n",
       "      <td>0.391728</td>\n",
       "      <td>0.224729</td>\n",
       "      <td>0.480992</td>\n",
       "      <td>0.421599</td>\n",
       "      <td>0.178132</td>\n",
       "      <td>0.181609</td>\n",
       "      <td>0.481978</td>\n",
       "      <td>0.510254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.476676</td>\n",
       "      <td>0.677442</td>\n",
       "      <td>0.300121</td>\n",
       "      <td>0.303911</td>\n",
       "      <td>0.709617</td>\n",
       "      <td>0.680310</td>\n",
       "      <td>0.557829</td>\n",
       "      <td>0.067174</td>\n",
       "      <td>0.292479</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.266709</td>\n",
       "      <td>0.493554</td>\n",
       "      <td>0.395203</td>\n",
       "      <td>0.231550</td>\n",
       "      <td>0.474545</td>\n",
       "      <td>0.381759</td>\n",
       "      <td>0.194651</td>\n",
       "      <td>0.170328</td>\n",
       "      <td>0.549200</td>\n",
       "      <td>0.506738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.493368</td>\n",
       "      <td>0.565591</td>\n",
       "      <td>0.313593</td>\n",
       "      <td>0.333914</td>\n",
       "      <td>0.727944</td>\n",
       "      <td>0.722998</td>\n",
       "      <td>0.546425</td>\n",
       "      <td>0.075612</td>\n",
       "      <td>0.278850</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>0.265920</td>\n",
       "      <td>0.668810</td>\n",
       "      <td>0.538530</td>\n",
       "      <td>0.221750</td>\n",
       "      <td>0.450940</td>\n",
       "      <td>0.409590</td>\n",
       "      <td>0.261340</td>\n",
       "      <td>0.202900</td>\n",
       "      <td>0.703690</td>\n",
       "      <td>0.427100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556560</td>\n",
       "      <td>0.634700</td>\n",
       "      <td>0.281210</td>\n",
       "      <td>0.423670</td>\n",
       "      <td>0.846850</td>\n",
       "      <td>0.811640</td>\n",
       "      <td>0.426310</td>\n",
       "      <td>0.106370</td>\n",
       "      <td>0.148490</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>0.264220</td>\n",
       "      <td>0.673100</td>\n",
       "      <td>0.552430</td>\n",
       "      <td>0.211480</td>\n",
       "      <td>0.484940</td>\n",
       "      <td>0.468880</td>\n",
       "      <td>0.259820</td>\n",
       "      <td>0.240040</td>\n",
       "      <td>0.703450</td>\n",
       "      <td>0.470970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517510</td>\n",
       "      <td>0.617870</td>\n",
       "      <td>0.277860</td>\n",
       "      <td>0.421900</td>\n",
       "      <td>0.806160</td>\n",
       "      <td>0.824450</td>\n",
       "      <td>0.465400</td>\n",
       "      <td>0.121100</td>\n",
       "      <td>0.114360</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>0.300860</td>\n",
       "      <td>0.586160</td>\n",
       "      <td>0.691140</td>\n",
       "      <td>0.240640</td>\n",
       "      <td>0.481040</td>\n",
       "      <td>0.579180</td>\n",
       "      <td>0.275920</td>\n",
       "      <td>0.234960</td>\n",
       "      <td>0.633500</td>\n",
       "      <td>0.356980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.493220</td>\n",
       "      <td>0.592040</td>\n",
       "      <td>0.311790</td>\n",
       "      <td>0.427700</td>\n",
       "      <td>0.797160</td>\n",
       "      <td>0.820790</td>\n",
       "      <td>0.438150</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>0.127200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>0.289540</td>\n",
       "      <td>0.679960</td>\n",
       "      <td>0.553150</td>\n",
       "      <td>0.286020</td>\n",
       "      <td>0.315550</td>\n",
       "      <td>0.421130</td>\n",
       "      <td>0.231630</td>\n",
       "      <td>0.297830</td>\n",
       "      <td>0.680370</td>\n",
       "      <td>0.273990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.479540</td>\n",
       "      <td>0.627130</td>\n",
       "      <td>0.294900</td>\n",
       "      <td>0.414230</td>\n",
       "      <td>0.841330</td>\n",
       "      <td>0.798780</td>\n",
       "      <td>0.394220</td>\n",
       "      <td>0.143910</td>\n",
       "      <td>0.156560</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>0.255850</td>\n",
       "      <td>0.591080</td>\n",
       "      <td>0.525270</td>\n",
       "      <td>0.268260</td>\n",
       "      <td>0.343080</td>\n",
       "      <td>0.478540</td>\n",
       "      <td>0.231610</td>\n",
       "      <td>0.272100</td>\n",
       "      <td>0.707310</td>\n",
       "      <td>0.350310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500320</td>\n",
       "      <td>0.628200</td>\n",
       "      <td>0.279540</td>\n",
       "      <td>0.440560</td>\n",
       "      <td>0.829100</td>\n",
       "      <td>0.865710</td>\n",
       "      <td>0.399760</td>\n",
       "      <td>0.157860</td>\n",
       "      <td>0.119080</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2630 rows × 281 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cg22695986  cg01193368  cg22056094  cg06098368  cg08690094  cg11236452  \\\n",
       "0      0.233353    0.606732    0.730107    0.240143    0.561082    0.533287   \n",
       "1      0.284813    0.599726    0.715363    0.242588    0.564277    0.578224   \n",
       "2      0.206618    0.552816    0.572559    0.169127    0.541453    0.509944   \n",
       "3      0.203151    0.655871    0.391728    0.224729    0.480992    0.421599   \n",
       "4      0.266709    0.493554    0.395203    0.231550    0.474545    0.381759   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "643    0.265920    0.668810    0.538530    0.221750    0.450940    0.409590   \n",
       "644    0.264220    0.673100    0.552430    0.211480    0.484940    0.468880   \n",
       "645    0.300860    0.586160    0.691140    0.240640    0.481040    0.579180   \n",
       "646    0.289540    0.679960    0.553150    0.286020    0.315550    0.421130   \n",
       "647    0.255850    0.591080    0.525270    0.268260    0.343080    0.478540   \n",
       "\n",
       "     cg26916862  cg03124146  cg14770527  cg23316599  ...  cg25787956  \\\n",
       "0      0.194961    0.144218    0.597436    0.396608  ...    0.503054   \n",
       "1      0.192382    0.139766    0.570267    0.419441  ...    0.466924   \n",
       "2      0.197505    0.193932    0.510173    0.479463  ...    0.441580   \n",
       "3      0.178132    0.181609    0.481978    0.510254  ...    0.476676   \n",
       "4      0.194651    0.170328    0.549200    0.506738  ...    0.493368   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "643    0.261340    0.202900    0.703690    0.427100  ...    0.556560   \n",
       "644    0.259820    0.240040    0.703450    0.470970  ...    0.517510   \n",
       "645    0.275920    0.234960    0.633500    0.356980  ...    0.493220   \n",
       "646    0.231630    0.297830    0.680370    0.273990  ...    0.479540   \n",
       "647    0.231610    0.272100    0.707310    0.350310  ...    0.500320   \n",
       "\n",
       "     cg04182912  cg09009380  cg09990584  cg07291889  cg21808635  cg07635017  \\\n",
       "0      0.588122    0.323892    0.342347    0.706627    0.780742    0.532118   \n",
       "1      0.645375    0.315845    0.321944    0.740209    0.696424    0.546033   \n",
       "2      0.689910    0.313867    0.327677    0.643731    0.606163    0.536528   \n",
       "3      0.677442    0.300121    0.303911    0.709617    0.680310    0.557829   \n",
       "4      0.565591    0.313593    0.333914    0.727944    0.722998    0.546425   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "643    0.634700    0.281210    0.423670    0.846850    0.811640    0.426310   \n",
       "644    0.617870    0.277860    0.421900    0.806160    0.824450    0.465400   \n",
       "645    0.592040    0.311790    0.427700    0.797160    0.820790    0.438150   \n",
       "646    0.627130    0.294900    0.414230    0.841330    0.798780    0.394220   \n",
       "647    0.628200    0.279540    0.440560    0.829100    0.865710    0.399760   \n",
       "\n",
       "     cg08641118  cg17805624  label  \n",
       "0      0.081238    0.279428    1.0  \n",
       "1      0.099777    0.360946    1.0  \n",
       "2      0.079384    0.296357    1.0  \n",
       "3      0.067174    0.292479    1.0  \n",
       "4      0.075612    0.278850    0.0  \n",
       "..          ...         ...    ...  \n",
       "643    0.106370    0.148490    1.0  \n",
       "644    0.121100    0.114360    1.0  \n",
       "645    0.134000    0.127200    0.0  \n",
       "646    0.143910    0.156560    0.0  \n",
       "647    0.157860    0.119080    0.0  \n",
       "\n",
       "[2630 rows x 281 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_selected_train_data2 = train_data2.loc[:,[i for i in selected_feat_rf2]]\n",
    "rf_selected_train_data2['label'] = train_data2['label'] \n",
    "rf_selected_train_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1972, 280), (658, 280), (1972,), (658,))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training 75% developing 25%\n",
    "x_train_rf2, x_dev_rf2, y_train_rf2, y_dev_rf2 = train_test_split(rf_selected_train_data2.drop(columns=['label']), rf_selected_train_data2['label'])\n",
    "x_train_rf2.shape, x_dev_rf2.shape, y_train_rf2.shape, y_dev_rf2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8438077548606491"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_selected2 = grid_result_rf2.best_estimator_\n",
    "rf_selected2.fit(x_train_rf2, y_train_rf2)\n",
    "rf_selected_auc2 = roc_auc_score(y_dev_rf2, rf_selected2.predict_proba(x_dev_rf2)[:, 1])\n",
    "rf_selected_auc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_AMDTSS_rf_selected = data_AMDTSS['label']\n",
    "x_AMDTSS_rf_selected = data_AMDTSS.loc[:,[i for i in selected_feat_rf2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6046544995408633"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_selected_auc_AMDTSS = roc_auc_score(y_AMDTSS_rf_selected, rf_selected2.predict_proba(x_AMDTSS_rf_selected)[:, 1])\n",
    "rf_selected_auc_AMDTSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7148186409550047"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking_rf_AMDTSS = get_stacking()\n",
    "stacking_rf_AMDTSS.fit(x_train_rf2, y_train_rf2)\n",
    "stacking_rf_AMDTSS_auc = roc_auc_score(y_AMDTSS_rf_selected, stacking_rf_AMDTSS.predict_proba(x_AMDTSS_rf_selected)[:, 1])\n",
    "stacking_rf_AMDTSS_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6966253443526171"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_rf_AMDTSS = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gb', clf3), ('svc', clf4)], voting='soft')\n",
    "voting_rf_AMDTSS.fit(x_train_rf2, y_train_rf2)\n",
    "voting__rf_AMDTSS_auc = roc_auc_score(y_AMDTSS_rf_selected, voting_rf_AMDTSS.predict_proba(x_AMDTSS_rf_selected)[:, 1])\n",
    "voting__rf_AMDTSS_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Selection by LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 450, in _check_solver\n",
      "    raise ValueError(\"Only 'saga' solver supports elasticnet penalty,\"\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 450, in _check_solver\n",
      "    raise ValueError(\"Only 'saga' solver supports elasticnet penalty,\"\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 450, in _check_solver\n",
      "    raise ValueError(\"Only 'saga' solver supports elasticnet penalty,\"\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 450, in _check_solver\n",
      "    raise ValueError(\"Only 'saga' solver supports elasticnet penalty,\"\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 450, in _check_solver\n",
      "    raise ValueError(\"Only 'saga' solver supports elasticnet penalty,\"\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1406, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 790, in _logistic_regression_path\n",
      "    beta = 1. / C\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1406, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 790, in _logistic_regression_path\n",
      "    beta = 1. / C\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1406, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 790, in _logistic_regression_path\n",
      "    beta = 1. / C\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1406, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 790, in _logistic_regression_path\n",
      "    beta = 1. / C\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1406, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 222, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 790, in _logistic_regression_path\n",
      "    beta = 1. / C\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan 0.80891233        nan        nan\n",
      " 0.80276232        nan        nan 0.80869556]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.5, solver='newton-cg', tol=1e-07)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid search for lr\n",
    "penalty = [\"l1\", \"l2\", \"elasticnet\"]\n",
    "# Tolerance for stopping criteria.\n",
    "tol = [0.00001, 0.001, 0.0000001]\n",
    "# Inverse of regularization strength\n",
    "C = [0,1, 0.5, 1, 10, 20, 30, 50, 100]\n",
    "# Algorithm to use in the optimization problem\n",
    "solver = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "\n",
    "\n",
    "param_distributions = dict(penalty = penalty, tol = tol, C = C, solver = solver)\n",
    "lr = LogisticRegression()\n",
    "grid_lr2 = RandomizedSearchCV(estimator = lr, param_distributions = param_distributions, scoring = \"roc_auc\",\n",
    "                          verbose = 1, n_jobs = -1) \n",
    "grid_result_lr2 = grid_lr2.fit(x_train2, y_train2) \n",
    "\n",
    "grid_result_lr2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=LogisticRegression(C=0.5, solver='newton-cg',\n",
       "                                             tol=1e-07))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_selection2 = SelectFromModel(grid_result_lr2.best_estimator_)\n",
    "lr_selection2.fit(x_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_feat_lr2 = x_train2.columns[(lr_selection2.get_support())]\n",
    "len(selected_feat_lr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cg01193368</th>\n",
       "      <th>cg22056094</th>\n",
       "      <th>cg06098368</th>\n",
       "      <th>cg08690094</th>\n",
       "      <th>cg11236452</th>\n",
       "      <th>cg03124146</th>\n",
       "      <th>cg14770527</th>\n",
       "      <th>cg23316599</th>\n",
       "      <th>cg11108474</th>\n",
       "      <th>cg26262573</th>\n",
       "      <th>...</th>\n",
       "      <th>cg09990584</th>\n",
       "      <th>cg06495631</th>\n",
       "      <th>cg15236528</th>\n",
       "      <th>cg07291889</th>\n",
       "      <th>cg21808635</th>\n",
       "      <th>cg07635017</th>\n",
       "      <th>cg08641118</th>\n",
       "      <th>cg22034735</th>\n",
       "      <th>cg17805624</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.606732</td>\n",
       "      <td>0.730107</td>\n",
       "      <td>0.240143</td>\n",
       "      <td>0.561082</td>\n",
       "      <td>0.533287</td>\n",
       "      <td>0.144218</td>\n",
       "      <td>0.597436</td>\n",
       "      <td>0.396608</td>\n",
       "      <td>0.479727</td>\n",
       "      <td>0.367165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342347</td>\n",
       "      <td>0.570667</td>\n",
       "      <td>0.156715</td>\n",
       "      <td>0.706627</td>\n",
       "      <td>0.780742</td>\n",
       "      <td>0.532118</td>\n",
       "      <td>0.081238</td>\n",
       "      <td>0.508394</td>\n",
       "      <td>0.279428</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.599726</td>\n",
       "      <td>0.715363</td>\n",
       "      <td>0.242588</td>\n",
       "      <td>0.564277</td>\n",
       "      <td>0.578224</td>\n",
       "      <td>0.139766</td>\n",
       "      <td>0.570267</td>\n",
       "      <td>0.419441</td>\n",
       "      <td>0.483118</td>\n",
       "      <td>0.364813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.321944</td>\n",
       "      <td>0.589975</td>\n",
       "      <td>0.220170</td>\n",
       "      <td>0.740209</td>\n",
       "      <td>0.696424</td>\n",
       "      <td>0.546033</td>\n",
       "      <td>0.099777</td>\n",
       "      <td>0.498426</td>\n",
       "      <td>0.360946</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.552816</td>\n",
       "      <td>0.572559</td>\n",
       "      <td>0.169127</td>\n",
       "      <td>0.541453</td>\n",
       "      <td>0.509944</td>\n",
       "      <td>0.193932</td>\n",
       "      <td>0.510173</td>\n",
       "      <td>0.479463</td>\n",
       "      <td>0.368669</td>\n",
       "      <td>0.334221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327677</td>\n",
       "      <td>0.507627</td>\n",
       "      <td>0.178511</td>\n",
       "      <td>0.643731</td>\n",
       "      <td>0.606163</td>\n",
       "      <td>0.536528</td>\n",
       "      <td>0.079384</td>\n",
       "      <td>0.204013</td>\n",
       "      <td>0.296357</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.655871</td>\n",
       "      <td>0.391728</td>\n",
       "      <td>0.224729</td>\n",
       "      <td>0.480992</td>\n",
       "      <td>0.421599</td>\n",
       "      <td>0.181609</td>\n",
       "      <td>0.481978</td>\n",
       "      <td>0.510254</td>\n",
       "      <td>0.360383</td>\n",
       "      <td>0.352629</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303911</td>\n",
       "      <td>0.507012</td>\n",
       "      <td>0.086153</td>\n",
       "      <td>0.709617</td>\n",
       "      <td>0.680310</td>\n",
       "      <td>0.557829</td>\n",
       "      <td>0.067174</td>\n",
       "      <td>0.297127</td>\n",
       "      <td>0.292479</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.493554</td>\n",
       "      <td>0.395203</td>\n",
       "      <td>0.231550</td>\n",
       "      <td>0.474545</td>\n",
       "      <td>0.381759</td>\n",
       "      <td>0.170328</td>\n",
       "      <td>0.549200</td>\n",
       "      <td>0.506738</td>\n",
       "      <td>0.401675</td>\n",
       "      <td>0.441518</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333914</td>\n",
       "      <td>0.673096</td>\n",
       "      <td>0.137047</td>\n",
       "      <td>0.727944</td>\n",
       "      <td>0.722998</td>\n",
       "      <td>0.546425</td>\n",
       "      <td>0.075612</td>\n",
       "      <td>0.526409</td>\n",
       "      <td>0.278850</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>0.668810</td>\n",
       "      <td>0.538530</td>\n",
       "      <td>0.221750</td>\n",
       "      <td>0.450940</td>\n",
       "      <td>0.409590</td>\n",
       "      <td>0.202900</td>\n",
       "      <td>0.703690</td>\n",
       "      <td>0.427100</td>\n",
       "      <td>0.379050</td>\n",
       "      <td>0.481300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423670</td>\n",
       "      <td>0.362890</td>\n",
       "      <td>0.032750</td>\n",
       "      <td>0.846850</td>\n",
       "      <td>0.811640</td>\n",
       "      <td>0.426310</td>\n",
       "      <td>0.106370</td>\n",
       "      <td>0.381660</td>\n",
       "      <td>0.148490</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>0.673100</td>\n",
       "      <td>0.552430</td>\n",
       "      <td>0.211480</td>\n",
       "      <td>0.484940</td>\n",
       "      <td>0.468880</td>\n",
       "      <td>0.240040</td>\n",
       "      <td>0.703450</td>\n",
       "      <td>0.470970</td>\n",
       "      <td>0.357540</td>\n",
       "      <td>0.524900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.421900</td>\n",
       "      <td>0.285190</td>\n",
       "      <td>0.034740</td>\n",
       "      <td>0.806160</td>\n",
       "      <td>0.824450</td>\n",
       "      <td>0.465400</td>\n",
       "      <td>0.121100</td>\n",
       "      <td>0.315100</td>\n",
       "      <td>0.114360</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>0.586160</td>\n",
       "      <td>0.691140</td>\n",
       "      <td>0.240640</td>\n",
       "      <td>0.481040</td>\n",
       "      <td>0.579180</td>\n",
       "      <td>0.234960</td>\n",
       "      <td>0.633500</td>\n",
       "      <td>0.356980</td>\n",
       "      <td>0.222150</td>\n",
       "      <td>0.374090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.427700</td>\n",
       "      <td>0.339370</td>\n",
       "      <td>0.025840</td>\n",
       "      <td>0.797160</td>\n",
       "      <td>0.820790</td>\n",
       "      <td>0.438150</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>0.261410</td>\n",
       "      <td>0.127200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>0.679960</td>\n",
       "      <td>0.553150</td>\n",
       "      <td>0.286020</td>\n",
       "      <td>0.315550</td>\n",
       "      <td>0.421130</td>\n",
       "      <td>0.297830</td>\n",
       "      <td>0.680370</td>\n",
       "      <td>0.273990</td>\n",
       "      <td>0.159820</td>\n",
       "      <td>0.345570</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414230</td>\n",
       "      <td>0.327810</td>\n",
       "      <td>0.025380</td>\n",
       "      <td>0.841330</td>\n",
       "      <td>0.798780</td>\n",
       "      <td>0.394220</td>\n",
       "      <td>0.143910</td>\n",
       "      <td>0.566440</td>\n",
       "      <td>0.156560</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>0.591080</td>\n",
       "      <td>0.525270</td>\n",
       "      <td>0.268260</td>\n",
       "      <td>0.343080</td>\n",
       "      <td>0.478540</td>\n",
       "      <td>0.272100</td>\n",
       "      <td>0.707310</td>\n",
       "      <td>0.350310</td>\n",
       "      <td>0.157690</td>\n",
       "      <td>0.378280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440560</td>\n",
       "      <td>0.344550</td>\n",
       "      <td>0.045710</td>\n",
       "      <td>0.829100</td>\n",
       "      <td>0.865710</td>\n",
       "      <td>0.399760</td>\n",
       "      <td>0.157860</td>\n",
       "      <td>0.629880</td>\n",
       "      <td>0.119080</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2630 rows × 352 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cg01193368  cg22056094  cg06098368  cg08690094  cg11236452  cg03124146  \\\n",
       "0      0.606732    0.730107    0.240143    0.561082    0.533287    0.144218   \n",
       "1      0.599726    0.715363    0.242588    0.564277    0.578224    0.139766   \n",
       "2      0.552816    0.572559    0.169127    0.541453    0.509944    0.193932   \n",
       "3      0.655871    0.391728    0.224729    0.480992    0.421599    0.181609   \n",
       "4      0.493554    0.395203    0.231550    0.474545    0.381759    0.170328   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "643    0.668810    0.538530    0.221750    0.450940    0.409590    0.202900   \n",
       "644    0.673100    0.552430    0.211480    0.484940    0.468880    0.240040   \n",
       "645    0.586160    0.691140    0.240640    0.481040    0.579180    0.234960   \n",
       "646    0.679960    0.553150    0.286020    0.315550    0.421130    0.297830   \n",
       "647    0.591080    0.525270    0.268260    0.343080    0.478540    0.272100   \n",
       "\n",
       "     cg14770527  cg23316599  cg11108474  cg26262573  ...  cg09990584  \\\n",
       "0      0.597436    0.396608    0.479727    0.367165  ...    0.342347   \n",
       "1      0.570267    0.419441    0.483118    0.364813  ...    0.321944   \n",
       "2      0.510173    0.479463    0.368669    0.334221  ...    0.327677   \n",
       "3      0.481978    0.510254    0.360383    0.352629  ...    0.303911   \n",
       "4      0.549200    0.506738    0.401675    0.441518  ...    0.333914   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "643    0.703690    0.427100    0.379050    0.481300  ...    0.423670   \n",
       "644    0.703450    0.470970    0.357540    0.524900  ...    0.421900   \n",
       "645    0.633500    0.356980    0.222150    0.374090  ...    0.427700   \n",
       "646    0.680370    0.273990    0.159820    0.345570  ...    0.414230   \n",
       "647    0.707310    0.350310    0.157690    0.378280  ...    0.440560   \n",
       "\n",
       "     cg06495631  cg15236528  cg07291889  cg21808635  cg07635017  cg08641118  \\\n",
       "0      0.570667    0.156715    0.706627    0.780742    0.532118    0.081238   \n",
       "1      0.589975    0.220170    0.740209    0.696424    0.546033    0.099777   \n",
       "2      0.507627    0.178511    0.643731    0.606163    0.536528    0.079384   \n",
       "3      0.507012    0.086153    0.709617    0.680310    0.557829    0.067174   \n",
       "4      0.673096    0.137047    0.727944    0.722998    0.546425    0.075612   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "643    0.362890    0.032750    0.846850    0.811640    0.426310    0.106370   \n",
       "644    0.285190    0.034740    0.806160    0.824450    0.465400    0.121100   \n",
       "645    0.339370    0.025840    0.797160    0.820790    0.438150    0.134000   \n",
       "646    0.327810    0.025380    0.841330    0.798780    0.394220    0.143910   \n",
       "647    0.344550    0.045710    0.829100    0.865710    0.399760    0.157860   \n",
       "\n",
       "     cg22034735  cg17805624  label  \n",
       "0      0.508394    0.279428    1.0  \n",
       "1      0.498426    0.360946    1.0  \n",
       "2      0.204013    0.296357    1.0  \n",
       "3      0.297127    0.292479    1.0  \n",
       "4      0.526409    0.278850    0.0  \n",
       "..          ...         ...    ...  \n",
       "643    0.381660    0.148490    1.0  \n",
       "644    0.315100    0.114360    1.0  \n",
       "645    0.261410    0.127200    0.0  \n",
       "646    0.566440    0.156560    0.0  \n",
       "647    0.629880    0.119080    0.0  \n",
       "\n",
       "[2630 rows x 352 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_selected_train_data2 = train_data2.loc[:,[i for i in selected_feat_lr2]]\n",
    "lr_selected_train_data2['label'] = train_data2['label'] \n",
    "lr_selected_train_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1972, 351), (658, 351), (1972,), (658,))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training 75% developing 25%\n",
    "x_train_lr2, x_dev_lr2, y_train_lr2, y_dev_lr2 = train_test_split(lr_selected_train_data2.drop(columns=['label']), lr_selected_train_data2['label'])\n",
    "x_train_lr2.shape, x_dev_lr2.shape, y_train_lr2.shape, y_dev_lr2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8356017556017555"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_selected2 = grid_result_lr2.best_estimator_\n",
    "lr_selected2.fit(x_train_lr2, y_train_lr2)\n",
    "lr_selected_auc2 = roc_auc_score(y_dev_lr2, lr_selected2.predict_proba(x_dev_lr2)[:, 1])\n",
    "lr_selected_auc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_AMDTSS_lr_selected = data_AMDTSS['label']\n",
    "x_AMDTSS_lr_selected = data_AMDTSS.loc[:,[i for i in selected_feat_lr2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6904269972451791"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_selected_auc_AMDTSS = roc_auc_score(y_AMDTSS_lr_selected, lr_selected2.predict_proba(x_AMDTSS_lr_selected)[:, 1])\n",
    "lr_selected_auc_AMDTSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6887052341597796"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking_lr_AMDTSS = get_stacking()\n",
    "stacking_lr_AMDTSS.fit(x_train_lr2, y_train_lr2)\n",
    "stacking_lr_AMDTSS_auc = roc_auc_score(y_AMDTSS_lr_selected, stacking_lr_AMDTSS.predict_proba(x_AMDTSS_lr_selected)[:, 1])\n",
    "stacking_lr_AMDTSS_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6836547291092746"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_lr_AMDTSS = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gb', clf3), ('svc', clf4)], voting='soft')\n",
    "voting_lr_AMDTSS.fit(x_train_lr2, y_train_lr2)\n",
    "voting__lr_AMDTSS_auc = roc_auc_score(y_AMDTSS_lr_selected, voting_lr_AMDTSS.predict_proba(x_AMDTSS_lr_selected)[:, 1])\n",
    "voting__lr_AMDTSS_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training: E-Risk, BSGS, AMDTSS, E-MTAB\n",
    "# Testing: Denmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cg22695986</th>\n",
       "      <th>cg01193368</th>\n",
       "      <th>cg22056094</th>\n",
       "      <th>cg06098368</th>\n",
       "      <th>cg08690094</th>\n",
       "      <th>cg11236452</th>\n",
       "      <th>cg26916862</th>\n",
       "      <th>cg03124146</th>\n",
       "      <th>cg14770527</th>\n",
       "      <th>...</th>\n",
       "      <th>cg11174855</th>\n",
       "      <th>cg04524933</th>\n",
       "      <th>cg19418458</th>\n",
       "      <th>cg21808635</th>\n",
       "      <th>cg11359720</th>\n",
       "      <th>cg07635017</th>\n",
       "      <th>cg08641118</th>\n",
       "      <th>cg09166085</th>\n",
       "      <th>cg22034735</th>\n",
       "      <th>cg17805624</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.233353</td>\n",
       "      <td>0.606732</td>\n",
       "      <td>0.730107</td>\n",
       "      <td>0.240143</td>\n",
       "      <td>0.561082</td>\n",
       "      <td>0.533287</td>\n",
       "      <td>0.194961</td>\n",
       "      <td>0.144218</td>\n",
       "      <td>0.597436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327951</td>\n",
       "      <td>0.919347</td>\n",
       "      <td>0.365084</td>\n",
       "      <td>0.780742</td>\n",
       "      <td>0.731394</td>\n",
       "      <td>0.532118</td>\n",
       "      <td>0.081238</td>\n",
       "      <td>0.108676</td>\n",
       "      <td>0.508394</td>\n",
       "      <td>0.279428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.284813</td>\n",
       "      <td>0.599726</td>\n",
       "      <td>0.715363</td>\n",
       "      <td>0.242588</td>\n",
       "      <td>0.564277</td>\n",
       "      <td>0.578224</td>\n",
       "      <td>0.192382</td>\n",
       "      <td>0.139766</td>\n",
       "      <td>0.570267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195741</td>\n",
       "      <td>0.945236</td>\n",
       "      <td>0.337817</td>\n",
       "      <td>0.696424</td>\n",
       "      <td>0.745175</td>\n",
       "      <td>0.546033</td>\n",
       "      <td>0.099777</td>\n",
       "      <td>0.128168</td>\n",
       "      <td>0.498426</td>\n",
       "      <td>0.360946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.206618</td>\n",
       "      <td>0.552816</td>\n",
       "      <td>0.572559</td>\n",
       "      <td>0.169127</td>\n",
       "      <td>0.541453</td>\n",
       "      <td>0.509944</td>\n",
       "      <td>0.197505</td>\n",
       "      <td>0.193932</td>\n",
       "      <td>0.510173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083195</td>\n",
       "      <td>0.906838</td>\n",
       "      <td>0.357911</td>\n",
       "      <td>0.606163</td>\n",
       "      <td>0.773520</td>\n",
       "      <td>0.536528</td>\n",
       "      <td>0.079384</td>\n",
       "      <td>0.091236</td>\n",
       "      <td>0.204013</td>\n",
       "      <td>0.296357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.203151</td>\n",
       "      <td>0.655871</td>\n",
       "      <td>0.391728</td>\n",
       "      <td>0.224729</td>\n",
       "      <td>0.480992</td>\n",
       "      <td>0.421599</td>\n",
       "      <td>0.178132</td>\n",
       "      <td>0.181609</td>\n",
       "      <td>0.481978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134511</td>\n",
       "      <td>0.956986</td>\n",
       "      <td>0.583261</td>\n",
       "      <td>0.680310</td>\n",
       "      <td>0.753129</td>\n",
       "      <td>0.557829</td>\n",
       "      <td>0.067174</td>\n",
       "      <td>0.158550</td>\n",
       "      <td>0.297127</td>\n",
       "      <td>0.292479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.266709</td>\n",
       "      <td>0.493554</td>\n",
       "      <td>0.395203</td>\n",
       "      <td>0.231550</td>\n",
       "      <td>0.474545</td>\n",
       "      <td>0.381759</td>\n",
       "      <td>0.194651</td>\n",
       "      <td>0.170328</td>\n",
       "      <td>0.549200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069399</td>\n",
       "      <td>0.921778</td>\n",
       "      <td>0.333275</td>\n",
       "      <td>0.722998</td>\n",
       "      <td>0.824286</td>\n",
       "      <td>0.546425</td>\n",
       "      <td>0.075612</td>\n",
       "      <td>0.178922</td>\n",
       "      <td>0.526409</td>\n",
       "      <td>0.278850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.265920</td>\n",
       "      <td>0.668810</td>\n",
       "      <td>0.538530</td>\n",
       "      <td>0.221750</td>\n",
       "      <td>0.450940</td>\n",
       "      <td>0.409590</td>\n",
       "      <td>0.261340</td>\n",
       "      <td>0.202900</td>\n",
       "      <td>0.703690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074460</td>\n",
       "      <td>0.932110</td>\n",
       "      <td>0.309700</td>\n",
       "      <td>0.811640</td>\n",
       "      <td>0.651310</td>\n",
       "      <td>0.426310</td>\n",
       "      <td>0.106370</td>\n",
       "      <td>0.128560</td>\n",
       "      <td>0.381660</td>\n",
       "      <td>0.148490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.264220</td>\n",
       "      <td>0.673100</td>\n",
       "      <td>0.552430</td>\n",
       "      <td>0.211480</td>\n",
       "      <td>0.484940</td>\n",
       "      <td>0.468880</td>\n",
       "      <td>0.259820</td>\n",
       "      <td>0.240040</td>\n",
       "      <td>0.703450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033430</td>\n",
       "      <td>0.918230</td>\n",
       "      <td>0.273310</td>\n",
       "      <td>0.824450</td>\n",
       "      <td>0.609600</td>\n",
       "      <td>0.465400</td>\n",
       "      <td>0.121100</td>\n",
       "      <td>0.089490</td>\n",
       "      <td>0.315100</td>\n",
       "      <td>0.114360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.300860</td>\n",
       "      <td>0.586160</td>\n",
       "      <td>0.691140</td>\n",
       "      <td>0.240640</td>\n",
       "      <td>0.481040</td>\n",
       "      <td>0.579180</td>\n",
       "      <td>0.275920</td>\n",
       "      <td>0.234960</td>\n",
       "      <td>0.633500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117130</td>\n",
       "      <td>0.936460</td>\n",
       "      <td>0.436270</td>\n",
       "      <td>0.820790</td>\n",
       "      <td>0.597970</td>\n",
       "      <td>0.438150</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>0.214790</td>\n",
       "      <td>0.261410</td>\n",
       "      <td>0.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.289540</td>\n",
       "      <td>0.679960</td>\n",
       "      <td>0.553150</td>\n",
       "      <td>0.286020</td>\n",
       "      <td>0.315550</td>\n",
       "      <td>0.421130</td>\n",
       "      <td>0.231630</td>\n",
       "      <td>0.297830</td>\n",
       "      <td>0.680370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043240</td>\n",
       "      <td>0.931800</td>\n",
       "      <td>0.296000</td>\n",
       "      <td>0.798780</td>\n",
       "      <td>0.609940</td>\n",
       "      <td>0.394220</td>\n",
       "      <td>0.143910</td>\n",
       "      <td>0.143810</td>\n",
       "      <td>0.566440</td>\n",
       "      <td>0.156560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.255850</td>\n",
       "      <td>0.591080</td>\n",
       "      <td>0.525270</td>\n",
       "      <td>0.268260</td>\n",
       "      <td>0.343080</td>\n",
       "      <td>0.478540</td>\n",
       "      <td>0.231610</td>\n",
       "      <td>0.272100</td>\n",
       "      <td>0.707310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094330</td>\n",
       "      <td>0.924410</td>\n",
       "      <td>0.299930</td>\n",
       "      <td>0.865710</td>\n",
       "      <td>0.703540</td>\n",
       "      <td>0.399760</td>\n",
       "      <td>0.157860</td>\n",
       "      <td>0.165840</td>\n",
       "      <td>0.629880</td>\n",
       "      <td>0.119080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2470 rows × 834 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  cg22695986  cg01193368  cg22056094  cg06098368  cg08690094  \\\n",
       "0      1.0    0.233353    0.606732    0.730107    0.240143    0.561082   \n",
       "1      1.0    0.284813    0.599726    0.715363    0.242588    0.564277   \n",
       "2      1.0    0.206618    0.552816    0.572559    0.169127    0.541453   \n",
       "3      1.0    0.203151    0.655871    0.391728    0.224729    0.480992   \n",
       "4      0.0    0.266709    0.493554    0.395203    0.231550    0.474545   \n",
       "..     ...         ...         ...         ...         ...         ...   \n",
       "643    1.0    0.265920    0.668810    0.538530    0.221750    0.450940   \n",
       "644    1.0    0.264220    0.673100    0.552430    0.211480    0.484940   \n",
       "645    0.0    0.300860    0.586160    0.691140    0.240640    0.481040   \n",
       "646    0.0    0.289540    0.679960    0.553150    0.286020    0.315550   \n",
       "647    0.0    0.255850    0.591080    0.525270    0.268260    0.343080   \n",
       "\n",
       "     cg11236452  cg26916862  cg03124146  cg14770527  ...  cg11174855  \\\n",
       "0      0.533287    0.194961    0.144218    0.597436  ...    0.327951   \n",
       "1      0.578224    0.192382    0.139766    0.570267  ...    0.195741   \n",
       "2      0.509944    0.197505    0.193932    0.510173  ...    0.083195   \n",
       "3      0.421599    0.178132    0.181609    0.481978  ...    0.134511   \n",
       "4      0.381759    0.194651    0.170328    0.549200  ...    0.069399   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "643    0.409590    0.261340    0.202900    0.703690  ...    0.074460   \n",
       "644    0.468880    0.259820    0.240040    0.703450  ...    0.033430   \n",
       "645    0.579180    0.275920    0.234960    0.633500  ...    0.117130   \n",
       "646    0.421130    0.231630    0.297830    0.680370  ...    0.043240   \n",
       "647    0.478540    0.231610    0.272100    0.707310  ...    0.094330   \n",
       "\n",
       "     cg04524933  cg19418458  cg21808635  cg11359720  cg07635017  cg08641118  \\\n",
       "0      0.919347    0.365084    0.780742    0.731394    0.532118    0.081238   \n",
       "1      0.945236    0.337817    0.696424    0.745175    0.546033    0.099777   \n",
       "2      0.906838    0.357911    0.606163    0.773520    0.536528    0.079384   \n",
       "3      0.956986    0.583261    0.680310    0.753129    0.557829    0.067174   \n",
       "4      0.921778    0.333275    0.722998    0.824286    0.546425    0.075612   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "643    0.932110    0.309700    0.811640    0.651310    0.426310    0.106370   \n",
       "644    0.918230    0.273310    0.824450    0.609600    0.465400    0.121100   \n",
       "645    0.936460    0.436270    0.820790    0.597970    0.438150    0.134000   \n",
       "646    0.931800    0.296000    0.798780    0.609940    0.394220    0.143910   \n",
       "647    0.924410    0.299930    0.865710    0.703540    0.399760    0.157860   \n",
       "\n",
       "     cg09166085  cg22034735  cg17805624  \n",
       "0      0.108676    0.508394    0.279428  \n",
       "1      0.128168    0.498426    0.360946  \n",
       "2      0.091236    0.204013    0.296357  \n",
       "3      0.158550    0.297127    0.292479  \n",
       "4      0.178922    0.526409    0.278850  \n",
       "..          ...         ...         ...  \n",
       "643    0.128560    0.381660    0.148490  \n",
       "644    0.089490    0.315100    0.114360  \n",
       "645    0.214790    0.261410    0.127200  \n",
       "646    0.143810    0.566440    0.156560  \n",
       "647    0.165840    0.629880    0.119080  \n",
       "\n",
       "[2470 rows x 834 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2470 * 834\n",
    "train_data3 = pd.concat([data_ERISK, data_BSGS, data_EMTAB])\n",
    "train_data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cg22695986</th>\n",
       "      <th>cg01193368</th>\n",
       "      <th>cg22056094</th>\n",
       "      <th>cg06098368</th>\n",
       "      <th>cg08690094</th>\n",
       "      <th>cg11236452</th>\n",
       "      <th>cg26916862</th>\n",
       "      <th>cg03124146</th>\n",
       "      <th>cg14770527</th>\n",
       "      <th>...</th>\n",
       "      <th>cg11174855</th>\n",
       "      <th>cg04524933</th>\n",
       "      <th>cg19418458</th>\n",
       "      <th>cg21808635</th>\n",
       "      <th>cg11359720</th>\n",
       "      <th>cg07635017</th>\n",
       "      <th>cg08641118</th>\n",
       "      <th>cg09166085</th>\n",
       "      <th>cg22034735</th>\n",
       "      <th>cg17805624</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.233353</td>\n",
       "      <td>0.606732</td>\n",
       "      <td>0.730107</td>\n",
       "      <td>0.240143</td>\n",
       "      <td>0.561082</td>\n",
       "      <td>0.533287</td>\n",
       "      <td>0.194961</td>\n",
       "      <td>0.144218</td>\n",
       "      <td>0.597436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327951</td>\n",
       "      <td>0.919347</td>\n",
       "      <td>0.365084</td>\n",
       "      <td>0.780742</td>\n",
       "      <td>0.731394</td>\n",
       "      <td>0.532118</td>\n",
       "      <td>0.081238</td>\n",
       "      <td>0.108676</td>\n",
       "      <td>0.508394</td>\n",
       "      <td>0.279428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.284813</td>\n",
       "      <td>0.599726</td>\n",
       "      <td>0.715363</td>\n",
       "      <td>0.242588</td>\n",
       "      <td>0.564277</td>\n",
       "      <td>0.578224</td>\n",
       "      <td>0.192382</td>\n",
       "      <td>0.139766</td>\n",
       "      <td>0.570267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195741</td>\n",
       "      <td>0.945236</td>\n",
       "      <td>0.337817</td>\n",
       "      <td>0.696424</td>\n",
       "      <td>0.745175</td>\n",
       "      <td>0.546033</td>\n",
       "      <td>0.099777</td>\n",
       "      <td>0.128168</td>\n",
       "      <td>0.498426</td>\n",
       "      <td>0.360946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.206618</td>\n",
       "      <td>0.552816</td>\n",
       "      <td>0.572559</td>\n",
       "      <td>0.169127</td>\n",
       "      <td>0.541453</td>\n",
       "      <td>0.509944</td>\n",
       "      <td>0.197505</td>\n",
       "      <td>0.193932</td>\n",
       "      <td>0.510173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083195</td>\n",
       "      <td>0.906838</td>\n",
       "      <td>0.357911</td>\n",
       "      <td>0.606163</td>\n",
       "      <td>0.773520</td>\n",
       "      <td>0.536528</td>\n",
       "      <td>0.079384</td>\n",
       "      <td>0.091236</td>\n",
       "      <td>0.204013</td>\n",
       "      <td>0.296357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.203151</td>\n",
       "      <td>0.655871</td>\n",
       "      <td>0.391728</td>\n",
       "      <td>0.224729</td>\n",
       "      <td>0.480992</td>\n",
       "      <td>0.421599</td>\n",
       "      <td>0.178132</td>\n",
       "      <td>0.181609</td>\n",
       "      <td>0.481978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134511</td>\n",
       "      <td>0.956986</td>\n",
       "      <td>0.583261</td>\n",
       "      <td>0.680310</td>\n",
       "      <td>0.753129</td>\n",
       "      <td>0.557829</td>\n",
       "      <td>0.067174</td>\n",
       "      <td>0.158550</td>\n",
       "      <td>0.297127</td>\n",
       "      <td>0.292479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.266709</td>\n",
       "      <td>0.493554</td>\n",
       "      <td>0.395203</td>\n",
       "      <td>0.231550</td>\n",
       "      <td>0.474545</td>\n",
       "      <td>0.381759</td>\n",
       "      <td>0.194651</td>\n",
       "      <td>0.170328</td>\n",
       "      <td>0.549200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069399</td>\n",
       "      <td>0.921778</td>\n",
       "      <td>0.333275</td>\n",
       "      <td>0.722998</td>\n",
       "      <td>0.824286</td>\n",
       "      <td>0.546425</td>\n",
       "      <td>0.075612</td>\n",
       "      <td>0.178922</td>\n",
       "      <td>0.526409</td>\n",
       "      <td>0.278850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.265920</td>\n",
       "      <td>0.668810</td>\n",
       "      <td>0.538530</td>\n",
       "      <td>0.221750</td>\n",
       "      <td>0.450940</td>\n",
       "      <td>0.409590</td>\n",
       "      <td>0.261340</td>\n",
       "      <td>0.202900</td>\n",
       "      <td>0.703690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074460</td>\n",
       "      <td>0.932110</td>\n",
       "      <td>0.309700</td>\n",
       "      <td>0.811640</td>\n",
       "      <td>0.651310</td>\n",
       "      <td>0.426310</td>\n",
       "      <td>0.106370</td>\n",
       "      <td>0.128560</td>\n",
       "      <td>0.381660</td>\n",
       "      <td>0.148490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.264220</td>\n",
       "      <td>0.673100</td>\n",
       "      <td>0.552430</td>\n",
       "      <td>0.211480</td>\n",
       "      <td>0.484940</td>\n",
       "      <td>0.468880</td>\n",
       "      <td>0.259820</td>\n",
       "      <td>0.240040</td>\n",
       "      <td>0.703450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033430</td>\n",
       "      <td>0.918230</td>\n",
       "      <td>0.273310</td>\n",
       "      <td>0.824450</td>\n",
       "      <td>0.609600</td>\n",
       "      <td>0.465400</td>\n",
       "      <td>0.121100</td>\n",
       "      <td>0.089490</td>\n",
       "      <td>0.315100</td>\n",
       "      <td>0.114360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.300860</td>\n",
       "      <td>0.586160</td>\n",
       "      <td>0.691140</td>\n",
       "      <td>0.240640</td>\n",
       "      <td>0.481040</td>\n",
       "      <td>0.579180</td>\n",
       "      <td>0.275920</td>\n",
       "      <td>0.234960</td>\n",
       "      <td>0.633500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117130</td>\n",
       "      <td>0.936460</td>\n",
       "      <td>0.436270</td>\n",
       "      <td>0.820790</td>\n",
       "      <td>0.597970</td>\n",
       "      <td>0.438150</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>0.214790</td>\n",
       "      <td>0.261410</td>\n",
       "      <td>0.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.289540</td>\n",
       "      <td>0.679960</td>\n",
       "      <td>0.553150</td>\n",
       "      <td>0.286020</td>\n",
       "      <td>0.315550</td>\n",
       "      <td>0.421130</td>\n",
       "      <td>0.231630</td>\n",
       "      <td>0.297830</td>\n",
       "      <td>0.680370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043240</td>\n",
       "      <td>0.931800</td>\n",
       "      <td>0.296000</td>\n",
       "      <td>0.798780</td>\n",
       "      <td>0.609940</td>\n",
       "      <td>0.394220</td>\n",
       "      <td>0.143910</td>\n",
       "      <td>0.143810</td>\n",
       "      <td>0.566440</td>\n",
       "      <td>0.156560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.255850</td>\n",
       "      <td>0.591080</td>\n",
       "      <td>0.525270</td>\n",
       "      <td>0.268260</td>\n",
       "      <td>0.343080</td>\n",
       "      <td>0.478540</td>\n",
       "      <td>0.231610</td>\n",
       "      <td>0.272100</td>\n",
       "      <td>0.707310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094330</td>\n",
       "      <td>0.924410</td>\n",
       "      <td>0.299930</td>\n",
       "      <td>0.865710</td>\n",
       "      <td>0.703540</td>\n",
       "      <td>0.399760</td>\n",
       "      <td>0.157860</td>\n",
       "      <td>0.165840</td>\n",
       "      <td>0.629880</td>\n",
       "      <td>0.119080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2470 rows × 832 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  cg22695986  cg01193368  cg22056094  cg06098368  cg08690094  \\\n",
       "0      1.0    0.233353    0.606732    0.730107    0.240143    0.561082   \n",
       "1      1.0    0.284813    0.599726    0.715363    0.242588    0.564277   \n",
       "2      1.0    0.206618    0.552816    0.572559    0.169127    0.541453   \n",
       "3      1.0    0.203151    0.655871    0.391728    0.224729    0.480992   \n",
       "4      0.0    0.266709    0.493554    0.395203    0.231550    0.474545   \n",
       "..     ...         ...         ...         ...         ...         ...   \n",
       "643    1.0    0.265920    0.668810    0.538530    0.221750    0.450940   \n",
       "644    1.0    0.264220    0.673100    0.552430    0.211480    0.484940   \n",
       "645    0.0    0.300860    0.586160    0.691140    0.240640    0.481040   \n",
       "646    0.0    0.289540    0.679960    0.553150    0.286020    0.315550   \n",
       "647    0.0    0.255850    0.591080    0.525270    0.268260    0.343080   \n",
       "\n",
       "     cg11236452  cg26916862  cg03124146  cg14770527  ...  cg11174855  \\\n",
       "0      0.533287    0.194961    0.144218    0.597436  ...    0.327951   \n",
       "1      0.578224    0.192382    0.139766    0.570267  ...    0.195741   \n",
       "2      0.509944    0.197505    0.193932    0.510173  ...    0.083195   \n",
       "3      0.421599    0.178132    0.181609    0.481978  ...    0.134511   \n",
       "4      0.381759    0.194651    0.170328    0.549200  ...    0.069399   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "643    0.409590    0.261340    0.202900    0.703690  ...    0.074460   \n",
       "644    0.468880    0.259820    0.240040    0.703450  ...    0.033430   \n",
       "645    0.579180    0.275920    0.234960    0.633500  ...    0.117130   \n",
       "646    0.421130    0.231630    0.297830    0.680370  ...    0.043240   \n",
       "647    0.478540    0.231610    0.272100    0.707310  ...    0.094330   \n",
       "\n",
       "     cg04524933  cg19418458  cg21808635  cg11359720  cg07635017  cg08641118  \\\n",
       "0      0.919347    0.365084    0.780742    0.731394    0.532118    0.081238   \n",
       "1      0.945236    0.337817    0.696424    0.745175    0.546033    0.099777   \n",
       "2      0.906838    0.357911    0.606163    0.773520    0.536528    0.079384   \n",
       "3      0.956986    0.583261    0.680310    0.753129    0.557829    0.067174   \n",
       "4      0.921778    0.333275    0.722998    0.824286    0.546425    0.075612   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "643    0.932110    0.309700    0.811640    0.651310    0.426310    0.106370   \n",
       "644    0.918230    0.273310    0.824450    0.609600    0.465400    0.121100   \n",
       "645    0.936460    0.436270    0.820790    0.597970    0.438150    0.134000   \n",
       "646    0.931800    0.296000    0.798780    0.609940    0.394220    0.143910   \n",
       "647    0.924410    0.299930    0.865710    0.703540    0.399760    0.157860   \n",
       "\n",
       "     cg09166085  cg22034735  cg17805624  \n",
       "0      0.108676    0.508394    0.279428  \n",
       "1      0.128168    0.498426    0.360946  \n",
       "2      0.091236    0.204013    0.296357  \n",
       "3      0.158550    0.297127    0.292479  \n",
       "4      0.178922    0.526409    0.278850  \n",
       "..          ...         ...         ...  \n",
       "643    0.128560    0.381660    0.148490  \n",
       "644    0.089490    0.315100    0.114360  \n",
       "645    0.214790    0.261410    0.127200  \n",
       "646    0.143810    0.566440    0.156560  \n",
       "647    0.165840    0.629880    0.119080  \n",
       "\n",
       "[2470 rows x 832 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2470 * 832\n",
    "train_data3 = train_data3.loc[:,[i for i in data_AMDTSS.columns]]\n",
    "train_data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cg22695986</th>\n",
       "      <th>cg01193368</th>\n",
       "      <th>cg22056094</th>\n",
       "      <th>cg06098368</th>\n",
       "      <th>cg08690094</th>\n",
       "      <th>cg11236452</th>\n",
       "      <th>cg26916862</th>\n",
       "      <th>cg03124146</th>\n",
       "      <th>cg14770527</th>\n",
       "      <th>...</th>\n",
       "      <th>cg11174855</th>\n",
       "      <th>cg04524933</th>\n",
       "      <th>cg19418458</th>\n",
       "      <th>cg21808635</th>\n",
       "      <th>cg11359720</th>\n",
       "      <th>cg07635017</th>\n",
       "      <th>cg08641118</th>\n",
       "      <th>cg09166085</th>\n",
       "      <th>cg22034735</th>\n",
       "      <th>cg17805624</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.233353</td>\n",
       "      <td>0.606732</td>\n",
       "      <td>0.730107</td>\n",
       "      <td>0.240143</td>\n",
       "      <td>0.561082</td>\n",
       "      <td>0.533287</td>\n",
       "      <td>0.194961</td>\n",
       "      <td>0.144218</td>\n",
       "      <td>0.597436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327951</td>\n",
       "      <td>0.919347</td>\n",
       "      <td>0.365084</td>\n",
       "      <td>0.780742</td>\n",
       "      <td>0.731394</td>\n",
       "      <td>0.532118</td>\n",
       "      <td>0.081238</td>\n",
       "      <td>0.108676</td>\n",
       "      <td>0.508394</td>\n",
       "      <td>0.279428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.284813</td>\n",
       "      <td>0.599726</td>\n",
       "      <td>0.715363</td>\n",
       "      <td>0.242588</td>\n",
       "      <td>0.564277</td>\n",
       "      <td>0.578224</td>\n",
       "      <td>0.192382</td>\n",
       "      <td>0.139766</td>\n",
       "      <td>0.570267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195741</td>\n",
       "      <td>0.945236</td>\n",
       "      <td>0.337817</td>\n",
       "      <td>0.696424</td>\n",
       "      <td>0.745175</td>\n",
       "      <td>0.546033</td>\n",
       "      <td>0.099777</td>\n",
       "      <td>0.128168</td>\n",
       "      <td>0.498426</td>\n",
       "      <td>0.360946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.206618</td>\n",
       "      <td>0.552816</td>\n",
       "      <td>0.572559</td>\n",
       "      <td>0.169127</td>\n",
       "      <td>0.541453</td>\n",
       "      <td>0.509944</td>\n",
       "      <td>0.197505</td>\n",
       "      <td>0.193932</td>\n",
       "      <td>0.510173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083195</td>\n",
       "      <td>0.906838</td>\n",
       "      <td>0.357911</td>\n",
       "      <td>0.606163</td>\n",
       "      <td>0.773520</td>\n",
       "      <td>0.536528</td>\n",
       "      <td>0.079384</td>\n",
       "      <td>0.091236</td>\n",
       "      <td>0.204013</td>\n",
       "      <td>0.296357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.203151</td>\n",
       "      <td>0.655871</td>\n",
       "      <td>0.391728</td>\n",
       "      <td>0.224729</td>\n",
       "      <td>0.480992</td>\n",
       "      <td>0.421599</td>\n",
       "      <td>0.178132</td>\n",
       "      <td>0.181609</td>\n",
       "      <td>0.481978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134511</td>\n",
       "      <td>0.956986</td>\n",
       "      <td>0.583261</td>\n",
       "      <td>0.680310</td>\n",
       "      <td>0.753129</td>\n",
       "      <td>0.557829</td>\n",
       "      <td>0.067174</td>\n",
       "      <td>0.158550</td>\n",
       "      <td>0.297127</td>\n",
       "      <td>0.292479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.266709</td>\n",
       "      <td>0.493554</td>\n",
       "      <td>0.395203</td>\n",
       "      <td>0.231550</td>\n",
       "      <td>0.474545</td>\n",
       "      <td>0.381759</td>\n",
       "      <td>0.194651</td>\n",
       "      <td>0.170328</td>\n",
       "      <td>0.549200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069399</td>\n",
       "      <td>0.921778</td>\n",
       "      <td>0.333275</td>\n",
       "      <td>0.722998</td>\n",
       "      <td>0.824286</td>\n",
       "      <td>0.546425</td>\n",
       "      <td>0.075612</td>\n",
       "      <td>0.178922</td>\n",
       "      <td>0.526409</td>\n",
       "      <td>0.278850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.421692</td>\n",
       "      <td>0.819136</td>\n",
       "      <td>0.889120</td>\n",
       "      <td>0.274042</td>\n",
       "      <td>0.525204</td>\n",
       "      <td>0.698463</td>\n",
       "      <td>0.415842</td>\n",
       "      <td>0.387231</td>\n",
       "      <td>0.759353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285387</td>\n",
       "      <td>0.953237</td>\n",
       "      <td>0.542180</td>\n",
       "      <td>0.869079</td>\n",
       "      <td>0.713411</td>\n",
       "      <td>0.647941</td>\n",
       "      <td>0.176504</td>\n",
       "      <td>0.168747</td>\n",
       "      <td>0.529537</td>\n",
       "      <td>0.275265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.155194</td>\n",
       "      <td>0.650936</td>\n",
       "      <td>0.436741</td>\n",
       "      <td>0.127649</td>\n",
       "      <td>0.458996</td>\n",
       "      <td>0.414456</td>\n",
       "      <td>0.214664</td>\n",
       "      <td>0.116075</td>\n",
       "      <td>0.700554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098905</td>\n",
       "      <td>0.945503</td>\n",
       "      <td>0.361169</td>\n",
       "      <td>0.766231</td>\n",
       "      <td>0.652946</td>\n",
       "      <td>0.603185</td>\n",
       "      <td>0.143384</td>\n",
       "      <td>0.172315</td>\n",
       "      <td>0.522733</td>\n",
       "      <td>0.230949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.170119</td>\n",
       "      <td>0.668992</td>\n",
       "      <td>0.532885</td>\n",
       "      <td>0.141059</td>\n",
       "      <td>0.522190</td>\n",
       "      <td>0.469892</td>\n",
       "      <td>0.177975</td>\n",
       "      <td>0.141652</td>\n",
       "      <td>0.686412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208649</td>\n",
       "      <td>0.962932</td>\n",
       "      <td>0.609414</td>\n",
       "      <td>0.827691</td>\n",
       "      <td>0.850505</td>\n",
       "      <td>0.631754</td>\n",
       "      <td>0.077538</td>\n",
       "      <td>0.129992</td>\n",
       "      <td>0.617378</td>\n",
       "      <td>0.242766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.316241</td>\n",
       "      <td>0.642261</td>\n",
       "      <td>0.906532</td>\n",
       "      <td>0.303791</td>\n",
       "      <td>0.403003</td>\n",
       "      <td>0.830418</td>\n",
       "      <td>0.242378</td>\n",
       "      <td>0.278751</td>\n",
       "      <td>0.764943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223300</td>\n",
       "      <td>0.917177</td>\n",
       "      <td>0.357777</td>\n",
       "      <td>0.776557</td>\n",
       "      <td>0.607103</td>\n",
       "      <td>0.595563</td>\n",
       "      <td>0.108415</td>\n",
       "      <td>0.338011</td>\n",
       "      <td>0.444886</td>\n",
       "      <td>0.302313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.356544</td>\n",
       "      <td>0.780978</td>\n",
       "      <td>0.565044</td>\n",
       "      <td>0.301901</td>\n",
       "      <td>0.552029</td>\n",
       "      <td>0.542632</td>\n",
       "      <td>0.365609</td>\n",
       "      <td>0.374958</td>\n",
       "      <td>0.796539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231661</td>\n",
       "      <td>0.958034</td>\n",
       "      <td>0.734586</td>\n",
       "      <td>0.737711</td>\n",
       "      <td>0.774517</td>\n",
       "      <td>0.587225</td>\n",
       "      <td>0.164567</td>\n",
       "      <td>0.167376</td>\n",
       "      <td>0.740924</td>\n",
       "      <td>0.223064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2734 rows × 832 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  cg22695986  cg01193368  cg22056094  cg06098368  cg08690094  \\\n",
       "0      1.0    0.233353    0.606732    0.730107    0.240143    0.561082   \n",
       "1      1.0    0.284813    0.599726    0.715363    0.242588    0.564277   \n",
       "2      1.0    0.206618    0.552816    0.572559    0.169127    0.541453   \n",
       "3      1.0    0.203151    0.655871    0.391728    0.224729    0.480992   \n",
       "4      0.0    0.266709    0.493554    0.395203    0.231550    0.474545   \n",
       "..     ...         ...         ...         ...         ...         ...   \n",
       "471    0.0    0.421692    0.819136    0.889120    0.274042    0.525204   \n",
       "472    1.0    0.155194    0.650936    0.436741    0.127649    0.458996   \n",
       "475    1.0    0.170119    0.668992    0.532885    0.141059    0.522190   \n",
       "477    0.0    0.316241    0.642261    0.906532    0.303791    0.403003   \n",
       "478    1.0    0.356544    0.780978    0.565044    0.301901    0.552029   \n",
       "\n",
       "     cg11236452  cg26916862  cg03124146  cg14770527  ...  cg11174855  \\\n",
       "0      0.533287    0.194961    0.144218    0.597436  ...    0.327951   \n",
       "1      0.578224    0.192382    0.139766    0.570267  ...    0.195741   \n",
       "2      0.509944    0.197505    0.193932    0.510173  ...    0.083195   \n",
       "3      0.421599    0.178132    0.181609    0.481978  ...    0.134511   \n",
       "4      0.381759    0.194651    0.170328    0.549200  ...    0.069399   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "471    0.698463    0.415842    0.387231    0.759353  ...    0.285387   \n",
       "472    0.414456    0.214664    0.116075    0.700554  ...    0.098905   \n",
       "475    0.469892    0.177975    0.141652    0.686412  ...    0.208649   \n",
       "477    0.830418    0.242378    0.278751    0.764943  ...    0.223300   \n",
       "478    0.542632    0.365609    0.374958    0.796539  ...    0.231661   \n",
       "\n",
       "     cg04524933  cg19418458  cg21808635  cg11359720  cg07635017  cg08641118  \\\n",
       "0      0.919347    0.365084    0.780742    0.731394    0.532118    0.081238   \n",
       "1      0.945236    0.337817    0.696424    0.745175    0.546033    0.099777   \n",
       "2      0.906838    0.357911    0.606163    0.773520    0.536528    0.079384   \n",
       "3      0.956986    0.583261    0.680310    0.753129    0.557829    0.067174   \n",
       "4      0.921778    0.333275    0.722998    0.824286    0.546425    0.075612   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "471    0.953237    0.542180    0.869079    0.713411    0.647941    0.176504   \n",
       "472    0.945503    0.361169    0.766231    0.652946    0.603185    0.143384   \n",
       "475    0.962932    0.609414    0.827691    0.850505    0.631754    0.077538   \n",
       "477    0.917177    0.357777    0.776557    0.607103    0.595563    0.108415   \n",
       "478    0.958034    0.734586    0.737711    0.774517    0.587225    0.164567   \n",
       "\n",
       "     cg09166085  cg22034735  cg17805624  \n",
       "0      0.108676    0.508394    0.279428  \n",
       "1      0.128168    0.498426    0.360946  \n",
       "2      0.091236    0.204013    0.296357  \n",
       "3      0.158550    0.297127    0.292479  \n",
       "4      0.178922    0.526409    0.278850  \n",
       "..          ...         ...         ...  \n",
       "471    0.168747    0.529537    0.275265  \n",
       "472    0.172315    0.522733    0.230949  \n",
       "475    0.129992    0.617378    0.242766  \n",
       "477    0.338011    0.444886    0.302313  \n",
       "478    0.167376    0.740924    0.223064  \n",
       "\n",
       "[2734 rows x 832 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2734 * 832\n",
    "train_data3 = pd.concat([train_data3, data_AMDTSS])\n",
    "train_data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2050, 831), (684, 831), (2050,), (684,))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training 75% developing 25%\n",
    "x_train3, x_dev3, y_train3, y_dev3 = train_test_split(train_data3.drop(columns=['label']), train_data3['label'])\n",
    "x_train3.shape, x_dev3.shape, y_train3.shape, y_dev3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Original Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6893796992481203"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en3 = LogisticRegression(penalty = \"elasticnet\", solver = \"saga\", l1_ratio = 0.5)\n",
    "en3.fit(x_train3, y_train3)\n",
    "y_DENMARK = data_DENMARK['label']\n",
    "x_DENMARK = data_DENMARK.loc[:,[i for i in data_AMDTSS.columns]]\n",
    "x_DENMARK = x_DENMARK.drop(['label'], axis = 1)\n",
    "en_auc3 = roc_auc_score(y_DENMARK, en3.predict_proba(x_DENMARK)[:, 1])\n",
    "en_auc3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train New Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Selection by RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', max_depth=20,\n",
       "                       min_impurity_decrease=1e-05, n_estimators=300)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid search for rf\n",
    "# The number of trees in the forest.\n",
    "n_estimators = [50, 100, 200, 300, 500]\n",
    "# The function to measure the quality of a split\n",
    "criterion = [\"gini\", \"entropy\"]\n",
    "# A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "min_impurity_decrease = [0.1, 0.000001, 0.00001]\n",
    "# The maximum depth of the tree.\n",
    "max_depth = [20, 50, 100, 500, 1000]\n",
    "\n",
    "param_distributions = dict(n_estimators = n_estimators, criterion = criterion, min_impurity_decrease = min_impurity_decrease, max_depth = max_depth)\n",
    "rf = RandomForestClassifier()\n",
    "grid_rf3 = RandomizedSearchCV(estimator = rf, param_distributions = param_distributions, scoring = \"roc_auc\",\n",
    "                          verbose = 1, n_jobs = -1) \n",
    "grid_result_rf3 = grid_rf3.fit(x_train3, y_train3) \n",
    "\n",
    "grid_result_rf3.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=RandomForestClassifier(criterion='entropy',\n",
       "                                                 max_depth=20,\n",
       "                                                 min_impurity_decrease=1e-05,\n",
       "                                                 n_estimators=300))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variable selection by random forest\n",
    "rf_selection3 = SelectFromModel(grid_result_rf3.best_estimator_)\n",
    "rf_selection3.fit(x_train3, y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selected variables\n",
    "selected_feat_rf3 = x_train3.columns[(rf_selection3.get_support())]\n",
    "len(selected_feat_rf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cg22695986</th>\n",
       "      <th>cg01193368</th>\n",
       "      <th>cg22056094</th>\n",
       "      <th>cg06098368</th>\n",
       "      <th>cg08690094</th>\n",
       "      <th>cg11236452</th>\n",
       "      <th>cg26916862</th>\n",
       "      <th>cg14770527</th>\n",
       "      <th>cg23316599</th>\n",
       "      <th>cg10933186</th>\n",
       "      <th>...</th>\n",
       "      <th>cg07430760</th>\n",
       "      <th>cg01929855</th>\n",
       "      <th>cg09243445</th>\n",
       "      <th>cg27132152</th>\n",
       "      <th>cg15489799</th>\n",
       "      <th>cg09990584</th>\n",
       "      <th>cg11174855</th>\n",
       "      <th>cg21808635</th>\n",
       "      <th>cg08641118</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.233353</td>\n",
       "      <td>0.606732</td>\n",
       "      <td>0.730107</td>\n",
       "      <td>0.240143</td>\n",
       "      <td>0.561082</td>\n",
       "      <td>0.533287</td>\n",
       "      <td>0.194961</td>\n",
       "      <td>0.597436</td>\n",
       "      <td>0.396608</td>\n",
       "      <td>0.850368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090786</td>\n",
       "      <td>0.865708</td>\n",
       "      <td>0.306735</td>\n",
       "      <td>0.363816</td>\n",
       "      <td>0.434138</td>\n",
       "      <td>0.342347</td>\n",
       "      <td>0.327951</td>\n",
       "      <td>0.780742</td>\n",
       "      <td>0.081238</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.284813</td>\n",
       "      <td>0.599726</td>\n",
       "      <td>0.715363</td>\n",
       "      <td>0.242588</td>\n",
       "      <td>0.564277</td>\n",
       "      <td>0.578224</td>\n",
       "      <td>0.192382</td>\n",
       "      <td>0.570267</td>\n",
       "      <td>0.419441</td>\n",
       "      <td>0.770310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086232</td>\n",
       "      <td>0.889347</td>\n",
       "      <td>0.316390</td>\n",
       "      <td>0.454734</td>\n",
       "      <td>0.456402</td>\n",
       "      <td>0.321944</td>\n",
       "      <td>0.195741</td>\n",
       "      <td>0.696424</td>\n",
       "      <td>0.099777</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.206618</td>\n",
       "      <td>0.552816</td>\n",
       "      <td>0.572559</td>\n",
       "      <td>0.169127</td>\n",
       "      <td>0.541453</td>\n",
       "      <td>0.509944</td>\n",
       "      <td>0.197505</td>\n",
       "      <td>0.510173</td>\n",
       "      <td>0.479463</td>\n",
       "      <td>0.741130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062408</td>\n",
       "      <td>0.880054</td>\n",
       "      <td>0.335758</td>\n",
       "      <td>0.479652</td>\n",
       "      <td>0.476032</td>\n",
       "      <td>0.327677</td>\n",
       "      <td>0.083195</td>\n",
       "      <td>0.606163</td>\n",
       "      <td>0.079384</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.203151</td>\n",
       "      <td>0.655871</td>\n",
       "      <td>0.391728</td>\n",
       "      <td>0.224729</td>\n",
       "      <td>0.480992</td>\n",
       "      <td>0.421599</td>\n",
       "      <td>0.178132</td>\n",
       "      <td>0.481978</td>\n",
       "      <td>0.510254</td>\n",
       "      <td>0.667161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119200</td>\n",
       "      <td>0.849659</td>\n",
       "      <td>0.355541</td>\n",
       "      <td>0.414647</td>\n",
       "      <td>0.461083</td>\n",
       "      <td>0.303911</td>\n",
       "      <td>0.134511</td>\n",
       "      <td>0.680310</td>\n",
       "      <td>0.067174</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.266709</td>\n",
       "      <td>0.493554</td>\n",
       "      <td>0.395203</td>\n",
       "      <td>0.231550</td>\n",
       "      <td>0.474545</td>\n",
       "      <td>0.381759</td>\n",
       "      <td>0.194651</td>\n",
       "      <td>0.549200</td>\n",
       "      <td>0.506738</td>\n",
       "      <td>0.731597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129792</td>\n",
       "      <td>0.886471</td>\n",
       "      <td>0.359560</td>\n",
       "      <td>0.443079</td>\n",
       "      <td>0.502171</td>\n",
       "      <td>0.333914</td>\n",
       "      <td>0.069399</td>\n",
       "      <td>0.722998</td>\n",
       "      <td>0.075612</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>0.421692</td>\n",
       "      <td>0.819136</td>\n",
       "      <td>0.889120</td>\n",
       "      <td>0.274042</td>\n",
       "      <td>0.525204</td>\n",
       "      <td>0.698463</td>\n",
       "      <td>0.415842</td>\n",
       "      <td>0.759353</td>\n",
       "      <td>0.573460</td>\n",
       "      <td>0.799908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163088</td>\n",
       "      <td>0.900970</td>\n",
       "      <td>0.365249</td>\n",
       "      <td>0.557590</td>\n",
       "      <td>0.546019</td>\n",
       "      <td>0.385300</td>\n",
       "      <td>0.285387</td>\n",
       "      <td>0.869079</td>\n",
       "      <td>0.176504</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>0.155194</td>\n",
       "      <td>0.650936</td>\n",
       "      <td>0.436741</td>\n",
       "      <td>0.127649</td>\n",
       "      <td>0.458996</td>\n",
       "      <td>0.414456</td>\n",
       "      <td>0.214664</td>\n",
       "      <td>0.700554</td>\n",
       "      <td>0.475778</td>\n",
       "      <td>0.679567</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092700</td>\n",
       "      <td>0.859307</td>\n",
       "      <td>0.362257</td>\n",
       "      <td>0.488550</td>\n",
       "      <td>0.478794</td>\n",
       "      <td>0.347106</td>\n",
       "      <td>0.098905</td>\n",
       "      <td>0.766231</td>\n",
       "      <td>0.143384</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0.170119</td>\n",
       "      <td>0.668992</td>\n",
       "      <td>0.532885</td>\n",
       "      <td>0.141059</td>\n",
       "      <td>0.522190</td>\n",
       "      <td>0.469892</td>\n",
       "      <td>0.177975</td>\n",
       "      <td>0.686412</td>\n",
       "      <td>0.454601</td>\n",
       "      <td>0.703902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106060</td>\n",
       "      <td>0.922259</td>\n",
       "      <td>0.403914</td>\n",
       "      <td>0.515793</td>\n",
       "      <td>0.603469</td>\n",
       "      <td>0.507703</td>\n",
       "      <td>0.208649</td>\n",
       "      <td>0.827691</td>\n",
       "      <td>0.077538</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0.316241</td>\n",
       "      <td>0.642261</td>\n",
       "      <td>0.906532</td>\n",
       "      <td>0.303791</td>\n",
       "      <td>0.403003</td>\n",
       "      <td>0.830418</td>\n",
       "      <td>0.242378</td>\n",
       "      <td>0.764943</td>\n",
       "      <td>0.376560</td>\n",
       "      <td>0.921208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107320</td>\n",
       "      <td>0.909179</td>\n",
       "      <td>0.381835</td>\n",
       "      <td>0.512241</td>\n",
       "      <td>0.594803</td>\n",
       "      <td>0.452558</td>\n",
       "      <td>0.223300</td>\n",
       "      <td>0.776557</td>\n",
       "      <td>0.108415</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0.356544</td>\n",
       "      <td>0.780978</td>\n",
       "      <td>0.565044</td>\n",
       "      <td>0.301901</td>\n",
       "      <td>0.552029</td>\n",
       "      <td>0.542632</td>\n",
       "      <td>0.365609</td>\n",
       "      <td>0.796539</td>\n",
       "      <td>0.529338</td>\n",
       "      <td>0.782587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075424</td>\n",
       "      <td>0.900922</td>\n",
       "      <td>0.358353</td>\n",
       "      <td>0.460155</td>\n",
       "      <td>0.511126</td>\n",
       "      <td>0.379746</td>\n",
       "      <td>0.231661</td>\n",
       "      <td>0.737711</td>\n",
       "      <td>0.164567</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2734 rows × 286 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cg22695986  cg01193368  cg22056094  cg06098368  cg08690094  cg11236452  \\\n",
       "0      0.233353    0.606732    0.730107    0.240143    0.561082    0.533287   \n",
       "1      0.284813    0.599726    0.715363    0.242588    0.564277    0.578224   \n",
       "2      0.206618    0.552816    0.572559    0.169127    0.541453    0.509944   \n",
       "3      0.203151    0.655871    0.391728    0.224729    0.480992    0.421599   \n",
       "4      0.266709    0.493554    0.395203    0.231550    0.474545    0.381759   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "471    0.421692    0.819136    0.889120    0.274042    0.525204    0.698463   \n",
       "472    0.155194    0.650936    0.436741    0.127649    0.458996    0.414456   \n",
       "475    0.170119    0.668992    0.532885    0.141059    0.522190    0.469892   \n",
       "477    0.316241    0.642261    0.906532    0.303791    0.403003    0.830418   \n",
       "478    0.356544    0.780978    0.565044    0.301901    0.552029    0.542632   \n",
       "\n",
       "     cg26916862  cg14770527  cg23316599  cg10933186  ...  cg07430760  \\\n",
       "0      0.194961    0.597436    0.396608    0.850368  ...    0.090786   \n",
       "1      0.192382    0.570267    0.419441    0.770310  ...    0.086232   \n",
       "2      0.197505    0.510173    0.479463    0.741130  ...    0.062408   \n",
       "3      0.178132    0.481978    0.510254    0.667161  ...    0.119200   \n",
       "4      0.194651    0.549200    0.506738    0.731597  ...    0.129792   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "471    0.415842    0.759353    0.573460    0.799908  ...    0.163088   \n",
       "472    0.214664    0.700554    0.475778    0.679567  ...    0.092700   \n",
       "475    0.177975    0.686412    0.454601    0.703902  ...    0.106060   \n",
       "477    0.242378    0.764943    0.376560    0.921208  ...    0.107320   \n",
       "478    0.365609    0.796539    0.529338    0.782587  ...    0.075424   \n",
       "\n",
       "     cg01929855  cg09243445  cg27132152  cg15489799  cg09990584  cg11174855  \\\n",
       "0      0.865708    0.306735    0.363816    0.434138    0.342347    0.327951   \n",
       "1      0.889347    0.316390    0.454734    0.456402    0.321944    0.195741   \n",
       "2      0.880054    0.335758    0.479652    0.476032    0.327677    0.083195   \n",
       "3      0.849659    0.355541    0.414647    0.461083    0.303911    0.134511   \n",
       "4      0.886471    0.359560    0.443079    0.502171    0.333914    0.069399   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "471    0.900970    0.365249    0.557590    0.546019    0.385300    0.285387   \n",
       "472    0.859307    0.362257    0.488550    0.478794    0.347106    0.098905   \n",
       "475    0.922259    0.403914    0.515793    0.603469    0.507703    0.208649   \n",
       "477    0.909179    0.381835    0.512241    0.594803    0.452558    0.223300   \n",
       "478    0.900922    0.358353    0.460155    0.511126    0.379746    0.231661   \n",
       "\n",
       "     cg21808635  cg08641118  label  \n",
       "0      0.780742    0.081238    1.0  \n",
       "1      0.696424    0.099777    1.0  \n",
       "2      0.606163    0.079384    1.0  \n",
       "3      0.680310    0.067174    1.0  \n",
       "4      0.722998    0.075612    0.0  \n",
       "..          ...         ...    ...  \n",
       "471    0.869079    0.176504    0.0  \n",
       "472    0.766231    0.143384    1.0  \n",
       "475    0.827691    0.077538    1.0  \n",
       "477    0.776557    0.108415    0.0  \n",
       "478    0.737711    0.164567    1.0  \n",
       "\n",
       "[2734 rows x 286 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_selected_train_data3 = train_data3.loc[:,[i for i in selected_feat_rf3]]\n",
    "rf_selected_train_data3['label'] = train_data3['label'] \n",
    "rf_selected_train_data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2050, 285), (684, 285), (2050,), (684,))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training 75% developing 25%\n",
    "x_train_rf3, x_dev_rf3, y_train_rf3, y_dev_rf3 = train_test_split(rf_selected_train_data3.drop(columns=['label']), rf_selected_train_data3['label'])\n",
    "x_train_rf3.shape, x_dev_rf3.shape, y_train_rf3.shape, y_dev_rf3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.793661699660755"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_selected3 = grid_result_rf3.best_estimator_\n",
    "rf_selected3.fit(x_train_rf3, y_train_rf3)\n",
    "rf_selected_auc3 = roc_auc_score(y_dev_rf3, rf_selected3.predict_proba(x_dev_rf3)[:, 1])\n",
    "rf_selected_auc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_DENMARK_rf_selected = data_DENMARK['label']\n",
    "x_DENMARK_rf_selected = data_DENMARK.loc[:,[i for i in selected_feat_rf3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6067512531328321"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_selected_auc_DENMARK = roc_auc_score(y_DENMARK_rf_selected, rf_selected3.predict_proba(x_DENMARK_rf_selected)[:, 1])\n",
    "rf_selected_auc_DENMARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6076127819548872"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking_rf_DENMARK = get_stacking()\n",
    "stacking_rf_DENMARK.fit(x_train_rf3, y_train_rf3)\n",
    "stacking_rf_DENMARK_auc = roc_auc_score(y_DENMARK_rf_selected, stacking_rf_DENMARK.predict_proba(x_DENMARK_rf_selected)[:, 1])\n",
    "stacking_rf_DENMARK_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6915726817042607"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_rf_DENMARK = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gb', clf3), ('svc', clf4)], voting='soft')\n",
    "voting_rf_DENMARK.fit(x_train_rf3, y_train_rf3)\n",
    "voting_rf_DENMARK_auc = roc_auc_score(y_DENMARK_rf_selected, voting_rf_DENMARK.predict_proba(x_DENMARK_rf_selected)[:, 1])\n",
    "voting_rf_DENMARK_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Selection by LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 450, in _check_solver\n",
      "    raise ValueError(\"Only 'saga' solver supports elasticnet penalty,\"\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 450, in _check_solver\n",
      "    raise ValueError(\"Only 'saga' solver supports elasticnet penalty,\"\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 450, in _check_solver\n",
      "    raise ValueError(\"Only 'saga' solver supports elasticnet penalty,\"\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 450, in _check_solver\n",
      "    raise ValueError(\"Only 'saga' solver supports elasticnet penalty,\"\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 450, in _check_solver\n",
      "    raise ValueError(\"Only 'saga' solver supports elasticnet penalty,\"\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan 0.71185513        nan 0.788992          nan        nan\n",
      "        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.5, solver='liblinear', tol=1e-05)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid search for lr\n",
    "penalty = [\"l1\", \"l2\", \"elasticnet\"]\n",
    "# Tolerance for stopping criteria.\n",
    "tol = [0.00001, 0.001, 0.0000001]\n",
    "# Inverse of regularization strength\n",
    "C = [0,1, 0.5, 1, 10, 20, 30, 50, 100]\n",
    "# Algorithm to use in the optimization problem\n",
    "solver = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "\n",
    "\n",
    "param_distributions = dict(penalty = penalty, tol = tol, C = C, solver = solver)\n",
    "lr = LogisticRegression()\n",
    "grid_lr3 = RandomizedSearchCV(estimator = lr, param_distributions = param_distributions, scoring = \"roc_auc\",\n",
    "                          verbose = 1, n_jobs = -1) \n",
    "grid_result_lr3 = grid_lr3.fit(x_train3, y_train3) \n",
    "\n",
    "grid_result_lr3.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=LogisticRegression(C=0.5, solver='liblinear',\n",
       "                                             tol=1e-05))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_selection3 = SelectFromModel(grid_result_lr3.best_estimator_)\n",
    "lr_selection3.fit(x_train3, y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_feat_lr3 = x_train3.columns[(lr_selection3.get_support())]\n",
    "len(selected_feat_lr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cg01193368</th>\n",
       "      <th>cg22056094</th>\n",
       "      <th>cg06098368</th>\n",
       "      <th>cg08690094</th>\n",
       "      <th>cg11236452</th>\n",
       "      <th>cg03124146</th>\n",
       "      <th>cg14770527</th>\n",
       "      <th>cg11108474</th>\n",
       "      <th>cg16340103</th>\n",
       "      <th>cg04838249</th>\n",
       "      <th>...</th>\n",
       "      <th>cg09990584</th>\n",
       "      <th>cg14466863</th>\n",
       "      <th>cg11174855</th>\n",
       "      <th>cg04524933</th>\n",
       "      <th>cg11359720</th>\n",
       "      <th>cg07635017</th>\n",
       "      <th>cg08641118</th>\n",
       "      <th>cg22034735</th>\n",
       "      <th>cg17805624</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.606732</td>\n",
       "      <td>0.730107</td>\n",
       "      <td>0.240143</td>\n",
       "      <td>0.561082</td>\n",
       "      <td>0.533287</td>\n",
       "      <td>0.144218</td>\n",
       "      <td>0.597436</td>\n",
       "      <td>0.479727</td>\n",
       "      <td>0.556758</td>\n",
       "      <td>0.709753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342347</td>\n",
       "      <td>0.825537</td>\n",
       "      <td>0.327951</td>\n",
       "      <td>0.919347</td>\n",
       "      <td>0.731394</td>\n",
       "      <td>0.532118</td>\n",
       "      <td>0.081238</td>\n",
       "      <td>0.508394</td>\n",
       "      <td>0.279428</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.599726</td>\n",
       "      <td>0.715363</td>\n",
       "      <td>0.242588</td>\n",
       "      <td>0.564277</td>\n",
       "      <td>0.578224</td>\n",
       "      <td>0.139766</td>\n",
       "      <td>0.570267</td>\n",
       "      <td>0.483118</td>\n",
       "      <td>0.481381</td>\n",
       "      <td>0.727749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.321944</td>\n",
       "      <td>0.823126</td>\n",
       "      <td>0.195741</td>\n",
       "      <td>0.945236</td>\n",
       "      <td>0.745175</td>\n",
       "      <td>0.546033</td>\n",
       "      <td>0.099777</td>\n",
       "      <td>0.498426</td>\n",
       "      <td>0.360946</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.552816</td>\n",
       "      <td>0.572559</td>\n",
       "      <td>0.169127</td>\n",
       "      <td>0.541453</td>\n",
       "      <td>0.509944</td>\n",
       "      <td>0.193932</td>\n",
       "      <td>0.510173</td>\n",
       "      <td>0.368669</td>\n",
       "      <td>0.513161</td>\n",
       "      <td>0.668243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327677</td>\n",
       "      <td>0.748225</td>\n",
       "      <td>0.083195</td>\n",
       "      <td>0.906838</td>\n",
       "      <td>0.773520</td>\n",
       "      <td>0.536528</td>\n",
       "      <td>0.079384</td>\n",
       "      <td>0.204013</td>\n",
       "      <td>0.296357</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.655871</td>\n",
       "      <td>0.391728</td>\n",
       "      <td>0.224729</td>\n",
       "      <td>0.480992</td>\n",
       "      <td>0.421599</td>\n",
       "      <td>0.181609</td>\n",
       "      <td>0.481978</td>\n",
       "      <td>0.360383</td>\n",
       "      <td>0.432874</td>\n",
       "      <td>0.745117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303911</td>\n",
       "      <td>0.761062</td>\n",
       "      <td>0.134511</td>\n",
       "      <td>0.956986</td>\n",
       "      <td>0.753129</td>\n",
       "      <td>0.557829</td>\n",
       "      <td>0.067174</td>\n",
       "      <td>0.297127</td>\n",
       "      <td>0.292479</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.493554</td>\n",
       "      <td>0.395203</td>\n",
       "      <td>0.231550</td>\n",
       "      <td>0.474545</td>\n",
       "      <td>0.381759</td>\n",
       "      <td>0.170328</td>\n",
       "      <td>0.549200</td>\n",
       "      <td>0.401675</td>\n",
       "      <td>0.475677</td>\n",
       "      <td>0.783492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333914</td>\n",
       "      <td>0.805394</td>\n",
       "      <td>0.069399</td>\n",
       "      <td>0.921778</td>\n",
       "      <td>0.824286</td>\n",
       "      <td>0.546425</td>\n",
       "      <td>0.075612</td>\n",
       "      <td>0.526409</td>\n",
       "      <td>0.278850</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>0.819136</td>\n",
       "      <td>0.889120</td>\n",
       "      <td>0.274042</td>\n",
       "      <td>0.525204</td>\n",
       "      <td>0.698463</td>\n",
       "      <td>0.387231</td>\n",
       "      <td>0.759353</td>\n",
       "      <td>0.401122</td>\n",
       "      <td>0.677112</td>\n",
       "      <td>0.805374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.385300</td>\n",
       "      <td>0.780383</td>\n",
       "      <td>0.285387</td>\n",
       "      <td>0.953237</td>\n",
       "      <td>0.713411</td>\n",
       "      <td>0.647941</td>\n",
       "      <td>0.176504</td>\n",
       "      <td>0.529537</td>\n",
       "      <td>0.275265</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>0.650936</td>\n",
       "      <td>0.436741</td>\n",
       "      <td>0.127649</td>\n",
       "      <td>0.458996</td>\n",
       "      <td>0.414456</td>\n",
       "      <td>0.116075</td>\n",
       "      <td>0.700554</td>\n",
       "      <td>0.298102</td>\n",
       "      <td>0.585688</td>\n",
       "      <td>0.670181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.347106</td>\n",
       "      <td>0.734427</td>\n",
       "      <td>0.098905</td>\n",
       "      <td>0.945503</td>\n",
       "      <td>0.652946</td>\n",
       "      <td>0.603185</td>\n",
       "      <td>0.143384</td>\n",
       "      <td>0.522733</td>\n",
       "      <td>0.230949</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0.668992</td>\n",
       "      <td>0.532885</td>\n",
       "      <td>0.141059</td>\n",
       "      <td>0.522190</td>\n",
       "      <td>0.469892</td>\n",
       "      <td>0.141652</td>\n",
       "      <td>0.686412</td>\n",
       "      <td>0.322749</td>\n",
       "      <td>0.536903</td>\n",
       "      <td>0.624104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507703</td>\n",
       "      <td>0.805425</td>\n",
       "      <td>0.208649</td>\n",
       "      <td>0.962932</td>\n",
       "      <td>0.850505</td>\n",
       "      <td>0.631754</td>\n",
       "      <td>0.077538</td>\n",
       "      <td>0.617378</td>\n",
       "      <td>0.242766</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0.642261</td>\n",
       "      <td>0.906532</td>\n",
       "      <td>0.303791</td>\n",
       "      <td>0.403003</td>\n",
       "      <td>0.830418</td>\n",
       "      <td>0.278751</td>\n",
       "      <td>0.764943</td>\n",
       "      <td>0.445176</td>\n",
       "      <td>0.485595</td>\n",
       "      <td>0.663237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.452558</td>\n",
       "      <td>0.760405</td>\n",
       "      <td>0.223300</td>\n",
       "      <td>0.917177</td>\n",
       "      <td>0.607103</td>\n",
       "      <td>0.595563</td>\n",
       "      <td>0.108415</td>\n",
       "      <td>0.444886</td>\n",
       "      <td>0.302313</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0.780978</td>\n",
       "      <td>0.565044</td>\n",
       "      <td>0.301901</td>\n",
       "      <td>0.552029</td>\n",
       "      <td>0.542632</td>\n",
       "      <td>0.374958</td>\n",
       "      <td>0.796539</td>\n",
       "      <td>0.463919</td>\n",
       "      <td>0.555628</td>\n",
       "      <td>0.780577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.379746</td>\n",
       "      <td>0.777106</td>\n",
       "      <td>0.231661</td>\n",
       "      <td>0.958034</td>\n",
       "      <td>0.774517</td>\n",
       "      <td>0.587225</td>\n",
       "      <td>0.164567</td>\n",
       "      <td>0.740924</td>\n",
       "      <td>0.223064</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2734 rows × 348 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cg01193368  cg22056094  cg06098368  cg08690094  cg11236452  cg03124146  \\\n",
       "0      0.606732    0.730107    0.240143    0.561082    0.533287    0.144218   \n",
       "1      0.599726    0.715363    0.242588    0.564277    0.578224    0.139766   \n",
       "2      0.552816    0.572559    0.169127    0.541453    0.509944    0.193932   \n",
       "3      0.655871    0.391728    0.224729    0.480992    0.421599    0.181609   \n",
       "4      0.493554    0.395203    0.231550    0.474545    0.381759    0.170328   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "471    0.819136    0.889120    0.274042    0.525204    0.698463    0.387231   \n",
       "472    0.650936    0.436741    0.127649    0.458996    0.414456    0.116075   \n",
       "475    0.668992    0.532885    0.141059    0.522190    0.469892    0.141652   \n",
       "477    0.642261    0.906532    0.303791    0.403003    0.830418    0.278751   \n",
       "478    0.780978    0.565044    0.301901    0.552029    0.542632    0.374958   \n",
       "\n",
       "     cg14770527  cg11108474  cg16340103  cg04838249  ...  cg09990584  \\\n",
       "0      0.597436    0.479727    0.556758    0.709753  ...    0.342347   \n",
       "1      0.570267    0.483118    0.481381    0.727749  ...    0.321944   \n",
       "2      0.510173    0.368669    0.513161    0.668243  ...    0.327677   \n",
       "3      0.481978    0.360383    0.432874    0.745117  ...    0.303911   \n",
       "4      0.549200    0.401675    0.475677    0.783492  ...    0.333914   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "471    0.759353    0.401122    0.677112    0.805374  ...    0.385300   \n",
       "472    0.700554    0.298102    0.585688    0.670181  ...    0.347106   \n",
       "475    0.686412    0.322749    0.536903    0.624104  ...    0.507703   \n",
       "477    0.764943    0.445176    0.485595    0.663237  ...    0.452558   \n",
       "478    0.796539    0.463919    0.555628    0.780577  ...    0.379746   \n",
       "\n",
       "     cg14466863  cg11174855  cg04524933  cg11359720  cg07635017  cg08641118  \\\n",
       "0      0.825537    0.327951    0.919347    0.731394    0.532118    0.081238   \n",
       "1      0.823126    0.195741    0.945236    0.745175    0.546033    0.099777   \n",
       "2      0.748225    0.083195    0.906838    0.773520    0.536528    0.079384   \n",
       "3      0.761062    0.134511    0.956986    0.753129    0.557829    0.067174   \n",
       "4      0.805394    0.069399    0.921778    0.824286    0.546425    0.075612   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "471    0.780383    0.285387    0.953237    0.713411    0.647941    0.176504   \n",
       "472    0.734427    0.098905    0.945503    0.652946    0.603185    0.143384   \n",
       "475    0.805425    0.208649    0.962932    0.850505    0.631754    0.077538   \n",
       "477    0.760405    0.223300    0.917177    0.607103    0.595563    0.108415   \n",
       "478    0.777106    0.231661    0.958034    0.774517    0.587225    0.164567   \n",
       "\n",
       "     cg22034735  cg17805624  label  \n",
       "0      0.508394    0.279428    1.0  \n",
       "1      0.498426    0.360946    1.0  \n",
       "2      0.204013    0.296357    1.0  \n",
       "3      0.297127    0.292479    1.0  \n",
       "4      0.526409    0.278850    0.0  \n",
       "..          ...         ...    ...  \n",
       "471    0.529537    0.275265    0.0  \n",
       "472    0.522733    0.230949    1.0  \n",
       "475    0.617378    0.242766    1.0  \n",
       "477    0.444886    0.302313    0.0  \n",
       "478    0.740924    0.223064    1.0  \n",
       "\n",
       "[2734 rows x 348 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_selected_train_data3 = train_data3.loc[:,[i for i in selected_feat_lr3]]\n",
    "lr_selected_train_data3['label'] = train_data3['label'] \n",
    "lr_selected_train_data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2050, 347), (684, 347), (2050,), (684,))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training 75% developing 25%\n",
    "x_train_lr3, x_dev_lr3, y_train_lr3, y_dev_lr3 = train_test_split(lr_selected_train_data3.drop(columns=['label']), lr_selected_train_data3['label'])\n",
    "x_train_lr3.shape, x_dev_lr3.shape, y_train_lr3.shape, y_dev_lr3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8164935231499293"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_selected3 = grid_result_lr3.best_estimator_\n",
    "lr_selected3.fit(x_train_lr3, y_train_lr3)\n",
    "lr_selected_auc3 = roc_auc_score(y_dev_lr3, lr_selected3.predict_proba(x_dev_lr3)[:, 1])\n",
    "lr_selected_auc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_DENMARK_lr_selected = data_DENMARK['label']\n",
    "x_DENMARK_lr_selected = data_DENMARK.loc[:,[i for i in selected_feat_lr3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6920426065162907"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_selected_auc_DENMARK = roc_auc_score(y_DENMARK_lr_selected, lr_selected3.predict_proba(x_DENMARK_lr_selected)[:, 1])\n",
    "lr_selected_auc_DENMARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5366541353383458"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking_lr_DENMARK = get_stacking()\n",
    "stacking_lr_DENMARK.fit(x_train_lr3, y_train_lr3)\n",
    "stacking_lr_DENMARK_auc = roc_auc_score(y_DENMARK_lr_selected, stacking_lr_DENMARK.predict_proba(x_DENMARK_lr_selected)[:, 1])\n",
    "stacking_lr_DENMARK_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6099624060150376"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_lr_DENMARK = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gb', clf3), ('svc', clf4)], voting='soft')\n",
    "voting_lr_DENMARK.fit(x_train_lr3, y_train_lr3)\n",
    "voting_lr_DENMARK_auc = roc_auc_score(y_DENMARK_lr_selected, voting_lr_DENMARK.predict_proba(x_DENMARK_lr_selected)[:, 1])\n",
    "voting_lr_DENMARK_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training: E-Risk, AMDTSS, E-MTAB, Denmark\n",
    "# Testing: BSGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cg22695986</th>\n",
       "      <th>cg01193368</th>\n",
       "      <th>cg22056094</th>\n",
       "      <th>cg06098368</th>\n",
       "      <th>cg08690094</th>\n",
       "      <th>cg11236452</th>\n",
       "      <th>cg26916862</th>\n",
       "      <th>cg03124146</th>\n",
       "      <th>cg14770527</th>\n",
       "      <th>...</th>\n",
       "      <th>cg11174855</th>\n",
       "      <th>cg04524933</th>\n",
       "      <th>cg19418458</th>\n",
       "      <th>cg21808635</th>\n",
       "      <th>cg11359720</th>\n",
       "      <th>cg07635017</th>\n",
       "      <th>cg08641118</th>\n",
       "      <th>cg09166085</th>\n",
       "      <th>cg22034735</th>\n",
       "      <th>cg17805624</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.233353</td>\n",
       "      <td>0.606732</td>\n",
       "      <td>0.730107</td>\n",
       "      <td>0.240143</td>\n",
       "      <td>0.561082</td>\n",
       "      <td>0.533287</td>\n",
       "      <td>0.194961</td>\n",
       "      <td>0.144218</td>\n",
       "      <td>0.597436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327951</td>\n",
       "      <td>0.919347</td>\n",
       "      <td>0.365084</td>\n",
       "      <td>0.780742</td>\n",
       "      <td>0.731394</td>\n",
       "      <td>0.532118</td>\n",
       "      <td>0.081238</td>\n",
       "      <td>0.108676</td>\n",
       "      <td>0.508394</td>\n",
       "      <td>0.279428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.284813</td>\n",
       "      <td>0.599726</td>\n",
       "      <td>0.715363</td>\n",
       "      <td>0.242588</td>\n",
       "      <td>0.564277</td>\n",
       "      <td>0.578224</td>\n",
       "      <td>0.192382</td>\n",
       "      <td>0.139766</td>\n",
       "      <td>0.570267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195741</td>\n",
       "      <td>0.945236</td>\n",
       "      <td>0.337817</td>\n",
       "      <td>0.696424</td>\n",
       "      <td>0.745175</td>\n",
       "      <td>0.546033</td>\n",
       "      <td>0.099777</td>\n",
       "      <td>0.128168</td>\n",
       "      <td>0.498426</td>\n",
       "      <td>0.360946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.206618</td>\n",
       "      <td>0.552816</td>\n",
       "      <td>0.572559</td>\n",
       "      <td>0.169127</td>\n",
       "      <td>0.541453</td>\n",
       "      <td>0.509944</td>\n",
       "      <td>0.197505</td>\n",
       "      <td>0.193932</td>\n",
       "      <td>0.510173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083195</td>\n",
       "      <td>0.906838</td>\n",
       "      <td>0.357911</td>\n",
       "      <td>0.606163</td>\n",
       "      <td>0.773520</td>\n",
       "      <td>0.536528</td>\n",
       "      <td>0.079384</td>\n",
       "      <td>0.091236</td>\n",
       "      <td>0.204013</td>\n",
       "      <td>0.296357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.203151</td>\n",
       "      <td>0.655871</td>\n",
       "      <td>0.391728</td>\n",
       "      <td>0.224729</td>\n",
       "      <td>0.480992</td>\n",
       "      <td>0.421599</td>\n",
       "      <td>0.178132</td>\n",
       "      <td>0.181609</td>\n",
       "      <td>0.481978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134511</td>\n",
       "      <td>0.956986</td>\n",
       "      <td>0.583261</td>\n",
       "      <td>0.680310</td>\n",
       "      <td>0.753129</td>\n",
       "      <td>0.557829</td>\n",
       "      <td>0.067174</td>\n",
       "      <td>0.158550</td>\n",
       "      <td>0.297127</td>\n",
       "      <td>0.292479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.266709</td>\n",
       "      <td>0.493554</td>\n",
       "      <td>0.395203</td>\n",
       "      <td>0.231550</td>\n",
       "      <td>0.474545</td>\n",
       "      <td>0.381759</td>\n",
       "      <td>0.194651</td>\n",
       "      <td>0.170328</td>\n",
       "      <td>0.549200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069399</td>\n",
       "      <td>0.921778</td>\n",
       "      <td>0.333275</td>\n",
       "      <td>0.722998</td>\n",
       "      <td>0.824286</td>\n",
       "      <td>0.546425</td>\n",
       "      <td>0.075612</td>\n",
       "      <td>0.178922</td>\n",
       "      <td>0.526409</td>\n",
       "      <td>0.278850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.265920</td>\n",
       "      <td>0.668810</td>\n",
       "      <td>0.538530</td>\n",
       "      <td>0.221750</td>\n",
       "      <td>0.450940</td>\n",
       "      <td>0.409590</td>\n",
       "      <td>0.261340</td>\n",
       "      <td>0.202900</td>\n",
       "      <td>0.703690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074460</td>\n",
       "      <td>0.932110</td>\n",
       "      <td>0.309700</td>\n",
       "      <td>0.811640</td>\n",
       "      <td>0.651310</td>\n",
       "      <td>0.426310</td>\n",
       "      <td>0.106370</td>\n",
       "      <td>0.128560</td>\n",
       "      <td>0.381660</td>\n",
       "      <td>0.148490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.264220</td>\n",
       "      <td>0.673100</td>\n",
       "      <td>0.552430</td>\n",
       "      <td>0.211480</td>\n",
       "      <td>0.484940</td>\n",
       "      <td>0.468880</td>\n",
       "      <td>0.259820</td>\n",
       "      <td>0.240040</td>\n",
       "      <td>0.703450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033430</td>\n",
       "      <td>0.918230</td>\n",
       "      <td>0.273310</td>\n",
       "      <td>0.824450</td>\n",
       "      <td>0.609600</td>\n",
       "      <td>0.465400</td>\n",
       "      <td>0.121100</td>\n",
       "      <td>0.089490</td>\n",
       "      <td>0.315100</td>\n",
       "      <td>0.114360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.300860</td>\n",
       "      <td>0.586160</td>\n",
       "      <td>0.691140</td>\n",
       "      <td>0.240640</td>\n",
       "      <td>0.481040</td>\n",
       "      <td>0.579180</td>\n",
       "      <td>0.275920</td>\n",
       "      <td>0.234960</td>\n",
       "      <td>0.633500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117130</td>\n",
       "      <td>0.936460</td>\n",
       "      <td>0.436270</td>\n",
       "      <td>0.820790</td>\n",
       "      <td>0.597970</td>\n",
       "      <td>0.438150</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>0.214790</td>\n",
       "      <td>0.261410</td>\n",
       "      <td>0.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.289540</td>\n",
       "      <td>0.679960</td>\n",
       "      <td>0.553150</td>\n",
       "      <td>0.286020</td>\n",
       "      <td>0.315550</td>\n",
       "      <td>0.421130</td>\n",
       "      <td>0.231630</td>\n",
       "      <td>0.297830</td>\n",
       "      <td>0.680370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043240</td>\n",
       "      <td>0.931800</td>\n",
       "      <td>0.296000</td>\n",
       "      <td>0.798780</td>\n",
       "      <td>0.609940</td>\n",
       "      <td>0.394220</td>\n",
       "      <td>0.143910</td>\n",
       "      <td>0.143810</td>\n",
       "      <td>0.566440</td>\n",
       "      <td>0.156560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.255850</td>\n",
       "      <td>0.591080</td>\n",
       "      <td>0.525270</td>\n",
       "      <td>0.268260</td>\n",
       "      <td>0.343080</td>\n",
       "      <td>0.478540</td>\n",
       "      <td>0.231610</td>\n",
       "      <td>0.272100</td>\n",
       "      <td>0.707310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094330</td>\n",
       "      <td>0.924410</td>\n",
       "      <td>0.299930</td>\n",
       "      <td>0.865710</td>\n",
       "      <td>0.703540</td>\n",
       "      <td>0.399760</td>\n",
       "      <td>0.157860</td>\n",
       "      <td>0.165840</td>\n",
       "      <td>0.629880</td>\n",
       "      <td>0.119080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2272 rows × 834 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  cg22695986  cg01193368  cg22056094  cg06098368  cg08690094  \\\n",
       "0      1.0    0.233353    0.606732    0.730107    0.240143    0.561082   \n",
       "1      1.0    0.284813    0.599726    0.715363    0.242588    0.564277   \n",
       "2      1.0    0.206618    0.552816    0.572559    0.169127    0.541453   \n",
       "3      1.0    0.203151    0.655871    0.391728    0.224729    0.480992   \n",
       "4      0.0    0.266709    0.493554    0.395203    0.231550    0.474545   \n",
       "..     ...         ...         ...         ...         ...         ...   \n",
       "643    1.0    0.265920    0.668810    0.538530    0.221750    0.450940   \n",
       "644    1.0    0.264220    0.673100    0.552430    0.211480    0.484940   \n",
       "645    0.0    0.300860    0.586160    0.691140    0.240640    0.481040   \n",
       "646    0.0    0.289540    0.679960    0.553150    0.286020    0.315550   \n",
       "647    0.0    0.255850    0.591080    0.525270    0.268260    0.343080   \n",
       "\n",
       "     cg11236452  cg26916862  cg03124146  cg14770527  ...  cg11174855  \\\n",
       "0      0.533287    0.194961    0.144218    0.597436  ...    0.327951   \n",
       "1      0.578224    0.192382    0.139766    0.570267  ...    0.195741   \n",
       "2      0.509944    0.197505    0.193932    0.510173  ...    0.083195   \n",
       "3      0.421599    0.178132    0.181609    0.481978  ...    0.134511   \n",
       "4      0.381759    0.194651    0.170328    0.549200  ...    0.069399   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "643    0.409590    0.261340    0.202900    0.703690  ...    0.074460   \n",
       "644    0.468880    0.259820    0.240040    0.703450  ...    0.033430   \n",
       "645    0.579180    0.275920    0.234960    0.633500  ...    0.117130   \n",
       "646    0.421130    0.231630    0.297830    0.680370  ...    0.043240   \n",
       "647    0.478540    0.231610    0.272100    0.707310  ...    0.094330   \n",
       "\n",
       "     cg04524933  cg19418458  cg21808635  cg11359720  cg07635017  cg08641118  \\\n",
       "0      0.919347    0.365084    0.780742    0.731394    0.532118    0.081238   \n",
       "1      0.945236    0.337817    0.696424    0.745175    0.546033    0.099777   \n",
       "2      0.906838    0.357911    0.606163    0.773520    0.536528    0.079384   \n",
       "3      0.956986    0.583261    0.680310    0.753129    0.557829    0.067174   \n",
       "4      0.921778    0.333275    0.722998    0.824286    0.546425    0.075612   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "643    0.932110    0.309700    0.811640    0.651310    0.426310    0.106370   \n",
       "644    0.918230    0.273310    0.824450    0.609600    0.465400    0.121100   \n",
       "645    0.936460    0.436270    0.820790    0.597970    0.438150    0.134000   \n",
       "646    0.931800    0.296000    0.798780    0.609940    0.394220    0.143910   \n",
       "647    0.924410    0.299930    0.865710    0.703540    0.399760    0.157860   \n",
       "\n",
       "     cg09166085  cg22034735  cg17805624  \n",
       "0      0.108676    0.508394    0.279428  \n",
       "1      0.128168    0.498426    0.360946  \n",
       "2      0.091236    0.204013    0.296357  \n",
       "3      0.158550    0.297127    0.292479  \n",
       "4      0.178922    0.526409    0.278850  \n",
       "..          ...         ...         ...  \n",
       "643    0.128560    0.381660    0.148490  \n",
       "644    0.089490    0.315100    0.114360  \n",
       "645    0.214790    0.261410    0.127200  \n",
       "646    0.143810    0.566440    0.156560  \n",
       "647    0.165840    0.629880    0.119080  \n",
       "\n",
       "[2272 rows x 834 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2292 * 834\n",
    "train_data4 = pd.concat([data_ERISK, data_DENMARK, data_EMTAB])\n",
    "train_data4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cg22695986</th>\n",
       "      <th>cg01193368</th>\n",
       "      <th>cg22056094</th>\n",
       "      <th>cg06098368</th>\n",
       "      <th>cg08690094</th>\n",
       "      <th>cg11236452</th>\n",
       "      <th>cg26916862</th>\n",
       "      <th>cg03124146</th>\n",
       "      <th>cg14770527</th>\n",
       "      <th>...</th>\n",
       "      <th>cg11174855</th>\n",
       "      <th>cg04524933</th>\n",
       "      <th>cg19418458</th>\n",
       "      <th>cg21808635</th>\n",
       "      <th>cg11359720</th>\n",
       "      <th>cg07635017</th>\n",
       "      <th>cg08641118</th>\n",
       "      <th>cg09166085</th>\n",
       "      <th>cg22034735</th>\n",
       "      <th>cg17805624</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.233353</td>\n",
       "      <td>0.606732</td>\n",
       "      <td>0.730107</td>\n",
       "      <td>0.240143</td>\n",
       "      <td>0.561082</td>\n",
       "      <td>0.533287</td>\n",
       "      <td>0.194961</td>\n",
       "      <td>0.144218</td>\n",
       "      <td>0.597436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327951</td>\n",
       "      <td>0.919347</td>\n",
       "      <td>0.365084</td>\n",
       "      <td>0.780742</td>\n",
       "      <td>0.731394</td>\n",
       "      <td>0.532118</td>\n",
       "      <td>0.081238</td>\n",
       "      <td>0.108676</td>\n",
       "      <td>0.508394</td>\n",
       "      <td>0.279428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.284813</td>\n",
       "      <td>0.599726</td>\n",
       "      <td>0.715363</td>\n",
       "      <td>0.242588</td>\n",
       "      <td>0.564277</td>\n",
       "      <td>0.578224</td>\n",
       "      <td>0.192382</td>\n",
       "      <td>0.139766</td>\n",
       "      <td>0.570267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195741</td>\n",
       "      <td>0.945236</td>\n",
       "      <td>0.337817</td>\n",
       "      <td>0.696424</td>\n",
       "      <td>0.745175</td>\n",
       "      <td>0.546033</td>\n",
       "      <td>0.099777</td>\n",
       "      <td>0.128168</td>\n",
       "      <td>0.498426</td>\n",
       "      <td>0.360946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.206618</td>\n",
       "      <td>0.552816</td>\n",
       "      <td>0.572559</td>\n",
       "      <td>0.169127</td>\n",
       "      <td>0.541453</td>\n",
       "      <td>0.509944</td>\n",
       "      <td>0.197505</td>\n",
       "      <td>0.193932</td>\n",
       "      <td>0.510173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083195</td>\n",
       "      <td>0.906838</td>\n",
       "      <td>0.357911</td>\n",
       "      <td>0.606163</td>\n",
       "      <td>0.773520</td>\n",
       "      <td>0.536528</td>\n",
       "      <td>0.079384</td>\n",
       "      <td>0.091236</td>\n",
       "      <td>0.204013</td>\n",
       "      <td>0.296357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.203151</td>\n",
       "      <td>0.655871</td>\n",
       "      <td>0.391728</td>\n",
       "      <td>0.224729</td>\n",
       "      <td>0.480992</td>\n",
       "      <td>0.421599</td>\n",
       "      <td>0.178132</td>\n",
       "      <td>0.181609</td>\n",
       "      <td>0.481978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134511</td>\n",
       "      <td>0.956986</td>\n",
       "      <td>0.583261</td>\n",
       "      <td>0.680310</td>\n",
       "      <td>0.753129</td>\n",
       "      <td>0.557829</td>\n",
       "      <td>0.067174</td>\n",
       "      <td>0.158550</td>\n",
       "      <td>0.297127</td>\n",
       "      <td>0.292479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.266709</td>\n",
       "      <td>0.493554</td>\n",
       "      <td>0.395203</td>\n",
       "      <td>0.231550</td>\n",
       "      <td>0.474545</td>\n",
       "      <td>0.381759</td>\n",
       "      <td>0.194651</td>\n",
       "      <td>0.170328</td>\n",
       "      <td>0.549200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069399</td>\n",
       "      <td>0.921778</td>\n",
       "      <td>0.333275</td>\n",
       "      <td>0.722998</td>\n",
       "      <td>0.824286</td>\n",
       "      <td>0.546425</td>\n",
       "      <td>0.075612</td>\n",
       "      <td>0.178922</td>\n",
       "      <td>0.526409</td>\n",
       "      <td>0.278850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.265920</td>\n",
       "      <td>0.668810</td>\n",
       "      <td>0.538530</td>\n",
       "      <td>0.221750</td>\n",
       "      <td>0.450940</td>\n",
       "      <td>0.409590</td>\n",
       "      <td>0.261340</td>\n",
       "      <td>0.202900</td>\n",
       "      <td>0.703690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074460</td>\n",
       "      <td>0.932110</td>\n",
       "      <td>0.309700</td>\n",
       "      <td>0.811640</td>\n",
       "      <td>0.651310</td>\n",
       "      <td>0.426310</td>\n",
       "      <td>0.106370</td>\n",
       "      <td>0.128560</td>\n",
       "      <td>0.381660</td>\n",
       "      <td>0.148490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.264220</td>\n",
       "      <td>0.673100</td>\n",
       "      <td>0.552430</td>\n",
       "      <td>0.211480</td>\n",
       "      <td>0.484940</td>\n",
       "      <td>0.468880</td>\n",
       "      <td>0.259820</td>\n",
       "      <td>0.240040</td>\n",
       "      <td>0.703450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033430</td>\n",
       "      <td>0.918230</td>\n",
       "      <td>0.273310</td>\n",
       "      <td>0.824450</td>\n",
       "      <td>0.609600</td>\n",
       "      <td>0.465400</td>\n",
       "      <td>0.121100</td>\n",
       "      <td>0.089490</td>\n",
       "      <td>0.315100</td>\n",
       "      <td>0.114360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.300860</td>\n",
       "      <td>0.586160</td>\n",
       "      <td>0.691140</td>\n",
       "      <td>0.240640</td>\n",
       "      <td>0.481040</td>\n",
       "      <td>0.579180</td>\n",
       "      <td>0.275920</td>\n",
       "      <td>0.234960</td>\n",
       "      <td>0.633500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117130</td>\n",
       "      <td>0.936460</td>\n",
       "      <td>0.436270</td>\n",
       "      <td>0.820790</td>\n",
       "      <td>0.597970</td>\n",
       "      <td>0.438150</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>0.214790</td>\n",
       "      <td>0.261410</td>\n",
       "      <td>0.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.289540</td>\n",
       "      <td>0.679960</td>\n",
       "      <td>0.553150</td>\n",
       "      <td>0.286020</td>\n",
       "      <td>0.315550</td>\n",
       "      <td>0.421130</td>\n",
       "      <td>0.231630</td>\n",
       "      <td>0.297830</td>\n",
       "      <td>0.680370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043240</td>\n",
       "      <td>0.931800</td>\n",
       "      <td>0.296000</td>\n",
       "      <td>0.798780</td>\n",
       "      <td>0.609940</td>\n",
       "      <td>0.394220</td>\n",
       "      <td>0.143910</td>\n",
       "      <td>0.143810</td>\n",
       "      <td>0.566440</td>\n",
       "      <td>0.156560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.255850</td>\n",
       "      <td>0.591080</td>\n",
       "      <td>0.525270</td>\n",
       "      <td>0.268260</td>\n",
       "      <td>0.343080</td>\n",
       "      <td>0.478540</td>\n",
       "      <td>0.231610</td>\n",
       "      <td>0.272100</td>\n",
       "      <td>0.707310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094330</td>\n",
       "      <td>0.924410</td>\n",
       "      <td>0.299930</td>\n",
       "      <td>0.865710</td>\n",
       "      <td>0.703540</td>\n",
       "      <td>0.399760</td>\n",
       "      <td>0.157860</td>\n",
       "      <td>0.165840</td>\n",
       "      <td>0.629880</td>\n",
       "      <td>0.119080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2272 rows × 832 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  cg22695986  cg01193368  cg22056094  cg06098368  cg08690094  \\\n",
       "0      1.0    0.233353    0.606732    0.730107    0.240143    0.561082   \n",
       "1      1.0    0.284813    0.599726    0.715363    0.242588    0.564277   \n",
       "2      1.0    0.206618    0.552816    0.572559    0.169127    0.541453   \n",
       "3      1.0    0.203151    0.655871    0.391728    0.224729    0.480992   \n",
       "4      0.0    0.266709    0.493554    0.395203    0.231550    0.474545   \n",
       "..     ...         ...         ...         ...         ...         ...   \n",
       "643    1.0    0.265920    0.668810    0.538530    0.221750    0.450940   \n",
       "644    1.0    0.264220    0.673100    0.552430    0.211480    0.484940   \n",
       "645    0.0    0.300860    0.586160    0.691140    0.240640    0.481040   \n",
       "646    0.0    0.289540    0.679960    0.553150    0.286020    0.315550   \n",
       "647    0.0    0.255850    0.591080    0.525270    0.268260    0.343080   \n",
       "\n",
       "     cg11236452  cg26916862  cg03124146  cg14770527  ...  cg11174855  \\\n",
       "0      0.533287    0.194961    0.144218    0.597436  ...    0.327951   \n",
       "1      0.578224    0.192382    0.139766    0.570267  ...    0.195741   \n",
       "2      0.509944    0.197505    0.193932    0.510173  ...    0.083195   \n",
       "3      0.421599    0.178132    0.181609    0.481978  ...    0.134511   \n",
       "4      0.381759    0.194651    0.170328    0.549200  ...    0.069399   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "643    0.409590    0.261340    0.202900    0.703690  ...    0.074460   \n",
       "644    0.468880    0.259820    0.240040    0.703450  ...    0.033430   \n",
       "645    0.579180    0.275920    0.234960    0.633500  ...    0.117130   \n",
       "646    0.421130    0.231630    0.297830    0.680370  ...    0.043240   \n",
       "647    0.478540    0.231610    0.272100    0.707310  ...    0.094330   \n",
       "\n",
       "     cg04524933  cg19418458  cg21808635  cg11359720  cg07635017  cg08641118  \\\n",
       "0      0.919347    0.365084    0.780742    0.731394    0.532118    0.081238   \n",
       "1      0.945236    0.337817    0.696424    0.745175    0.546033    0.099777   \n",
       "2      0.906838    0.357911    0.606163    0.773520    0.536528    0.079384   \n",
       "3      0.956986    0.583261    0.680310    0.753129    0.557829    0.067174   \n",
       "4      0.921778    0.333275    0.722998    0.824286    0.546425    0.075612   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "643    0.932110    0.309700    0.811640    0.651310    0.426310    0.106370   \n",
       "644    0.918230    0.273310    0.824450    0.609600    0.465400    0.121100   \n",
       "645    0.936460    0.436270    0.820790    0.597970    0.438150    0.134000   \n",
       "646    0.931800    0.296000    0.798780    0.609940    0.394220    0.143910   \n",
       "647    0.924410    0.299930    0.865710    0.703540    0.399760    0.157860   \n",
       "\n",
       "     cg09166085  cg22034735  cg17805624  \n",
       "0      0.108676    0.508394    0.279428  \n",
       "1      0.128168    0.498426    0.360946  \n",
       "2      0.091236    0.204013    0.296357  \n",
       "3      0.158550    0.297127    0.292479  \n",
       "4      0.178922    0.526409    0.278850  \n",
       "..          ...         ...         ...  \n",
       "643    0.128560    0.381660    0.148490  \n",
       "644    0.089490    0.315100    0.114360  \n",
       "645    0.214790    0.261410    0.127200  \n",
       "646    0.143810    0.566440    0.156560  \n",
       "647    0.165840    0.629880    0.119080  \n",
       "\n",
       "[2272 rows x 832 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2292 * 832\n",
    "train_data4 = train_data4.loc[:,[i for i in data_AMDTSS.columns]]\n",
    "train_data4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cg22695986</th>\n",
       "      <th>cg01193368</th>\n",
       "      <th>cg22056094</th>\n",
       "      <th>cg06098368</th>\n",
       "      <th>cg08690094</th>\n",
       "      <th>cg11236452</th>\n",
       "      <th>cg26916862</th>\n",
       "      <th>cg03124146</th>\n",
       "      <th>cg14770527</th>\n",
       "      <th>...</th>\n",
       "      <th>cg11174855</th>\n",
       "      <th>cg04524933</th>\n",
       "      <th>cg19418458</th>\n",
       "      <th>cg21808635</th>\n",
       "      <th>cg11359720</th>\n",
       "      <th>cg07635017</th>\n",
       "      <th>cg08641118</th>\n",
       "      <th>cg09166085</th>\n",
       "      <th>cg22034735</th>\n",
       "      <th>cg17805624</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.233353</td>\n",
       "      <td>0.606732</td>\n",
       "      <td>0.730107</td>\n",
       "      <td>0.240143</td>\n",
       "      <td>0.561082</td>\n",
       "      <td>0.533287</td>\n",
       "      <td>0.194961</td>\n",
       "      <td>0.144218</td>\n",
       "      <td>0.597436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327951</td>\n",
       "      <td>0.919347</td>\n",
       "      <td>0.365084</td>\n",
       "      <td>0.780742</td>\n",
       "      <td>0.731394</td>\n",
       "      <td>0.532118</td>\n",
       "      <td>0.081238</td>\n",
       "      <td>0.108676</td>\n",
       "      <td>0.508394</td>\n",
       "      <td>0.279428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.284813</td>\n",
       "      <td>0.599726</td>\n",
       "      <td>0.715363</td>\n",
       "      <td>0.242588</td>\n",
       "      <td>0.564277</td>\n",
       "      <td>0.578224</td>\n",
       "      <td>0.192382</td>\n",
       "      <td>0.139766</td>\n",
       "      <td>0.570267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195741</td>\n",
       "      <td>0.945236</td>\n",
       "      <td>0.337817</td>\n",
       "      <td>0.696424</td>\n",
       "      <td>0.745175</td>\n",
       "      <td>0.546033</td>\n",
       "      <td>0.099777</td>\n",
       "      <td>0.128168</td>\n",
       "      <td>0.498426</td>\n",
       "      <td>0.360946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.206618</td>\n",
       "      <td>0.552816</td>\n",
       "      <td>0.572559</td>\n",
       "      <td>0.169127</td>\n",
       "      <td>0.541453</td>\n",
       "      <td>0.509944</td>\n",
       "      <td>0.197505</td>\n",
       "      <td>0.193932</td>\n",
       "      <td>0.510173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083195</td>\n",
       "      <td>0.906838</td>\n",
       "      <td>0.357911</td>\n",
       "      <td>0.606163</td>\n",
       "      <td>0.773520</td>\n",
       "      <td>0.536528</td>\n",
       "      <td>0.079384</td>\n",
       "      <td>0.091236</td>\n",
       "      <td>0.204013</td>\n",
       "      <td>0.296357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.203151</td>\n",
       "      <td>0.655871</td>\n",
       "      <td>0.391728</td>\n",
       "      <td>0.224729</td>\n",
       "      <td>0.480992</td>\n",
       "      <td>0.421599</td>\n",
       "      <td>0.178132</td>\n",
       "      <td>0.181609</td>\n",
       "      <td>0.481978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134511</td>\n",
       "      <td>0.956986</td>\n",
       "      <td>0.583261</td>\n",
       "      <td>0.680310</td>\n",
       "      <td>0.753129</td>\n",
       "      <td>0.557829</td>\n",
       "      <td>0.067174</td>\n",
       "      <td>0.158550</td>\n",
       "      <td>0.297127</td>\n",
       "      <td>0.292479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.266709</td>\n",
       "      <td>0.493554</td>\n",
       "      <td>0.395203</td>\n",
       "      <td>0.231550</td>\n",
       "      <td>0.474545</td>\n",
       "      <td>0.381759</td>\n",
       "      <td>0.194651</td>\n",
       "      <td>0.170328</td>\n",
       "      <td>0.549200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069399</td>\n",
       "      <td>0.921778</td>\n",
       "      <td>0.333275</td>\n",
       "      <td>0.722998</td>\n",
       "      <td>0.824286</td>\n",
       "      <td>0.546425</td>\n",
       "      <td>0.075612</td>\n",
       "      <td>0.178922</td>\n",
       "      <td>0.526409</td>\n",
       "      <td>0.278850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.421692</td>\n",
       "      <td>0.819136</td>\n",
       "      <td>0.889120</td>\n",
       "      <td>0.274042</td>\n",
       "      <td>0.525204</td>\n",
       "      <td>0.698463</td>\n",
       "      <td>0.415842</td>\n",
       "      <td>0.387231</td>\n",
       "      <td>0.759353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285387</td>\n",
       "      <td>0.953237</td>\n",
       "      <td>0.542180</td>\n",
       "      <td>0.869079</td>\n",
       "      <td>0.713411</td>\n",
       "      <td>0.647941</td>\n",
       "      <td>0.176504</td>\n",
       "      <td>0.168747</td>\n",
       "      <td>0.529537</td>\n",
       "      <td>0.275265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.155194</td>\n",
       "      <td>0.650936</td>\n",
       "      <td>0.436741</td>\n",
       "      <td>0.127649</td>\n",
       "      <td>0.458996</td>\n",
       "      <td>0.414456</td>\n",
       "      <td>0.214664</td>\n",
       "      <td>0.116075</td>\n",
       "      <td>0.700554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098905</td>\n",
       "      <td>0.945503</td>\n",
       "      <td>0.361169</td>\n",
       "      <td>0.766231</td>\n",
       "      <td>0.652946</td>\n",
       "      <td>0.603185</td>\n",
       "      <td>0.143384</td>\n",
       "      <td>0.172315</td>\n",
       "      <td>0.522733</td>\n",
       "      <td>0.230949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.170119</td>\n",
       "      <td>0.668992</td>\n",
       "      <td>0.532885</td>\n",
       "      <td>0.141059</td>\n",
       "      <td>0.522190</td>\n",
       "      <td>0.469892</td>\n",
       "      <td>0.177975</td>\n",
       "      <td>0.141652</td>\n",
       "      <td>0.686412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208649</td>\n",
       "      <td>0.962932</td>\n",
       "      <td>0.609414</td>\n",
       "      <td>0.827691</td>\n",
       "      <td>0.850505</td>\n",
       "      <td>0.631754</td>\n",
       "      <td>0.077538</td>\n",
       "      <td>0.129992</td>\n",
       "      <td>0.617378</td>\n",
       "      <td>0.242766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.316241</td>\n",
       "      <td>0.642261</td>\n",
       "      <td>0.906532</td>\n",
       "      <td>0.303791</td>\n",
       "      <td>0.403003</td>\n",
       "      <td>0.830418</td>\n",
       "      <td>0.242378</td>\n",
       "      <td>0.278751</td>\n",
       "      <td>0.764943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223300</td>\n",
       "      <td>0.917177</td>\n",
       "      <td>0.357777</td>\n",
       "      <td>0.776557</td>\n",
       "      <td>0.607103</td>\n",
       "      <td>0.595563</td>\n",
       "      <td>0.108415</td>\n",
       "      <td>0.338011</td>\n",
       "      <td>0.444886</td>\n",
       "      <td>0.302313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.356544</td>\n",
       "      <td>0.780978</td>\n",
       "      <td>0.565044</td>\n",
       "      <td>0.301901</td>\n",
       "      <td>0.552029</td>\n",
       "      <td>0.542632</td>\n",
       "      <td>0.365609</td>\n",
       "      <td>0.374958</td>\n",
       "      <td>0.796539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231661</td>\n",
       "      <td>0.958034</td>\n",
       "      <td>0.734586</td>\n",
       "      <td>0.737711</td>\n",
       "      <td>0.774517</td>\n",
       "      <td>0.587225</td>\n",
       "      <td>0.164567</td>\n",
       "      <td>0.167376</td>\n",
       "      <td>0.740924</td>\n",
       "      <td>0.223064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2536 rows × 832 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  cg22695986  cg01193368  cg22056094  cg06098368  cg08690094  \\\n",
       "0      1.0    0.233353    0.606732    0.730107    0.240143    0.561082   \n",
       "1      1.0    0.284813    0.599726    0.715363    0.242588    0.564277   \n",
       "2      1.0    0.206618    0.552816    0.572559    0.169127    0.541453   \n",
       "3      1.0    0.203151    0.655871    0.391728    0.224729    0.480992   \n",
       "4      0.0    0.266709    0.493554    0.395203    0.231550    0.474545   \n",
       "..     ...         ...         ...         ...         ...         ...   \n",
       "471    0.0    0.421692    0.819136    0.889120    0.274042    0.525204   \n",
       "472    1.0    0.155194    0.650936    0.436741    0.127649    0.458996   \n",
       "475    1.0    0.170119    0.668992    0.532885    0.141059    0.522190   \n",
       "477    0.0    0.316241    0.642261    0.906532    0.303791    0.403003   \n",
       "478    1.0    0.356544    0.780978    0.565044    0.301901    0.552029   \n",
       "\n",
       "     cg11236452  cg26916862  cg03124146  cg14770527  ...  cg11174855  \\\n",
       "0      0.533287    0.194961    0.144218    0.597436  ...    0.327951   \n",
       "1      0.578224    0.192382    0.139766    0.570267  ...    0.195741   \n",
       "2      0.509944    0.197505    0.193932    0.510173  ...    0.083195   \n",
       "3      0.421599    0.178132    0.181609    0.481978  ...    0.134511   \n",
       "4      0.381759    0.194651    0.170328    0.549200  ...    0.069399   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "471    0.698463    0.415842    0.387231    0.759353  ...    0.285387   \n",
       "472    0.414456    0.214664    0.116075    0.700554  ...    0.098905   \n",
       "475    0.469892    0.177975    0.141652    0.686412  ...    0.208649   \n",
       "477    0.830418    0.242378    0.278751    0.764943  ...    0.223300   \n",
       "478    0.542632    0.365609    0.374958    0.796539  ...    0.231661   \n",
       "\n",
       "     cg04524933  cg19418458  cg21808635  cg11359720  cg07635017  cg08641118  \\\n",
       "0      0.919347    0.365084    0.780742    0.731394    0.532118    0.081238   \n",
       "1      0.945236    0.337817    0.696424    0.745175    0.546033    0.099777   \n",
       "2      0.906838    0.357911    0.606163    0.773520    0.536528    0.079384   \n",
       "3      0.956986    0.583261    0.680310    0.753129    0.557829    0.067174   \n",
       "4      0.921778    0.333275    0.722998    0.824286    0.546425    0.075612   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "471    0.953237    0.542180    0.869079    0.713411    0.647941    0.176504   \n",
       "472    0.945503    0.361169    0.766231    0.652946    0.603185    0.143384   \n",
       "475    0.962932    0.609414    0.827691    0.850505    0.631754    0.077538   \n",
       "477    0.917177    0.357777    0.776557    0.607103    0.595563    0.108415   \n",
       "478    0.958034    0.734586    0.737711    0.774517    0.587225    0.164567   \n",
       "\n",
       "     cg09166085  cg22034735  cg17805624  \n",
       "0      0.108676    0.508394    0.279428  \n",
       "1      0.128168    0.498426    0.360946  \n",
       "2      0.091236    0.204013    0.296357  \n",
       "3      0.158550    0.297127    0.292479  \n",
       "4      0.178922    0.526409    0.278850  \n",
       "..          ...         ...         ...  \n",
       "471    0.168747    0.529537    0.275265  \n",
       "472    0.172315    0.522733    0.230949  \n",
       "475    0.129992    0.617378    0.242766  \n",
       "477    0.338011    0.444886    0.302313  \n",
       "478    0.167376    0.740924    0.223064  \n",
       "\n",
       "[2536 rows x 832 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2556 * 832\n",
    "train_data4 = pd.concat([train_data4, data_AMDTSS])\n",
    "train_data4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1902, 831), (634, 831), (1902,), (634,))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training 75% developing 25%\n",
    "x_train4, x_dev4, y_train4, y_dev4 = train_test_split(train_data4.drop(columns=['label']), train_data4['label'])\n",
    "x_train4.shape, x_dev4.shape, y_train4.shape, y_dev4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Original Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8167414050822123"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en4 = LogisticRegression(penalty = \"elasticnet\", solver = \"saga\", l1_ratio = 0.5)\n",
    "en4.fit(x_train4, y_train4)\n",
    "y_BSGS = data_BSGS['label']\n",
    "x_BSGS = data_BSGS.loc[:,[i for i in data_AMDTSS.columns]]\n",
    "x_BSGS = x_BSGS.drop(['label'], axis = 1)\n",
    "en_auc4 = roc_auc_score(y_BSGS, en4.predict_proba(x_BSGS)[:, 1])\n",
    "en_auc4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train New Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Selection by RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=1000, min_impurity_decrease=1e-06,\n",
       "                       n_estimators=500)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid search for rf\n",
    "# The number of trees in the forest.\n",
    "n_estimators = [50, 100, 200, 300, 500]\n",
    "# The function to measure the quality of a split\n",
    "criterion = [\"gini\", \"entropy\"]\n",
    "# A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "min_impurity_decrease = [0.1, 0.000001, 0.00001]\n",
    "# The maximum depth of the tree.\n",
    "max_depth = [20, 50, 100, 500, 1000]\n",
    "\n",
    "param_distributions = dict(n_estimators = n_estimators, criterion = criterion, min_impurity_decrease = min_impurity_decrease, max_depth = max_depth)\n",
    "rf = RandomForestClassifier()\n",
    "grid_rf4 = RandomizedSearchCV(estimator = rf, param_distributions = param_distributions, scoring = \"roc_auc\",\n",
    "                          verbose = 1, n_jobs = -1) \n",
    "grid_result_rf4 = grid_rf4.fit(x_train4, y_train4) \n",
    "\n",
    "grid_result_rf4.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=RandomForestClassifier(max_depth=1000,\n",
       "                                                 min_impurity_decrease=1e-06,\n",
       "                                                 n_estimators=500))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variable selection by random forest\n",
    "rf_selection4 = SelectFromModel(grid_result_rf4.best_estimator_)\n",
    "rf_selection4.fit(x_train4, y_train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selected variables\n",
    "selected_feat_rf4 = x_train4.columns[(rf_selection4.get_support())]\n",
    "len(selected_feat_rf4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cg22695986</th>\n",
       "      <th>cg01193368</th>\n",
       "      <th>cg22056094</th>\n",
       "      <th>cg06098368</th>\n",
       "      <th>cg08690094</th>\n",
       "      <th>cg11236452</th>\n",
       "      <th>cg26916862</th>\n",
       "      <th>cg03124146</th>\n",
       "      <th>cg14770527</th>\n",
       "      <th>cg23316599</th>\n",
       "      <th>...</th>\n",
       "      <th>cg22381068</th>\n",
       "      <th>cg24686497</th>\n",
       "      <th>cg06427838</th>\n",
       "      <th>cg06100807</th>\n",
       "      <th>cg15489799</th>\n",
       "      <th>cg09990584</th>\n",
       "      <th>cg15236528</th>\n",
       "      <th>cg03556669</th>\n",
       "      <th>cg21808635</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.233353</td>\n",
       "      <td>0.606732</td>\n",
       "      <td>0.730107</td>\n",
       "      <td>0.240143</td>\n",
       "      <td>0.561082</td>\n",
       "      <td>0.533287</td>\n",
       "      <td>0.194961</td>\n",
       "      <td>0.144218</td>\n",
       "      <td>0.597436</td>\n",
       "      <td>0.396608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267603</td>\n",
       "      <td>0.065910</td>\n",
       "      <td>0.089080</td>\n",
       "      <td>0.084025</td>\n",
       "      <td>0.434138</td>\n",
       "      <td>0.342347</td>\n",
       "      <td>0.156715</td>\n",
       "      <td>0.305896</td>\n",
       "      <td>0.780742</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.284813</td>\n",
       "      <td>0.599726</td>\n",
       "      <td>0.715363</td>\n",
       "      <td>0.242588</td>\n",
       "      <td>0.564277</td>\n",
       "      <td>0.578224</td>\n",
       "      <td>0.192382</td>\n",
       "      <td>0.139766</td>\n",
       "      <td>0.570267</td>\n",
       "      <td>0.419441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290622</td>\n",
       "      <td>0.068113</td>\n",
       "      <td>0.078382</td>\n",
       "      <td>0.062799</td>\n",
       "      <td>0.456402</td>\n",
       "      <td>0.321944</td>\n",
       "      <td>0.220170</td>\n",
       "      <td>0.313702</td>\n",
       "      <td>0.696424</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.206618</td>\n",
       "      <td>0.552816</td>\n",
       "      <td>0.572559</td>\n",
       "      <td>0.169127</td>\n",
       "      <td>0.541453</td>\n",
       "      <td>0.509944</td>\n",
       "      <td>0.197505</td>\n",
       "      <td>0.193932</td>\n",
       "      <td>0.510173</td>\n",
       "      <td>0.479463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240754</td>\n",
       "      <td>0.055055</td>\n",
       "      <td>0.044524</td>\n",
       "      <td>0.052757</td>\n",
       "      <td>0.476032</td>\n",
       "      <td>0.327677</td>\n",
       "      <td>0.178511</td>\n",
       "      <td>0.230778</td>\n",
       "      <td>0.606163</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.203151</td>\n",
       "      <td>0.655871</td>\n",
       "      <td>0.391728</td>\n",
       "      <td>0.224729</td>\n",
       "      <td>0.480992</td>\n",
       "      <td>0.421599</td>\n",
       "      <td>0.178132</td>\n",
       "      <td>0.181609</td>\n",
       "      <td>0.481978</td>\n",
       "      <td>0.510254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281359</td>\n",
       "      <td>0.059226</td>\n",
       "      <td>0.088794</td>\n",
       "      <td>0.061023</td>\n",
       "      <td>0.461083</td>\n",
       "      <td>0.303911</td>\n",
       "      <td>0.086153</td>\n",
       "      <td>0.257855</td>\n",
       "      <td>0.680310</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.266709</td>\n",
       "      <td>0.493554</td>\n",
       "      <td>0.395203</td>\n",
       "      <td>0.231550</td>\n",
       "      <td>0.474545</td>\n",
       "      <td>0.381759</td>\n",
       "      <td>0.194651</td>\n",
       "      <td>0.170328</td>\n",
       "      <td>0.549200</td>\n",
       "      <td>0.506738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269913</td>\n",
       "      <td>0.205009</td>\n",
       "      <td>0.073908</td>\n",
       "      <td>0.051648</td>\n",
       "      <td>0.502171</td>\n",
       "      <td>0.333914</td>\n",
       "      <td>0.137047</td>\n",
       "      <td>0.380755</td>\n",
       "      <td>0.722998</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>0.421692</td>\n",
       "      <td>0.819136</td>\n",
       "      <td>0.889120</td>\n",
       "      <td>0.274042</td>\n",
       "      <td>0.525204</td>\n",
       "      <td>0.698463</td>\n",
       "      <td>0.415842</td>\n",
       "      <td>0.387231</td>\n",
       "      <td>0.759353</td>\n",
       "      <td>0.573460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300424</td>\n",
       "      <td>0.079161</td>\n",
       "      <td>0.037285</td>\n",
       "      <td>0.094577</td>\n",
       "      <td>0.546019</td>\n",
       "      <td>0.385300</td>\n",
       "      <td>0.128186</td>\n",
       "      <td>0.425339</td>\n",
       "      <td>0.869079</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>0.155194</td>\n",
       "      <td>0.650936</td>\n",
       "      <td>0.436741</td>\n",
       "      <td>0.127649</td>\n",
       "      <td>0.458996</td>\n",
       "      <td>0.414456</td>\n",
       "      <td>0.214664</td>\n",
       "      <td>0.116075</td>\n",
       "      <td>0.700554</td>\n",
       "      <td>0.475778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.330803</td>\n",
       "      <td>0.041471</td>\n",
       "      <td>0.033304</td>\n",
       "      <td>0.041635</td>\n",
       "      <td>0.478794</td>\n",
       "      <td>0.347106</td>\n",
       "      <td>0.134853</td>\n",
       "      <td>0.290931</td>\n",
       "      <td>0.766231</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0.170119</td>\n",
       "      <td>0.668992</td>\n",
       "      <td>0.532885</td>\n",
       "      <td>0.141059</td>\n",
       "      <td>0.522190</td>\n",
       "      <td>0.469892</td>\n",
       "      <td>0.177975</td>\n",
       "      <td>0.141652</td>\n",
       "      <td>0.686412</td>\n",
       "      <td>0.454601</td>\n",
       "      <td>...</td>\n",
       "      <td>0.336278</td>\n",
       "      <td>0.044382</td>\n",
       "      <td>0.207561</td>\n",
       "      <td>0.024130</td>\n",
       "      <td>0.603469</td>\n",
       "      <td>0.507703</td>\n",
       "      <td>0.146198</td>\n",
       "      <td>0.502986</td>\n",
       "      <td>0.827691</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0.316241</td>\n",
       "      <td>0.642261</td>\n",
       "      <td>0.906532</td>\n",
       "      <td>0.303791</td>\n",
       "      <td>0.403003</td>\n",
       "      <td>0.830418</td>\n",
       "      <td>0.242378</td>\n",
       "      <td>0.278751</td>\n",
       "      <td>0.764943</td>\n",
       "      <td>0.376560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.353845</td>\n",
       "      <td>0.067505</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.045483</td>\n",
       "      <td>0.594803</td>\n",
       "      <td>0.452558</td>\n",
       "      <td>0.130743</td>\n",
       "      <td>0.225473</td>\n",
       "      <td>0.776557</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0.356544</td>\n",
       "      <td>0.780978</td>\n",
       "      <td>0.565044</td>\n",
       "      <td>0.301901</td>\n",
       "      <td>0.552029</td>\n",
       "      <td>0.542632</td>\n",
       "      <td>0.365609</td>\n",
       "      <td>0.374958</td>\n",
       "      <td>0.796539</td>\n",
       "      <td>0.529338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340431</td>\n",
       "      <td>0.048163</td>\n",
       "      <td>0.012417</td>\n",
       "      <td>0.022582</td>\n",
       "      <td>0.511126</td>\n",
       "      <td>0.379746</td>\n",
       "      <td>0.173264</td>\n",
       "      <td>0.396071</td>\n",
       "      <td>0.737711</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2536 rows × 286 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cg22695986  cg01193368  cg22056094  cg06098368  cg08690094  cg11236452  \\\n",
       "0      0.233353    0.606732    0.730107    0.240143    0.561082    0.533287   \n",
       "1      0.284813    0.599726    0.715363    0.242588    0.564277    0.578224   \n",
       "2      0.206618    0.552816    0.572559    0.169127    0.541453    0.509944   \n",
       "3      0.203151    0.655871    0.391728    0.224729    0.480992    0.421599   \n",
       "4      0.266709    0.493554    0.395203    0.231550    0.474545    0.381759   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "471    0.421692    0.819136    0.889120    0.274042    0.525204    0.698463   \n",
       "472    0.155194    0.650936    0.436741    0.127649    0.458996    0.414456   \n",
       "475    0.170119    0.668992    0.532885    0.141059    0.522190    0.469892   \n",
       "477    0.316241    0.642261    0.906532    0.303791    0.403003    0.830418   \n",
       "478    0.356544    0.780978    0.565044    0.301901    0.552029    0.542632   \n",
       "\n",
       "     cg26916862  cg03124146  cg14770527  cg23316599  ...  cg22381068  \\\n",
       "0      0.194961    0.144218    0.597436    0.396608  ...    0.267603   \n",
       "1      0.192382    0.139766    0.570267    0.419441  ...    0.290622   \n",
       "2      0.197505    0.193932    0.510173    0.479463  ...    0.240754   \n",
       "3      0.178132    0.181609    0.481978    0.510254  ...    0.281359   \n",
       "4      0.194651    0.170328    0.549200    0.506738  ...    0.269913   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "471    0.415842    0.387231    0.759353    0.573460  ...    0.300424   \n",
       "472    0.214664    0.116075    0.700554    0.475778  ...    0.330803   \n",
       "475    0.177975    0.141652    0.686412    0.454601  ...    0.336278   \n",
       "477    0.242378    0.278751    0.764943    0.376560  ...    0.353845   \n",
       "478    0.365609    0.374958    0.796539    0.529338  ...    0.340431   \n",
       "\n",
       "     cg24686497  cg06427838  cg06100807  cg15489799  cg09990584  cg15236528  \\\n",
       "0      0.065910    0.089080    0.084025    0.434138    0.342347    0.156715   \n",
       "1      0.068113    0.078382    0.062799    0.456402    0.321944    0.220170   \n",
       "2      0.055055    0.044524    0.052757    0.476032    0.327677    0.178511   \n",
       "3      0.059226    0.088794    0.061023    0.461083    0.303911    0.086153   \n",
       "4      0.205009    0.073908    0.051648    0.502171    0.333914    0.137047   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "471    0.079161    0.037285    0.094577    0.546019    0.385300    0.128186   \n",
       "472    0.041471    0.033304    0.041635    0.478794    0.347106    0.134853   \n",
       "475    0.044382    0.207561    0.024130    0.603469    0.507703    0.146198   \n",
       "477    0.067505    0.070588    0.045483    0.594803    0.452558    0.130743   \n",
       "478    0.048163    0.012417    0.022582    0.511126    0.379746    0.173264   \n",
       "\n",
       "     cg03556669  cg21808635  label  \n",
       "0      0.305896    0.780742    1.0  \n",
       "1      0.313702    0.696424    1.0  \n",
       "2      0.230778    0.606163    1.0  \n",
       "3      0.257855    0.680310    1.0  \n",
       "4      0.380755    0.722998    0.0  \n",
       "..          ...         ...    ...  \n",
       "471    0.425339    0.869079    0.0  \n",
       "472    0.290931    0.766231    1.0  \n",
       "475    0.502986    0.827691    1.0  \n",
       "477    0.225473    0.776557    0.0  \n",
       "478    0.396071    0.737711    1.0  \n",
       "\n",
       "[2536 rows x 286 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_selected_train_data4 = train_data4.loc[:,[i for i in selected_feat_rf4]]\n",
    "rf_selected_train_data4['label'] = train_data4['label'] \n",
    "rf_selected_train_data4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1902, 285), (634, 285), (1902,), (634,))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training 75% developing 25%\n",
    "x_train_rf4, x_dev_rf4, y_train_rf4, y_dev_rf4 = train_test_split(rf_selected_train_data4.drop(columns=['label']), rf_selected_train_data4['label'])\n",
    "x_train_rf4.shape, x_dev_rf4.shape, y_train_rf4.shape, y_dev_rf4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8177638984350876"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_selected4 = grid_result_rf4.best_estimator_\n",
    "rf_selected4.fit(x_train_rf4, y_train_rf4)\n",
    "rf_selected_auc4 = roc_auc_score(y_dev_rf4, rf_selected4.predict_proba(x_dev_rf4)[:, 1])\n",
    "rf_selected_auc4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_BSGS_rf_selected = data_BSGS['label']\n",
    "x_BSGS_rf_selected = data_BSGS.loc[:,[i for i in selected_feat_rf4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7798206278026906"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_selected_auc_BSGS = roc_auc_score(y_BSGS_rf_selected, rf_selected4.predict_proba(x_BSGS_rf_selected)[:, 1])\n",
    "rf_selected_auc_BSGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8097326025577146"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking_rf_BSGS = get_stacking()\n",
    "stacking_rf_BSGS.fit(x_train_rf4, y_train_rf4)\n",
    "stacking_rf_BSGS_auc = roc_auc_score(y_BSGS_rf_selected, stacking_rf_BSGS.predict_proba(x_BSGS_rf_selected)[:, 1])\n",
    "stacking_rf_BSGS_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8137186513868129"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_rf_BSGS = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gb', clf3), ('svc', clf4)], voting='soft')\n",
    "voting_rf_BSGS.fit(x_train_rf4, y_train_rf4)\n",
    "voting_rf_BSGS_auc = roc_auc_score(y_BSGS_rf_selected, voting_rf_BSGS.predict_proba(x_BSGS_rf_selected)[:, 1])\n",
    "voting_rf_BSGS_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Selection by LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1358, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/svm/_base.py\", line 975, in _fit_liblinear\n",
      "    raw_coef_, n_iter_ = liblinear.train_wrap(\n",
      "  File \"sklearn/svm/_liblinear.pyx\", line 52, in sklearn.svm._liblinear.train_wrap\n",
      "ValueError: b'C <= 0'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1358, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/svm/_base.py\", line 975, in _fit_liblinear\n",
      "    raw_coef_, n_iter_ = liblinear.train_wrap(\n",
      "  File \"sklearn/svm/_liblinear.pyx\", line 52, in sklearn.svm._liblinear.train_wrap\n",
      "ValueError: b'C <= 0'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1358, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/svm/_base.py\", line 975, in _fit_liblinear\n",
      "    raw_coef_, n_iter_ = liblinear.train_wrap(\n",
      "  File \"sklearn/svm/_liblinear.pyx\", line 52, in sklearn.svm._liblinear.train_wrap\n",
      "ValueError: b'C <= 0'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1358, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/svm/_base.py\", line 975, in _fit_liblinear\n",
      "    raw_coef_, n_iter_ = liblinear.train_wrap(\n",
      "  File \"sklearn/svm/_liblinear.pyx\", line 52, in sklearn.svm._liblinear.train_wrap\n",
      "ValueError: b'C <= 0'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1358, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/svm/_base.py\", line 975, in _fit_liblinear\n",
      "    raw_coef_, n_iter_ = liblinear.train_wrap(\n",
      "  File \"sklearn/svm/_liblinear.pyx\", line 52, in sklearn.svm._liblinear.train_wrap\n",
      "ValueError: b'C <= 0'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan 0.77581703        nan 0.70150925        nan 0.71047931\n",
      " 0.77622073 0.72144683 0.76858543        nan]\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=20, solver='sag', tol=1e-05)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid search for lr\n",
    "penalty = [\"l1\", \"l2\", \"elasticnet\"]\n",
    "# Tolerance for stopping criteria.\n",
    "tol = [0.00001, 0.001, 0.0000001]\n",
    "# Inverse of regularization strength\n",
    "C = [0,1, 0.5, 1, 10, 20, 30, 50, 100]\n",
    "# Algorithm to use in the optimization problem\n",
    "solver = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "\n",
    "\n",
    "param_distributions = dict(penalty = penalty, tol = tol, C = C, solver = solver)\n",
    "lr = LogisticRegression()\n",
    "grid_lr4 = RandomizedSearchCV(estimator = lr, param_distributions = param_distributions, scoring = \"roc_auc\",\n",
    "                          verbose = 1, n_jobs = -1) \n",
    "grid_result_lr4 = grid_lr4.fit(x_train4, y_train4) \n",
    "\n",
    "grid_result_lr4.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=LogisticRegression(C=20, solver='sag', tol=1e-05))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_selection4 = SelectFromModel(grid_result_lr4.best_estimator_)\n",
    "lr_selection4.fit(x_train4, y_train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "342"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_feat_lr4 = x_train4.columns[(lr_selection4.get_support())]\n",
    "len(selected_feat_lr4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cg01193368</th>\n",
       "      <th>cg06098368</th>\n",
       "      <th>cg08690094</th>\n",
       "      <th>cg03124146</th>\n",
       "      <th>cg14770527</th>\n",
       "      <th>cg23316599</th>\n",
       "      <th>cg11108474</th>\n",
       "      <th>cg16340103</th>\n",
       "      <th>cg04838249</th>\n",
       "      <th>cg26262573</th>\n",
       "      <th>...</th>\n",
       "      <th>cg27132152</th>\n",
       "      <th>cg09990584</th>\n",
       "      <th>cg07903023</th>\n",
       "      <th>cg15236528</th>\n",
       "      <th>cg21808635</th>\n",
       "      <th>cg07635017</th>\n",
       "      <th>cg08641118</th>\n",
       "      <th>cg22034735</th>\n",
       "      <th>cg17805624</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.606732</td>\n",
       "      <td>0.240143</td>\n",
       "      <td>0.561082</td>\n",
       "      <td>0.144218</td>\n",
       "      <td>0.597436</td>\n",
       "      <td>0.396608</td>\n",
       "      <td>0.479727</td>\n",
       "      <td>0.556758</td>\n",
       "      <td>0.709753</td>\n",
       "      <td>0.367165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363816</td>\n",
       "      <td>0.342347</td>\n",
       "      <td>0.641294</td>\n",
       "      <td>0.156715</td>\n",
       "      <td>0.780742</td>\n",
       "      <td>0.532118</td>\n",
       "      <td>0.081238</td>\n",
       "      <td>0.508394</td>\n",
       "      <td>0.279428</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.599726</td>\n",
       "      <td>0.242588</td>\n",
       "      <td>0.564277</td>\n",
       "      <td>0.139766</td>\n",
       "      <td>0.570267</td>\n",
       "      <td>0.419441</td>\n",
       "      <td>0.483118</td>\n",
       "      <td>0.481381</td>\n",
       "      <td>0.727749</td>\n",
       "      <td>0.364813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454734</td>\n",
       "      <td>0.321944</td>\n",
       "      <td>0.668892</td>\n",
       "      <td>0.220170</td>\n",
       "      <td>0.696424</td>\n",
       "      <td>0.546033</td>\n",
       "      <td>0.099777</td>\n",
       "      <td>0.498426</td>\n",
       "      <td>0.360946</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.552816</td>\n",
       "      <td>0.169127</td>\n",
       "      <td>0.541453</td>\n",
       "      <td>0.193932</td>\n",
       "      <td>0.510173</td>\n",
       "      <td>0.479463</td>\n",
       "      <td>0.368669</td>\n",
       "      <td>0.513161</td>\n",
       "      <td>0.668243</td>\n",
       "      <td>0.334221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.479652</td>\n",
       "      <td>0.327677</td>\n",
       "      <td>0.433934</td>\n",
       "      <td>0.178511</td>\n",
       "      <td>0.606163</td>\n",
       "      <td>0.536528</td>\n",
       "      <td>0.079384</td>\n",
       "      <td>0.204013</td>\n",
       "      <td>0.296357</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.655871</td>\n",
       "      <td>0.224729</td>\n",
       "      <td>0.480992</td>\n",
       "      <td>0.181609</td>\n",
       "      <td>0.481978</td>\n",
       "      <td>0.510254</td>\n",
       "      <td>0.360383</td>\n",
       "      <td>0.432874</td>\n",
       "      <td>0.745117</td>\n",
       "      <td>0.352629</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414647</td>\n",
       "      <td>0.303911</td>\n",
       "      <td>0.416025</td>\n",
       "      <td>0.086153</td>\n",
       "      <td>0.680310</td>\n",
       "      <td>0.557829</td>\n",
       "      <td>0.067174</td>\n",
       "      <td>0.297127</td>\n",
       "      <td>0.292479</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.493554</td>\n",
       "      <td>0.231550</td>\n",
       "      <td>0.474545</td>\n",
       "      <td>0.170328</td>\n",
       "      <td>0.549200</td>\n",
       "      <td>0.506738</td>\n",
       "      <td>0.401675</td>\n",
       "      <td>0.475677</td>\n",
       "      <td>0.783492</td>\n",
       "      <td>0.441518</td>\n",
       "      <td>...</td>\n",
       "      <td>0.443079</td>\n",
       "      <td>0.333914</td>\n",
       "      <td>0.593111</td>\n",
       "      <td>0.137047</td>\n",
       "      <td>0.722998</td>\n",
       "      <td>0.546425</td>\n",
       "      <td>0.075612</td>\n",
       "      <td>0.526409</td>\n",
       "      <td>0.278850</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>0.819136</td>\n",
       "      <td>0.274042</td>\n",
       "      <td>0.525204</td>\n",
       "      <td>0.387231</td>\n",
       "      <td>0.759353</td>\n",
       "      <td>0.573460</td>\n",
       "      <td>0.401122</td>\n",
       "      <td>0.677112</td>\n",
       "      <td>0.805374</td>\n",
       "      <td>0.666337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557590</td>\n",
       "      <td>0.385300</td>\n",
       "      <td>0.699565</td>\n",
       "      <td>0.128186</td>\n",
       "      <td>0.869079</td>\n",
       "      <td>0.647941</td>\n",
       "      <td>0.176504</td>\n",
       "      <td>0.529537</td>\n",
       "      <td>0.275265</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>0.650936</td>\n",
       "      <td>0.127649</td>\n",
       "      <td>0.458996</td>\n",
       "      <td>0.116075</td>\n",
       "      <td>0.700554</td>\n",
       "      <td>0.475778</td>\n",
       "      <td>0.298102</td>\n",
       "      <td>0.585688</td>\n",
       "      <td>0.670181</td>\n",
       "      <td>0.339098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.488550</td>\n",
       "      <td>0.347106</td>\n",
       "      <td>0.598367</td>\n",
       "      <td>0.134853</td>\n",
       "      <td>0.766231</td>\n",
       "      <td>0.603185</td>\n",
       "      <td>0.143384</td>\n",
       "      <td>0.522733</td>\n",
       "      <td>0.230949</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0.668992</td>\n",
       "      <td>0.141059</td>\n",
       "      <td>0.522190</td>\n",
       "      <td>0.141652</td>\n",
       "      <td>0.686412</td>\n",
       "      <td>0.454601</td>\n",
       "      <td>0.322749</td>\n",
       "      <td>0.536903</td>\n",
       "      <td>0.624104</td>\n",
       "      <td>0.516098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.515793</td>\n",
       "      <td>0.507703</td>\n",
       "      <td>0.744434</td>\n",
       "      <td>0.146198</td>\n",
       "      <td>0.827691</td>\n",
       "      <td>0.631754</td>\n",
       "      <td>0.077538</td>\n",
       "      <td>0.617378</td>\n",
       "      <td>0.242766</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0.642261</td>\n",
       "      <td>0.303791</td>\n",
       "      <td>0.403003</td>\n",
       "      <td>0.278751</td>\n",
       "      <td>0.764943</td>\n",
       "      <td>0.376560</td>\n",
       "      <td>0.445176</td>\n",
       "      <td>0.485595</td>\n",
       "      <td>0.663237</td>\n",
       "      <td>0.521479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.512241</td>\n",
       "      <td>0.452558</td>\n",
       "      <td>0.637866</td>\n",
       "      <td>0.130743</td>\n",
       "      <td>0.776557</td>\n",
       "      <td>0.595563</td>\n",
       "      <td>0.108415</td>\n",
       "      <td>0.444886</td>\n",
       "      <td>0.302313</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0.780978</td>\n",
       "      <td>0.301901</td>\n",
       "      <td>0.552029</td>\n",
       "      <td>0.374958</td>\n",
       "      <td>0.796539</td>\n",
       "      <td>0.529338</td>\n",
       "      <td>0.463919</td>\n",
       "      <td>0.555628</td>\n",
       "      <td>0.780577</td>\n",
       "      <td>0.489616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.460155</td>\n",
       "      <td>0.379746</td>\n",
       "      <td>0.654000</td>\n",
       "      <td>0.173264</td>\n",
       "      <td>0.737711</td>\n",
       "      <td>0.587225</td>\n",
       "      <td>0.164567</td>\n",
       "      <td>0.740924</td>\n",
       "      <td>0.223064</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2536 rows × 343 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cg01193368  cg06098368  cg08690094  cg03124146  cg14770527  cg23316599  \\\n",
       "0      0.606732    0.240143    0.561082    0.144218    0.597436    0.396608   \n",
       "1      0.599726    0.242588    0.564277    0.139766    0.570267    0.419441   \n",
       "2      0.552816    0.169127    0.541453    0.193932    0.510173    0.479463   \n",
       "3      0.655871    0.224729    0.480992    0.181609    0.481978    0.510254   \n",
       "4      0.493554    0.231550    0.474545    0.170328    0.549200    0.506738   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "471    0.819136    0.274042    0.525204    0.387231    0.759353    0.573460   \n",
       "472    0.650936    0.127649    0.458996    0.116075    0.700554    0.475778   \n",
       "475    0.668992    0.141059    0.522190    0.141652    0.686412    0.454601   \n",
       "477    0.642261    0.303791    0.403003    0.278751    0.764943    0.376560   \n",
       "478    0.780978    0.301901    0.552029    0.374958    0.796539    0.529338   \n",
       "\n",
       "     cg11108474  cg16340103  cg04838249  cg26262573  ...  cg27132152  \\\n",
       "0      0.479727    0.556758    0.709753    0.367165  ...    0.363816   \n",
       "1      0.483118    0.481381    0.727749    0.364813  ...    0.454734   \n",
       "2      0.368669    0.513161    0.668243    0.334221  ...    0.479652   \n",
       "3      0.360383    0.432874    0.745117    0.352629  ...    0.414647   \n",
       "4      0.401675    0.475677    0.783492    0.441518  ...    0.443079   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "471    0.401122    0.677112    0.805374    0.666337  ...    0.557590   \n",
       "472    0.298102    0.585688    0.670181    0.339098  ...    0.488550   \n",
       "475    0.322749    0.536903    0.624104    0.516098  ...    0.515793   \n",
       "477    0.445176    0.485595    0.663237    0.521479  ...    0.512241   \n",
       "478    0.463919    0.555628    0.780577    0.489616  ...    0.460155   \n",
       "\n",
       "     cg09990584  cg07903023  cg15236528  cg21808635  cg07635017  cg08641118  \\\n",
       "0      0.342347    0.641294    0.156715    0.780742    0.532118    0.081238   \n",
       "1      0.321944    0.668892    0.220170    0.696424    0.546033    0.099777   \n",
       "2      0.327677    0.433934    0.178511    0.606163    0.536528    0.079384   \n",
       "3      0.303911    0.416025    0.086153    0.680310    0.557829    0.067174   \n",
       "4      0.333914    0.593111    0.137047    0.722998    0.546425    0.075612   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "471    0.385300    0.699565    0.128186    0.869079    0.647941    0.176504   \n",
       "472    0.347106    0.598367    0.134853    0.766231    0.603185    0.143384   \n",
       "475    0.507703    0.744434    0.146198    0.827691    0.631754    0.077538   \n",
       "477    0.452558    0.637866    0.130743    0.776557    0.595563    0.108415   \n",
       "478    0.379746    0.654000    0.173264    0.737711    0.587225    0.164567   \n",
       "\n",
       "     cg22034735  cg17805624  label  \n",
       "0      0.508394    0.279428    1.0  \n",
       "1      0.498426    0.360946    1.0  \n",
       "2      0.204013    0.296357    1.0  \n",
       "3      0.297127    0.292479    1.0  \n",
       "4      0.526409    0.278850    0.0  \n",
       "..          ...         ...    ...  \n",
       "471    0.529537    0.275265    0.0  \n",
       "472    0.522733    0.230949    1.0  \n",
       "475    0.617378    0.242766    1.0  \n",
       "477    0.444886    0.302313    0.0  \n",
       "478    0.740924    0.223064    1.0  \n",
       "\n",
       "[2536 rows x 343 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_selected_train_data4 = train_data4.loc[:,[i for i in selected_feat_lr4]]\n",
    "lr_selected_train_data4['label'] = train_data4['label'] \n",
    "lr_selected_train_data4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1902, 342), (634, 342), (1902,), (634,))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training 75% developing 25%\n",
    "x_train_lr4, x_dev_lr4, y_train_lr4, y_dev_lr4 = train_test_split(lr_selected_train_data4.drop(columns=['label']), lr_selected_train_data4['label'])\n",
    "x_train_lr4.shape, x_dev_lr4.shape, y_train_lr4.shape, y_dev_lr4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8228367689357622"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_selected4 = grid_result_lr4.best_estimator_\n",
    "lr_selected4.fit(x_train_lr4, y_train_lr4)\n",
    "lr_selected_auc4 = roc_auc_score(y_dev_lr4, lr_selected4.predict_proba(x_dev_lr4)[:, 1])\n",
    "lr_selected_auc4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_BSGS_lr_selected = data_BSGS['label']\n",
    "x_BSGS_lr_selected = data_BSGS.loc[:,[i for i in selected_feat_lr4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7776449094834745"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_selected_auc_BSGS = roc_auc_score(y_BSGS_lr_selected, lr_selected4.predict_proba(x_BSGS_lr_selected)[:, 1])\n",
    "lr_selected_auc_BSGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8109948513535958"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking_lr_BSGS = get_stacking()\n",
    "stacking_lr_BSGS.fit(x_train_lr4, y_train_lr4)\n",
    "stacking_lr_BSGS_auc = roc_auc_score(y_BSGS_lr_selected, stacking_lr_BSGS.predict_proba(x_BSGS_lr_selected)[:, 1])\n",
    "stacking_lr_BSGS_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8100315562198971"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_lr_BSGS = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gb', clf3), ('svc', clf4)], voting='soft')\n",
    "voting_lr_BSGS.fit(x_train_lr4, y_train_lr4)\n",
    "voting_lr_BSGS_auc = roc_auc_score(y_BSGS_lr_selected, voting_lr_BSGS.predict_proba(x_BSGS_lr_selected)[:, 1])\n",
    "voting_lr_BSGS_auc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training: BSGS, AMDTSS, E-MTAB, Denmark\n",
    "# Testing: E-Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cg22695986</th>\n",
       "      <th>cg01193368</th>\n",
       "      <th>cg22056094</th>\n",
       "      <th>cg06098368</th>\n",
       "      <th>cg08690094</th>\n",
       "      <th>cg11236452</th>\n",
       "      <th>cg26916862</th>\n",
       "      <th>cg03124146</th>\n",
       "      <th>cg14770527</th>\n",
       "      <th>...</th>\n",
       "      <th>cg11174855</th>\n",
       "      <th>cg04524933</th>\n",
       "      <th>cg19418458</th>\n",
       "      <th>cg21808635</th>\n",
       "      <th>cg11359720</th>\n",
       "      <th>cg07635017</th>\n",
       "      <th>cg08641118</th>\n",
       "      <th>cg09166085</th>\n",
       "      <th>cg22034735</th>\n",
       "      <th>cg17805624</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.282164</td>\n",
       "      <td>0.613574</td>\n",
       "      <td>0.670013</td>\n",
       "      <td>0.269499</td>\n",
       "      <td>0.516672</td>\n",
       "      <td>0.459066</td>\n",
       "      <td>0.254743</td>\n",
       "      <td>0.224724</td>\n",
       "      <td>0.617466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071795</td>\n",
       "      <td>0.919869</td>\n",
       "      <td>0.448765</td>\n",
       "      <td>0.810244</td>\n",
       "      <td>0.686240</td>\n",
       "      <td>0.548766</td>\n",
       "      <td>0.113030</td>\n",
       "      <td>0.141956</td>\n",
       "      <td>0.334091</td>\n",
       "      <td>0.243362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.225027</td>\n",
       "      <td>0.619136</td>\n",
       "      <td>0.530781</td>\n",
       "      <td>0.219685</td>\n",
       "      <td>0.441304</td>\n",
       "      <td>0.504256</td>\n",
       "      <td>0.231294</td>\n",
       "      <td>0.169521</td>\n",
       "      <td>0.594098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080001</td>\n",
       "      <td>0.920140</td>\n",
       "      <td>0.458319</td>\n",
       "      <td>0.790811</td>\n",
       "      <td>0.622287</td>\n",
       "      <td>0.548240</td>\n",
       "      <td>0.182175</td>\n",
       "      <td>0.357434</td>\n",
       "      <td>0.660001</td>\n",
       "      <td>0.286040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.289924</td>\n",
       "      <td>0.691534</td>\n",
       "      <td>0.771177</td>\n",
       "      <td>0.314880</td>\n",
       "      <td>0.489395</td>\n",
       "      <td>0.531181</td>\n",
       "      <td>0.283582</td>\n",
       "      <td>0.206316</td>\n",
       "      <td>0.605732</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073612</td>\n",
       "      <td>0.950902</td>\n",
       "      <td>0.275012</td>\n",
       "      <td>0.837924</td>\n",
       "      <td>0.744836</td>\n",
       "      <td>0.619499</td>\n",
       "      <td>0.121501</td>\n",
       "      <td>0.141186</td>\n",
       "      <td>0.444107</td>\n",
       "      <td>0.277873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.333657</td>\n",
       "      <td>0.675944</td>\n",
       "      <td>0.679786</td>\n",
       "      <td>0.276872</td>\n",
       "      <td>0.506406</td>\n",
       "      <td>0.511713</td>\n",
       "      <td>0.353252</td>\n",
       "      <td>0.310907</td>\n",
       "      <td>0.625395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081113</td>\n",
       "      <td>0.946354</td>\n",
       "      <td>0.463502</td>\n",
       "      <td>0.834654</td>\n",
       "      <td>0.788736</td>\n",
       "      <td>0.613200</td>\n",
       "      <td>0.215362</td>\n",
       "      <td>0.117529</td>\n",
       "      <td>0.576769</td>\n",
       "      <td>0.308074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0.206107</td>\n",
       "      <td>0.472510</td>\n",
       "      <td>0.588723</td>\n",
       "      <td>0.137395</td>\n",
       "      <td>0.380327</td>\n",
       "      <td>0.392145</td>\n",
       "      <td>0.165503</td>\n",
       "      <td>0.170393</td>\n",
       "      <td>0.567658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117231</td>\n",
       "      <td>0.918648</td>\n",
       "      <td>0.225151</td>\n",
       "      <td>0.808565</td>\n",
       "      <td>0.739517</td>\n",
       "      <td>0.601517</td>\n",
       "      <td>0.150539</td>\n",
       "      <td>0.321008</td>\n",
       "      <td>0.673163</td>\n",
       "      <td>0.351189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>1</td>\n",
       "      <td>0.265920</td>\n",
       "      <td>0.668810</td>\n",
       "      <td>0.538530</td>\n",
       "      <td>0.221750</td>\n",
       "      <td>0.450940</td>\n",
       "      <td>0.409590</td>\n",
       "      <td>0.261340</td>\n",
       "      <td>0.202900</td>\n",
       "      <td>0.703690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074460</td>\n",
       "      <td>0.932110</td>\n",
       "      <td>0.309700</td>\n",
       "      <td>0.811640</td>\n",
       "      <td>0.651310</td>\n",
       "      <td>0.426310</td>\n",
       "      <td>0.106370</td>\n",
       "      <td>0.128560</td>\n",
       "      <td>0.381660</td>\n",
       "      <td>0.148490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>1</td>\n",
       "      <td>0.264220</td>\n",
       "      <td>0.673100</td>\n",
       "      <td>0.552430</td>\n",
       "      <td>0.211480</td>\n",
       "      <td>0.484940</td>\n",
       "      <td>0.468880</td>\n",
       "      <td>0.259820</td>\n",
       "      <td>0.240040</td>\n",
       "      <td>0.703450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033430</td>\n",
       "      <td>0.918230</td>\n",
       "      <td>0.273310</td>\n",
       "      <td>0.824450</td>\n",
       "      <td>0.609600</td>\n",
       "      <td>0.465400</td>\n",
       "      <td>0.121100</td>\n",
       "      <td>0.089490</td>\n",
       "      <td>0.315100</td>\n",
       "      <td>0.114360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>0</td>\n",
       "      <td>0.300860</td>\n",
       "      <td>0.586160</td>\n",
       "      <td>0.691140</td>\n",
       "      <td>0.240640</td>\n",
       "      <td>0.481040</td>\n",
       "      <td>0.579180</td>\n",
       "      <td>0.275920</td>\n",
       "      <td>0.234960</td>\n",
       "      <td>0.633500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117130</td>\n",
       "      <td>0.936460</td>\n",
       "      <td>0.436270</td>\n",
       "      <td>0.820790</td>\n",
       "      <td>0.597970</td>\n",
       "      <td>0.438150</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>0.214790</td>\n",
       "      <td>0.261410</td>\n",
       "      <td>0.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>0</td>\n",
       "      <td>0.289540</td>\n",
       "      <td>0.679960</td>\n",
       "      <td>0.553150</td>\n",
       "      <td>0.286020</td>\n",
       "      <td>0.315550</td>\n",
       "      <td>0.421130</td>\n",
       "      <td>0.231630</td>\n",
       "      <td>0.297830</td>\n",
       "      <td>0.680370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043240</td>\n",
       "      <td>0.931800</td>\n",
       "      <td>0.296000</td>\n",
       "      <td>0.798780</td>\n",
       "      <td>0.609940</td>\n",
       "      <td>0.394220</td>\n",
       "      <td>0.143910</td>\n",
       "      <td>0.143810</td>\n",
       "      <td>0.566440</td>\n",
       "      <td>0.156560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>0</td>\n",
       "      <td>0.255850</td>\n",
       "      <td>0.591080</td>\n",
       "      <td>0.525270</td>\n",
       "      <td>0.268260</td>\n",
       "      <td>0.343080</td>\n",
       "      <td>0.478540</td>\n",
       "      <td>0.231610</td>\n",
       "      <td>0.272100</td>\n",
       "      <td>0.707310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094330</td>\n",
       "      <td>0.924410</td>\n",
       "      <td>0.299930</td>\n",
       "      <td>0.865710</td>\n",
       "      <td>0.703540</td>\n",
       "      <td>0.399760</td>\n",
       "      <td>0.157860</td>\n",
       "      <td>0.165840</td>\n",
       "      <td>0.629880</td>\n",
       "      <td>0.119080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1166 rows × 834 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  cg22695986  cg01193368  cg22056094  cg06098368  cg08690094  \\\n",
       "0        0    0.282164    0.613574    0.670013    0.269499    0.516672   \n",
       "2        0    0.225027    0.619136    0.530781    0.219685    0.441304   \n",
       "4        1    0.289924    0.691534    0.771177    0.314880    0.489395   \n",
       "7        0    0.333657    0.675944    0.679786    0.276872    0.506406   \n",
       "10       0    0.206107    0.472510    0.588723    0.137395    0.380327   \n",
       "..     ...         ...         ...         ...         ...         ...   \n",
       "643      1    0.265920    0.668810    0.538530    0.221750    0.450940   \n",
       "644      1    0.264220    0.673100    0.552430    0.211480    0.484940   \n",
       "645      0    0.300860    0.586160    0.691140    0.240640    0.481040   \n",
       "646      0    0.289540    0.679960    0.553150    0.286020    0.315550   \n",
       "647      0    0.255850    0.591080    0.525270    0.268260    0.343080   \n",
       "\n",
       "     cg11236452  cg26916862  cg03124146  cg14770527  ...  cg11174855  \\\n",
       "0      0.459066    0.254743    0.224724    0.617466  ...    0.071795   \n",
       "2      0.504256    0.231294    0.169521    0.594098  ...    0.080001   \n",
       "4      0.531181    0.283582    0.206316    0.605732  ...    0.073612   \n",
       "7      0.511713    0.353252    0.310907    0.625395  ...    0.081113   \n",
       "10     0.392145    0.165503    0.170393    0.567658  ...    0.117231   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "643    0.409590    0.261340    0.202900    0.703690  ...    0.074460   \n",
       "644    0.468880    0.259820    0.240040    0.703450  ...    0.033430   \n",
       "645    0.579180    0.275920    0.234960    0.633500  ...    0.117130   \n",
       "646    0.421130    0.231630    0.297830    0.680370  ...    0.043240   \n",
       "647    0.478540    0.231610    0.272100    0.707310  ...    0.094330   \n",
       "\n",
       "     cg04524933  cg19418458  cg21808635  cg11359720  cg07635017  cg08641118  \\\n",
       "0      0.919869    0.448765    0.810244    0.686240    0.548766    0.113030   \n",
       "2      0.920140    0.458319    0.790811    0.622287    0.548240    0.182175   \n",
       "4      0.950902    0.275012    0.837924    0.744836    0.619499    0.121501   \n",
       "7      0.946354    0.463502    0.834654    0.788736    0.613200    0.215362   \n",
       "10     0.918648    0.225151    0.808565    0.739517    0.601517    0.150539   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "643    0.932110    0.309700    0.811640    0.651310    0.426310    0.106370   \n",
       "644    0.918230    0.273310    0.824450    0.609600    0.465400    0.121100   \n",
       "645    0.936460    0.436270    0.820790    0.597970    0.438150    0.134000   \n",
       "646    0.931800    0.296000    0.798780    0.609940    0.394220    0.143910   \n",
       "647    0.924410    0.299930    0.865710    0.703540    0.399760    0.157860   \n",
       "\n",
       "     cg09166085  cg22034735  cg17805624  \n",
       "0      0.141956    0.334091    0.243362  \n",
       "2      0.357434    0.660001    0.286040  \n",
       "4      0.141186    0.444107    0.277873  \n",
       "7      0.117529    0.576769    0.308074  \n",
       "10     0.321008    0.673163    0.351189  \n",
       "..          ...         ...         ...  \n",
       "643    0.128560    0.381660    0.148490  \n",
       "644    0.089490    0.315100    0.114360  \n",
       "645    0.214790    0.261410    0.127200  \n",
       "646    0.143810    0.566440    0.156560  \n",
       "647    0.165840    0.629880    0.119080  \n",
       "\n",
       "[1166 rows x 834 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1186 * 834\n",
    "train_data5 = pd.concat([data_BSGS, data_DENMARK, data_EMTAB])\n",
    "train_data5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cg22695986</th>\n",
       "      <th>cg01193368</th>\n",
       "      <th>cg22056094</th>\n",
       "      <th>cg06098368</th>\n",
       "      <th>cg08690094</th>\n",
       "      <th>cg11236452</th>\n",
       "      <th>cg26916862</th>\n",
       "      <th>cg03124146</th>\n",
       "      <th>cg14770527</th>\n",
       "      <th>...</th>\n",
       "      <th>cg11174855</th>\n",
       "      <th>cg04524933</th>\n",
       "      <th>cg19418458</th>\n",
       "      <th>cg21808635</th>\n",
       "      <th>cg11359720</th>\n",
       "      <th>cg07635017</th>\n",
       "      <th>cg08641118</th>\n",
       "      <th>cg09166085</th>\n",
       "      <th>cg22034735</th>\n",
       "      <th>cg17805624</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.282164</td>\n",
       "      <td>0.613574</td>\n",
       "      <td>0.670013</td>\n",
       "      <td>0.269499</td>\n",
       "      <td>0.516672</td>\n",
       "      <td>0.459066</td>\n",
       "      <td>0.254743</td>\n",
       "      <td>0.224724</td>\n",
       "      <td>0.617466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071795</td>\n",
       "      <td>0.919869</td>\n",
       "      <td>0.448765</td>\n",
       "      <td>0.810244</td>\n",
       "      <td>0.686240</td>\n",
       "      <td>0.548766</td>\n",
       "      <td>0.113030</td>\n",
       "      <td>0.141956</td>\n",
       "      <td>0.334091</td>\n",
       "      <td>0.243362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.225027</td>\n",
       "      <td>0.619136</td>\n",
       "      <td>0.530781</td>\n",
       "      <td>0.219685</td>\n",
       "      <td>0.441304</td>\n",
       "      <td>0.504256</td>\n",
       "      <td>0.231294</td>\n",
       "      <td>0.169521</td>\n",
       "      <td>0.594098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080001</td>\n",
       "      <td>0.920140</td>\n",
       "      <td>0.458319</td>\n",
       "      <td>0.790811</td>\n",
       "      <td>0.622287</td>\n",
       "      <td>0.548240</td>\n",
       "      <td>0.182175</td>\n",
       "      <td>0.357434</td>\n",
       "      <td>0.660001</td>\n",
       "      <td>0.286040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.289924</td>\n",
       "      <td>0.691534</td>\n",
       "      <td>0.771177</td>\n",
       "      <td>0.314880</td>\n",
       "      <td>0.489395</td>\n",
       "      <td>0.531181</td>\n",
       "      <td>0.283582</td>\n",
       "      <td>0.206316</td>\n",
       "      <td>0.605732</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073612</td>\n",
       "      <td>0.950902</td>\n",
       "      <td>0.275012</td>\n",
       "      <td>0.837924</td>\n",
       "      <td>0.744836</td>\n",
       "      <td>0.619499</td>\n",
       "      <td>0.121501</td>\n",
       "      <td>0.141186</td>\n",
       "      <td>0.444107</td>\n",
       "      <td>0.277873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.333657</td>\n",
       "      <td>0.675944</td>\n",
       "      <td>0.679786</td>\n",
       "      <td>0.276872</td>\n",
       "      <td>0.506406</td>\n",
       "      <td>0.511713</td>\n",
       "      <td>0.353252</td>\n",
       "      <td>0.310907</td>\n",
       "      <td>0.625395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081113</td>\n",
       "      <td>0.946354</td>\n",
       "      <td>0.463502</td>\n",
       "      <td>0.834654</td>\n",
       "      <td>0.788736</td>\n",
       "      <td>0.613200</td>\n",
       "      <td>0.215362</td>\n",
       "      <td>0.117529</td>\n",
       "      <td>0.576769</td>\n",
       "      <td>0.308074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0.206107</td>\n",
       "      <td>0.472510</td>\n",
       "      <td>0.588723</td>\n",
       "      <td>0.137395</td>\n",
       "      <td>0.380327</td>\n",
       "      <td>0.392145</td>\n",
       "      <td>0.165503</td>\n",
       "      <td>0.170393</td>\n",
       "      <td>0.567658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117231</td>\n",
       "      <td>0.918648</td>\n",
       "      <td>0.225151</td>\n",
       "      <td>0.808565</td>\n",
       "      <td>0.739517</td>\n",
       "      <td>0.601517</td>\n",
       "      <td>0.150539</td>\n",
       "      <td>0.321008</td>\n",
       "      <td>0.673163</td>\n",
       "      <td>0.351189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>1</td>\n",
       "      <td>0.265920</td>\n",
       "      <td>0.668810</td>\n",
       "      <td>0.538530</td>\n",
       "      <td>0.221750</td>\n",
       "      <td>0.450940</td>\n",
       "      <td>0.409590</td>\n",
       "      <td>0.261340</td>\n",
       "      <td>0.202900</td>\n",
       "      <td>0.703690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074460</td>\n",
       "      <td>0.932110</td>\n",
       "      <td>0.309700</td>\n",
       "      <td>0.811640</td>\n",
       "      <td>0.651310</td>\n",
       "      <td>0.426310</td>\n",
       "      <td>0.106370</td>\n",
       "      <td>0.128560</td>\n",
       "      <td>0.381660</td>\n",
       "      <td>0.148490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>1</td>\n",
       "      <td>0.264220</td>\n",
       "      <td>0.673100</td>\n",
       "      <td>0.552430</td>\n",
       "      <td>0.211480</td>\n",
       "      <td>0.484940</td>\n",
       "      <td>0.468880</td>\n",
       "      <td>0.259820</td>\n",
       "      <td>0.240040</td>\n",
       "      <td>0.703450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033430</td>\n",
       "      <td>0.918230</td>\n",
       "      <td>0.273310</td>\n",
       "      <td>0.824450</td>\n",
       "      <td>0.609600</td>\n",
       "      <td>0.465400</td>\n",
       "      <td>0.121100</td>\n",
       "      <td>0.089490</td>\n",
       "      <td>0.315100</td>\n",
       "      <td>0.114360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>0</td>\n",
       "      <td>0.300860</td>\n",
       "      <td>0.586160</td>\n",
       "      <td>0.691140</td>\n",
       "      <td>0.240640</td>\n",
       "      <td>0.481040</td>\n",
       "      <td>0.579180</td>\n",
       "      <td>0.275920</td>\n",
       "      <td>0.234960</td>\n",
       "      <td>0.633500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117130</td>\n",
       "      <td>0.936460</td>\n",
       "      <td>0.436270</td>\n",
       "      <td>0.820790</td>\n",
       "      <td>0.597970</td>\n",
       "      <td>0.438150</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>0.214790</td>\n",
       "      <td>0.261410</td>\n",
       "      <td>0.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>0</td>\n",
       "      <td>0.289540</td>\n",
       "      <td>0.679960</td>\n",
       "      <td>0.553150</td>\n",
       "      <td>0.286020</td>\n",
       "      <td>0.315550</td>\n",
       "      <td>0.421130</td>\n",
       "      <td>0.231630</td>\n",
       "      <td>0.297830</td>\n",
       "      <td>0.680370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043240</td>\n",
       "      <td>0.931800</td>\n",
       "      <td>0.296000</td>\n",
       "      <td>0.798780</td>\n",
       "      <td>0.609940</td>\n",
       "      <td>0.394220</td>\n",
       "      <td>0.143910</td>\n",
       "      <td>0.143810</td>\n",
       "      <td>0.566440</td>\n",
       "      <td>0.156560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>0</td>\n",
       "      <td>0.255850</td>\n",
       "      <td>0.591080</td>\n",
       "      <td>0.525270</td>\n",
       "      <td>0.268260</td>\n",
       "      <td>0.343080</td>\n",
       "      <td>0.478540</td>\n",
       "      <td>0.231610</td>\n",
       "      <td>0.272100</td>\n",
       "      <td>0.707310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094330</td>\n",
       "      <td>0.924410</td>\n",
       "      <td>0.299930</td>\n",
       "      <td>0.865710</td>\n",
       "      <td>0.703540</td>\n",
       "      <td>0.399760</td>\n",
       "      <td>0.157860</td>\n",
       "      <td>0.165840</td>\n",
       "      <td>0.629880</td>\n",
       "      <td>0.119080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1166 rows × 832 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  cg22695986  cg01193368  cg22056094  cg06098368  cg08690094  \\\n",
       "0        0    0.282164    0.613574    0.670013    0.269499    0.516672   \n",
       "2        0    0.225027    0.619136    0.530781    0.219685    0.441304   \n",
       "4        1    0.289924    0.691534    0.771177    0.314880    0.489395   \n",
       "7        0    0.333657    0.675944    0.679786    0.276872    0.506406   \n",
       "10       0    0.206107    0.472510    0.588723    0.137395    0.380327   \n",
       "..     ...         ...         ...         ...         ...         ...   \n",
       "643      1    0.265920    0.668810    0.538530    0.221750    0.450940   \n",
       "644      1    0.264220    0.673100    0.552430    0.211480    0.484940   \n",
       "645      0    0.300860    0.586160    0.691140    0.240640    0.481040   \n",
       "646      0    0.289540    0.679960    0.553150    0.286020    0.315550   \n",
       "647      0    0.255850    0.591080    0.525270    0.268260    0.343080   \n",
       "\n",
       "     cg11236452  cg26916862  cg03124146  cg14770527  ...  cg11174855  \\\n",
       "0      0.459066    0.254743    0.224724    0.617466  ...    0.071795   \n",
       "2      0.504256    0.231294    0.169521    0.594098  ...    0.080001   \n",
       "4      0.531181    0.283582    0.206316    0.605732  ...    0.073612   \n",
       "7      0.511713    0.353252    0.310907    0.625395  ...    0.081113   \n",
       "10     0.392145    0.165503    0.170393    0.567658  ...    0.117231   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "643    0.409590    0.261340    0.202900    0.703690  ...    0.074460   \n",
       "644    0.468880    0.259820    0.240040    0.703450  ...    0.033430   \n",
       "645    0.579180    0.275920    0.234960    0.633500  ...    0.117130   \n",
       "646    0.421130    0.231630    0.297830    0.680370  ...    0.043240   \n",
       "647    0.478540    0.231610    0.272100    0.707310  ...    0.094330   \n",
       "\n",
       "     cg04524933  cg19418458  cg21808635  cg11359720  cg07635017  cg08641118  \\\n",
       "0      0.919869    0.448765    0.810244    0.686240    0.548766    0.113030   \n",
       "2      0.920140    0.458319    0.790811    0.622287    0.548240    0.182175   \n",
       "4      0.950902    0.275012    0.837924    0.744836    0.619499    0.121501   \n",
       "7      0.946354    0.463502    0.834654    0.788736    0.613200    0.215362   \n",
       "10     0.918648    0.225151    0.808565    0.739517    0.601517    0.150539   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "643    0.932110    0.309700    0.811640    0.651310    0.426310    0.106370   \n",
       "644    0.918230    0.273310    0.824450    0.609600    0.465400    0.121100   \n",
       "645    0.936460    0.436270    0.820790    0.597970    0.438150    0.134000   \n",
       "646    0.931800    0.296000    0.798780    0.609940    0.394220    0.143910   \n",
       "647    0.924410    0.299930    0.865710    0.703540    0.399760    0.157860   \n",
       "\n",
       "     cg09166085  cg22034735  cg17805624  \n",
       "0      0.141956    0.334091    0.243362  \n",
       "2      0.357434    0.660001    0.286040  \n",
       "4      0.141186    0.444107    0.277873  \n",
       "7      0.117529    0.576769    0.308074  \n",
       "10     0.321008    0.673163    0.351189  \n",
       "..          ...         ...         ...  \n",
       "643    0.128560    0.381660    0.148490  \n",
       "644    0.089490    0.315100    0.114360  \n",
       "645    0.214790    0.261410    0.127200  \n",
       "646    0.143810    0.566440    0.156560  \n",
       "647    0.165840    0.629880    0.119080  \n",
       "\n",
       "[1166 rows x 832 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1186 * 832\n",
    "train_data5 = train_data5.loc[:,[i for i in data_AMDTSS.columns]]\n",
    "train_data5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cg22695986</th>\n",
       "      <th>cg01193368</th>\n",
       "      <th>cg22056094</th>\n",
       "      <th>cg06098368</th>\n",
       "      <th>cg08690094</th>\n",
       "      <th>cg11236452</th>\n",
       "      <th>cg26916862</th>\n",
       "      <th>cg03124146</th>\n",
       "      <th>cg14770527</th>\n",
       "      <th>...</th>\n",
       "      <th>cg11174855</th>\n",
       "      <th>cg04524933</th>\n",
       "      <th>cg19418458</th>\n",
       "      <th>cg21808635</th>\n",
       "      <th>cg11359720</th>\n",
       "      <th>cg07635017</th>\n",
       "      <th>cg08641118</th>\n",
       "      <th>cg09166085</th>\n",
       "      <th>cg22034735</th>\n",
       "      <th>cg17805624</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.282164</td>\n",
       "      <td>0.613574</td>\n",
       "      <td>0.670013</td>\n",
       "      <td>0.269499</td>\n",
       "      <td>0.516672</td>\n",
       "      <td>0.459066</td>\n",
       "      <td>0.254743</td>\n",
       "      <td>0.224724</td>\n",
       "      <td>0.617466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071795</td>\n",
       "      <td>0.919869</td>\n",
       "      <td>0.448765</td>\n",
       "      <td>0.810244</td>\n",
       "      <td>0.686240</td>\n",
       "      <td>0.548766</td>\n",
       "      <td>0.113030</td>\n",
       "      <td>0.141956</td>\n",
       "      <td>0.334091</td>\n",
       "      <td>0.243362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.225027</td>\n",
       "      <td>0.619136</td>\n",
       "      <td>0.530781</td>\n",
       "      <td>0.219685</td>\n",
       "      <td>0.441304</td>\n",
       "      <td>0.504256</td>\n",
       "      <td>0.231294</td>\n",
       "      <td>0.169521</td>\n",
       "      <td>0.594098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080001</td>\n",
       "      <td>0.920140</td>\n",
       "      <td>0.458319</td>\n",
       "      <td>0.790811</td>\n",
       "      <td>0.622287</td>\n",
       "      <td>0.548240</td>\n",
       "      <td>0.182175</td>\n",
       "      <td>0.357434</td>\n",
       "      <td>0.660001</td>\n",
       "      <td>0.286040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.289924</td>\n",
       "      <td>0.691534</td>\n",
       "      <td>0.771177</td>\n",
       "      <td>0.314880</td>\n",
       "      <td>0.489395</td>\n",
       "      <td>0.531181</td>\n",
       "      <td>0.283582</td>\n",
       "      <td>0.206316</td>\n",
       "      <td>0.605732</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073612</td>\n",
       "      <td>0.950902</td>\n",
       "      <td>0.275012</td>\n",
       "      <td>0.837924</td>\n",
       "      <td>0.744836</td>\n",
       "      <td>0.619499</td>\n",
       "      <td>0.121501</td>\n",
       "      <td>0.141186</td>\n",
       "      <td>0.444107</td>\n",
       "      <td>0.277873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.333657</td>\n",
       "      <td>0.675944</td>\n",
       "      <td>0.679786</td>\n",
       "      <td>0.276872</td>\n",
       "      <td>0.506406</td>\n",
       "      <td>0.511713</td>\n",
       "      <td>0.353252</td>\n",
       "      <td>0.310907</td>\n",
       "      <td>0.625395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081113</td>\n",
       "      <td>0.946354</td>\n",
       "      <td>0.463502</td>\n",
       "      <td>0.834654</td>\n",
       "      <td>0.788736</td>\n",
       "      <td>0.613200</td>\n",
       "      <td>0.215362</td>\n",
       "      <td>0.117529</td>\n",
       "      <td>0.576769</td>\n",
       "      <td>0.308074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0.206107</td>\n",
       "      <td>0.472510</td>\n",
       "      <td>0.588723</td>\n",
       "      <td>0.137395</td>\n",
       "      <td>0.380327</td>\n",
       "      <td>0.392145</td>\n",
       "      <td>0.165503</td>\n",
       "      <td>0.170393</td>\n",
       "      <td>0.567658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117231</td>\n",
       "      <td>0.918648</td>\n",
       "      <td>0.225151</td>\n",
       "      <td>0.808565</td>\n",
       "      <td>0.739517</td>\n",
       "      <td>0.601517</td>\n",
       "      <td>0.150539</td>\n",
       "      <td>0.321008</td>\n",
       "      <td>0.673163</td>\n",
       "      <td>0.351189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>0</td>\n",
       "      <td>0.421692</td>\n",
       "      <td>0.819136</td>\n",
       "      <td>0.889120</td>\n",
       "      <td>0.274042</td>\n",
       "      <td>0.525204</td>\n",
       "      <td>0.698463</td>\n",
       "      <td>0.415842</td>\n",
       "      <td>0.387231</td>\n",
       "      <td>0.759353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285387</td>\n",
       "      <td>0.953237</td>\n",
       "      <td>0.542180</td>\n",
       "      <td>0.869079</td>\n",
       "      <td>0.713411</td>\n",
       "      <td>0.647941</td>\n",
       "      <td>0.176504</td>\n",
       "      <td>0.168747</td>\n",
       "      <td>0.529537</td>\n",
       "      <td>0.275265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>1</td>\n",
       "      <td>0.155194</td>\n",
       "      <td>0.650936</td>\n",
       "      <td>0.436741</td>\n",
       "      <td>0.127649</td>\n",
       "      <td>0.458996</td>\n",
       "      <td>0.414456</td>\n",
       "      <td>0.214664</td>\n",
       "      <td>0.116075</td>\n",
       "      <td>0.700554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098905</td>\n",
       "      <td>0.945503</td>\n",
       "      <td>0.361169</td>\n",
       "      <td>0.766231</td>\n",
       "      <td>0.652946</td>\n",
       "      <td>0.603185</td>\n",
       "      <td>0.143384</td>\n",
       "      <td>0.172315</td>\n",
       "      <td>0.522733</td>\n",
       "      <td>0.230949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>1</td>\n",
       "      <td>0.170119</td>\n",
       "      <td>0.668992</td>\n",
       "      <td>0.532885</td>\n",
       "      <td>0.141059</td>\n",
       "      <td>0.522190</td>\n",
       "      <td>0.469892</td>\n",
       "      <td>0.177975</td>\n",
       "      <td>0.141652</td>\n",
       "      <td>0.686412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208649</td>\n",
       "      <td>0.962932</td>\n",
       "      <td>0.609414</td>\n",
       "      <td>0.827691</td>\n",
       "      <td>0.850505</td>\n",
       "      <td>0.631754</td>\n",
       "      <td>0.077538</td>\n",
       "      <td>0.129992</td>\n",
       "      <td>0.617378</td>\n",
       "      <td>0.242766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0</td>\n",
       "      <td>0.316241</td>\n",
       "      <td>0.642261</td>\n",
       "      <td>0.906532</td>\n",
       "      <td>0.303791</td>\n",
       "      <td>0.403003</td>\n",
       "      <td>0.830418</td>\n",
       "      <td>0.242378</td>\n",
       "      <td>0.278751</td>\n",
       "      <td>0.764943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223300</td>\n",
       "      <td>0.917177</td>\n",
       "      <td>0.357777</td>\n",
       "      <td>0.776557</td>\n",
       "      <td>0.607103</td>\n",
       "      <td>0.595563</td>\n",
       "      <td>0.108415</td>\n",
       "      <td>0.338011</td>\n",
       "      <td>0.444886</td>\n",
       "      <td>0.302313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>1</td>\n",
       "      <td>0.356544</td>\n",
       "      <td>0.780978</td>\n",
       "      <td>0.565044</td>\n",
       "      <td>0.301901</td>\n",
       "      <td>0.552029</td>\n",
       "      <td>0.542632</td>\n",
       "      <td>0.365609</td>\n",
       "      <td>0.374958</td>\n",
       "      <td>0.796539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231661</td>\n",
       "      <td>0.958034</td>\n",
       "      <td>0.734586</td>\n",
       "      <td>0.737711</td>\n",
       "      <td>0.774517</td>\n",
       "      <td>0.587225</td>\n",
       "      <td>0.164567</td>\n",
       "      <td>0.167376</td>\n",
       "      <td>0.740924</td>\n",
       "      <td>0.223064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1430 rows × 832 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  cg22695986  cg01193368  cg22056094  cg06098368  cg08690094  \\\n",
       "0        0    0.282164    0.613574    0.670013    0.269499    0.516672   \n",
       "2        0    0.225027    0.619136    0.530781    0.219685    0.441304   \n",
       "4        1    0.289924    0.691534    0.771177    0.314880    0.489395   \n",
       "7        0    0.333657    0.675944    0.679786    0.276872    0.506406   \n",
       "10       0    0.206107    0.472510    0.588723    0.137395    0.380327   \n",
       "..     ...         ...         ...         ...         ...         ...   \n",
       "471      0    0.421692    0.819136    0.889120    0.274042    0.525204   \n",
       "472      1    0.155194    0.650936    0.436741    0.127649    0.458996   \n",
       "475      1    0.170119    0.668992    0.532885    0.141059    0.522190   \n",
       "477      0    0.316241    0.642261    0.906532    0.303791    0.403003   \n",
       "478      1    0.356544    0.780978    0.565044    0.301901    0.552029   \n",
       "\n",
       "     cg11236452  cg26916862  cg03124146  cg14770527  ...  cg11174855  \\\n",
       "0      0.459066    0.254743    0.224724    0.617466  ...    0.071795   \n",
       "2      0.504256    0.231294    0.169521    0.594098  ...    0.080001   \n",
       "4      0.531181    0.283582    0.206316    0.605732  ...    0.073612   \n",
       "7      0.511713    0.353252    0.310907    0.625395  ...    0.081113   \n",
       "10     0.392145    0.165503    0.170393    0.567658  ...    0.117231   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "471    0.698463    0.415842    0.387231    0.759353  ...    0.285387   \n",
       "472    0.414456    0.214664    0.116075    0.700554  ...    0.098905   \n",
       "475    0.469892    0.177975    0.141652    0.686412  ...    0.208649   \n",
       "477    0.830418    0.242378    0.278751    0.764943  ...    0.223300   \n",
       "478    0.542632    0.365609    0.374958    0.796539  ...    0.231661   \n",
       "\n",
       "     cg04524933  cg19418458  cg21808635  cg11359720  cg07635017  cg08641118  \\\n",
       "0      0.919869    0.448765    0.810244    0.686240    0.548766    0.113030   \n",
       "2      0.920140    0.458319    0.790811    0.622287    0.548240    0.182175   \n",
       "4      0.950902    0.275012    0.837924    0.744836    0.619499    0.121501   \n",
       "7      0.946354    0.463502    0.834654    0.788736    0.613200    0.215362   \n",
       "10     0.918648    0.225151    0.808565    0.739517    0.601517    0.150539   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "471    0.953237    0.542180    0.869079    0.713411    0.647941    0.176504   \n",
       "472    0.945503    0.361169    0.766231    0.652946    0.603185    0.143384   \n",
       "475    0.962932    0.609414    0.827691    0.850505    0.631754    0.077538   \n",
       "477    0.917177    0.357777    0.776557    0.607103    0.595563    0.108415   \n",
       "478    0.958034    0.734586    0.737711    0.774517    0.587225    0.164567   \n",
       "\n",
       "     cg09166085  cg22034735  cg17805624  \n",
       "0      0.141956    0.334091    0.243362  \n",
       "2      0.357434    0.660001    0.286040  \n",
       "4      0.141186    0.444107    0.277873  \n",
       "7      0.117529    0.576769    0.308074  \n",
       "10     0.321008    0.673163    0.351189  \n",
       "..          ...         ...         ...  \n",
       "471    0.168747    0.529537    0.275265  \n",
       "472    0.172315    0.522733    0.230949  \n",
       "475    0.129992    0.617378    0.242766  \n",
       "477    0.338011    0.444886    0.302313  \n",
       "478    0.167376    0.740924    0.223064  \n",
       "\n",
       "[1430 rows x 832 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1450 * 832\n",
    "train_data5 = pd.concat([train_data5, data_AMDTSS])\n",
    "train_data5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1072, 831), (358, 831), (1072,), (358,))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training 75% developing 25%\n",
    "x_train5, x_dev5, y_train5, y_dev5 = train_test_split(train_data5.drop(columns=['label']), train_data5['label'])\n",
    "x_train5.shape, x_dev5.shape, y_train5.shape, y_dev5.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Original Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7316751818098132"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en5 = LogisticRegression(penalty = \"elasticnet\", solver = \"saga\", l1_ratio = 0.5)\n",
    "en5.fit(x_train5, y_train5)\n",
    "y_ERISK = data_ERISK['label']\n",
    "x_ERISK = data_ERISK.loc[:,[i for i in data_AMDTSS.columns]]\n",
    "x_ERISK = x_ERISK.drop(['label'], axis = 1)\n",
    "en_auc5 = roc_auc_score(y_ERISK, en5.predict_proba(x_ERISK)[:, 1])\n",
    "en_auc5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train New Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Selection by RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', max_depth=20,\n",
       "                       min_impurity_decrease=1e-05, n_estimators=300)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid search for rf\n",
    "# The number of trees in the forest.\n",
    "n_estimators = [50, 100, 200, 300, 500]\n",
    "# The function to measure the quality of a split\n",
    "criterion = [\"gini\", \"entropy\"]\n",
    "# A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "min_impurity_decrease = [0.1, 0.000001, 0.00001]\n",
    "# The maximum depth of the tree.\n",
    "max_depth = [20, 50, 100, 500, 1000]\n",
    "\n",
    "param_distributions = dict(n_estimators = n_estimators, criterion = criterion, min_impurity_decrease = min_impurity_decrease, max_depth = max_depth)\n",
    "rf = RandomForestClassifier()\n",
    "grid_rf5 = RandomizedSearchCV(estimator = rf, param_distributions = param_distributions, scoring = \"roc_auc\",\n",
    "                          verbose = 1, n_jobs = -1) \n",
    "grid_result_rf5 = grid_rf5.fit(x_train5, y_train5) \n",
    "\n",
    "grid_result_rf5.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=RandomForestClassifier(criterion='entropy',\n",
       "                                                 max_depth=20,\n",
       "                                                 min_impurity_decrease=1e-05,\n",
       "                                                 n_estimators=300))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variable selection by random forest\n",
    "rf_selection5 = SelectFromModel(grid_result_rf5.best_estimator_)\n",
    "rf_selection5.fit(x_train5, y_train5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selected variables\n",
    "selected_feat_rf5 = x_train5.columns[(rf_selection5.get_support())]\n",
    "len(selected_feat_rf5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cg22695986</th>\n",
       "      <th>cg01193368</th>\n",
       "      <th>cg22056094</th>\n",
       "      <th>cg06098368</th>\n",
       "      <th>cg08690094</th>\n",
       "      <th>cg11236452</th>\n",
       "      <th>cg26916862</th>\n",
       "      <th>cg14770527</th>\n",
       "      <th>cg23316599</th>\n",
       "      <th>cg11108474</th>\n",
       "      <th>...</th>\n",
       "      <th>cg20999932</th>\n",
       "      <th>cg06427838</th>\n",
       "      <th>cg15694117</th>\n",
       "      <th>cg25787956</th>\n",
       "      <th>cg13665998</th>\n",
       "      <th>cg15236528</th>\n",
       "      <th>cg19418458</th>\n",
       "      <th>cg21808635</th>\n",
       "      <th>cg09166085</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.282164</td>\n",
       "      <td>0.613574</td>\n",
       "      <td>0.670013</td>\n",
       "      <td>0.269499</td>\n",
       "      <td>0.516672</td>\n",
       "      <td>0.459066</td>\n",
       "      <td>0.254743</td>\n",
       "      <td>0.617466</td>\n",
       "      <td>0.462327</td>\n",
       "      <td>0.345466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292063</td>\n",
       "      <td>0.027239</td>\n",
       "      <td>0.891736</td>\n",
       "      <td>0.562426</td>\n",
       "      <td>0.110296</td>\n",
       "      <td>0.169342</td>\n",
       "      <td>0.448765</td>\n",
       "      <td>0.810244</td>\n",
       "      <td>0.141956</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.225027</td>\n",
       "      <td>0.619136</td>\n",
       "      <td>0.530781</td>\n",
       "      <td>0.219685</td>\n",
       "      <td>0.441304</td>\n",
       "      <td>0.504256</td>\n",
       "      <td>0.231294</td>\n",
       "      <td>0.594098</td>\n",
       "      <td>0.432864</td>\n",
       "      <td>0.322264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.279418</td>\n",
       "      <td>0.062395</td>\n",
       "      <td>0.991601</td>\n",
       "      <td>0.546175</td>\n",
       "      <td>0.355201</td>\n",
       "      <td>0.178129</td>\n",
       "      <td>0.458319</td>\n",
       "      <td>0.790811</td>\n",
       "      <td>0.357434</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.289924</td>\n",
       "      <td>0.691534</td>\n",
       "      <td>0.771177</td>\n",
       "      <td>0.314880</td>\n",
       "      <td>0.489395</td>\n",
       "      <td>0.531181</td>\n",
       "      <td>0.283582</td>\n",
       "      <td>0.605732</td>\n",
       "      <td>0.452428</td>\n",
       "      <td>0.369445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294543</td>\n",
       "      <td>0.032160</td>\n",
       "      <td>0.985825</td>\n",
       "      <td>0.557975</td>\n",
       "      <td>0.082696</td>\n",
       "      <td>0.190076</td>\n",
       "      <td>0.275012</td>\n",
       "      <td>0.837924</td>\n",
       "      <td>0.141186</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.333657</td>\n",
       "      <td>0.675944</td>\n",
       "      <td>0.679786</td>\n",
       "      <td>0.276872</td>\n",
       "      <td>0.506406</td>\n",
       "      <td>0.511713</td>\n",
       "      <td>0.353252</td>\n",
       "      <td>0.625395</td>\n",
       "      <td>0.477888</td>\n",
       "      <td>0.363694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282708</td>\n",
       "      <td>0.011340</td>\n",
       "      <td>0.979381</td>\n",
       "      <td>0.533838</td>\n",
       "      <td>0.084307</td>\n",
       "      <td>0.150414</td>\n",
       "      <td>0.463502</td>\n",
       "      <td>0.834654</td>\n",
       "      <td>0.117529</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.206107</td>\n",
       "      <td>0.472510</td>\n",
       "      <td>0.588723</td>\n",
       "      <td>0.137395</td>\n",
       "      <td>0.380327</td>\n",
       "      <td>0.392145</td>\n",
       "      <td>0.165503</td>\n",
       "      <td>0.567658</td>\n",
       "      <td>0.282244</td>\n",
       "      <td>0.324882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375531</td>\n",
       "      <td>0.038941</td>\n",
       "      <td>0.977677</td>\n",
       "      <td>0.553008</td>\n",
       "      <td>0.298719</td>\n",
       "      <td>0.246285</td>\n",
       "      <td>0.225151</td>\n",
       "      <td>0.808565</td>\n",
       "      <td>0.321008</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>0.421692</td>\n",
       "      <td>0.819136</td>\n",
       "      <td>0.889120</td>\n",
       "      <td>0.274042</td>\n",
       "      <td>0.525204</td>\n",
       "      <td>0.698463</td>\n",
       "      <td>0.415842</td>\n",
       "      <td>0.759353</td>\n",
       "      <td>0.573460</td>\n",
       "      <td>0.401122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223322</td>\n",
       "      <td>0.037285</td>\n",
       "      <td>0.982111</td>\n",
       "      <td>0.641851</td>\n",
       "      <td>0.088521</td>\n",
       "      <td>0.128186</td>\n",
       "      <td>0.542180</td>\n",
       "      <td>0.869079</td>\n",
       "      <td>0.168747</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>0.155194</td>\n",
       "      <td>0.650936</td>\n",
       "      <td>0.436741</td>\n",
       "      <td>0.127649</td>\n",
       "      <td>0.458996</td>\n",
       "      <td>0.414456</td>\n",
       "      <td>0.214664</td>\n",
       "      <td>0.700554</td>\n",
       "      <td>0.475778</td>\n",
       "      <td>0.298102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220185</td>\n",
       "      <td>0.033304</td>\n",
       "      <td>0.945254</td>\n",
       "      <td>0.513343</td>\n",
       "      <td>0.091538</td>\n",
       "      <td>0.134853</td>\n",
       "      <td>0.361169</td>\n",
       "      <td>0.766231</td>\n",
       "      <td>0.172315</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0.170119</td>\n",
       "      <td>0.668992</td>\n",
       "      <td>0.532885</td>\n",
       "      <td>0.141059</td>\n",
       "      <td>0.522190</td>\n",
       "      <td>0.469892</td>\n",
       "      <td>0.177975</td>\n",
       "      <td>0.686412</td>\n",
       "      <td>0.454601</td>\n",
       "      <td>0.322749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362009</td>\n",
       "      <td>0.207561</td>\n",
       "      <td>0.997431</td>\n",
       "      <td>0.595221</td>\n",
       "      <td>0.078325</td>\n",
       "      <td>0.146198</td>\n",
       "      <td>0.609414</td>\n",
       "      <td>0.827691</td>\n",
       "      <td>0.129992</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0.316241</td>\n",
       "      <td>0.642261</td>\n",
       "      <td>0.906532</td>\n",
       "      <td>0.303791</td>\n",
       "      <td>0.403003</td>\n",
       "      <td>0.830418</td>\n",
       "      <td>0.242378</td>\n",
       "      <td>0.764943</td>\n",
       "      <td>0.376560</td>\n",
       "      <td>0.445176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.337030</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.901474</td>\n",
       "      <td>0.579367</td>\n",
       "      <td>0.329829</td>\n",
       "      <td>0.130743</td>\n",
       "      <td>0.357777</td>\n",
       "      <td>0.776557</td>\n",
       "      <td>0.338011</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0.356544</td>\n",
       "      <td>0.780978</td>\n",
       "      <td>0.565044</td>\n",
       "      <td>0.301901</td>\n",
       "      <td>0.552029</td>\n",
       "      <td>0.542632</td>\n",
       "      <td>0.365609</td>\n",
       "      <td>0.796539</td>\n",
       "      <td>0.529338</td>\n",
       "      <td>0.463919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245911</td>\n",
       "      <td>0.012417</td>\n",
       "      <td>0.982211</td>\n",
       "      <td>0.653623</td>\n",
       "      <td>0.072343</td>\n",
       "      <td>0.173264</td>\n",
       "      <td>0.734586</td>\n",
       "      <td>0.737711</td>\n",
       "      <td>0.167376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1430 rows × 288 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cg22695986  cg01193368  cg22056094  cg06098368  cg08690094  cg11236452  \\\n",
       "0      0.282164    0.613574    0.670013    0.269499    0.516672    0.459066   \n",
       "2      0.225027    0.619136    0.530781    0.219685    0.441304    0.504256   \n",
       "4      0.289924    0.691534    0.771177    0.314880    0.489395    0.531181   \n",
       "7      0.333657    0.675944    0.679786    0.276872    0.506406    0.511713   \n",
       "10     0.206107    0.472510    0.588723    0.137395    0.380327    0.392145   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "471    0.421692    0.819136    0.889120    0.274042    0.525204    0.698463   \n",
       "472    0.155194    0.650936    0.436741    0.127649    0.458996    0.414456   \n",
       "475    0.170119    0.668992    0.532885    0.141059    0.522190    0.469892   \n",
       "477    0.316241    0.642261    0.906532    0.303791    0.403003    0.830418   \n",
       "478    0.356544    0.780978    0.565044    0.301901    0.552029    0.542632   \n",
       "\n",
       "     cg26916862  cg14770527  cg23316599  cg11108474  ...  cg20999932  \\\n",
       "0      0.254743    0.617466    0.462327    0.345466  ...    0.292063   \n",
       "2      0.231294    0.594098    0.432864    0.322264  ...    0.279418   \n",
       "4      0.283582    0.605732    0.452428    0.369445  ...    0.294543   \n",
       "7      0.353252    0.625395    0.477888    0.363694  ...    0.282708   \n",
       "10     0.165503    0.567658    0.282244    0.324882  ...    0.375531   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "471    0.415842    0.759353    0.573460    0.401122  ...    0.223322   \n",
       "472    0.214664    0.700554    0.475778    0.298102  ...    0.220185   \n",
       "475    0.177975    0.686412    0.454601    0.322749  ...    0.362009   \n",
       "477    0.242378    0.764943    0.376560    0.445176  ...    0.337030   \n",
       "478    0.365609    0.796539    0.529338    0.463919  ...    0.245911   \n",
       "\n",
       "     cg06427838  cg15694117  cg25787956  cg13665998  cg15236528  cg19418458  \\\n",
       "0      0.027239    0.891736    0.562426    0.110296    0.169342    0.448765   \n",
       "2      0.062395    0.991601    0.546175    0.355201    0.178129    0.458319   \n",
       "4      0.032160    0.985825    0.557975    0.082696    0.190076    0.275012   \n",
       "7      0.011340    0.979381    0.533838    0.084307    0.150414    0.463502   \n",
       "10     0.038941    0.977677    0.553008    0.298719    0.246285    0.225151   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "471    0.037285    0.982111    0.641851    0.088521    0.128186    0.542180   \n",
       "472    0.033304    0.945254    0.513343    0.091538    0.134853    0.361169   \n",
       "475    0.207561    0.997431    0.595221    0.078325    0.146198    0.609414   \n",
       "477    0.070588    0.901474    0.579367    0.329829    0.130743    0.357777   \n",
       "478    0.012417    0.982211    0.653623    0.072343    0.173264    0.734586   \n",
       "\n",
       "     cg21808635  cg09166085  label  \n",
       "0      0.810244    0.141956      0  \n",
       "2      0.790811    0.357434      0  \n",
       "4      0.837924    0.141186      1  \n",
       "7      0.834654    0.117529      0  \n",
       "10     0.808565    0.321008      0  \n",
       "..          ...         ...    ...  \n",
       "471    0.869079    0.168747      0  \n",
       "472    0.766231    0.172315      1  \n",
       "475    0.827691    0.129992      1  \n",
       "477    0.776557    0.338011      0  \n",
       "478    0.737711    0.167376      1  \n",
       "\n",
       "[1430 rows x 288 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_selected_train_data5 = train_data5.loc[:,[i for i in selected_feat_rf5]]\n",
    "rf_selected_train_data5['label'] = train_data5['label'] \n",
    "rf_selected_train_data5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1072, 287), (358, 287), (1072,), (358,))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training 75% developing 25%\n",
    "x_train_rf5, x_dev_rf5, y_train_rf5, y_dev_rf5 = train_test_split(rf_selected_train_data5.drop(columns=['label']), rf_selected_train_data5['label'])\n",
    "x_train_rf5.shape, x_dev_rf5.shape, y_train_rf5.shape, y_dev_rf5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7901511376847701"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_selected5 = grid_result_rf5.best_estimator_\n",
    "rf_selected5.fit(x_train_rf5, y_train_rf5)\n",
    "rf_selected_auc5 = roc_auc_score(y_dev_rf5, rf_selected5.predict_proba(x_dev_rf5)[:, 1])\n",
    "rf_selected_auc5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ERISK_rf_selected = data_ERISK['label']\n",
    "x_ERISK_rf_selected = data_ERISK.loc[:,[i for i in selected_feat_rf5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7183491745681058"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_selected_auc_ERISK = roc_auc_score(y_ERISK_rf_selected, rf_selected5.predict_proba(x_ERISK_rf_selected)[:, 1])\n",
    "rf_selected_auc_ERISK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7101456779895057"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking_rf_ERISK = get_stacking()\n",
    "stacking_rf_ERISK.fit(x_train_rf5, y_train_rf5)\n",
    "stacking_rf_ERISK_auc = roc_auc_score(y_ERISK_rf_selected, stacking_rf_ERISK.predict_proba(x_ERISK_rf_selected)[:, 1])\n",
    "stacking_rf_ERISK_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7205479609684249"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_rf_ERISK = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gb', clf3), ('svc', clf4)], voting='soft')\n",
    "voting_rf_ERISK.fit(x_train_rf5, y_train_rf5)\n",
    "voting_rf_ERISK_auc = roc_auc_score(y_ERISK_rf_selected, voting_rf_ERISK.predict_proba(x_ERISK_rf_selected)[:, 1])\n",
    "voting_rf_ERISK_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Selection by LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 450, in _check_solver\n",
      "    raise ValueError(\"Only 'saga' solver supports elasticnet penalty,\"\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 450, in _check_solver\n",
      "    raise ValueError(\"Only 'saga' solver supports elasticnet penalty,\"\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 450, in _check_solver\n",
      "    raise ValueError(\"Only 'saga' solver supports elasticnet penalty,\"\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 450, in _check_solver\n",
      "    raise ValueError(\"Only 'saga' solver supports elasticnet penalty,\"\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 450, in _check_solver\n",
      "    raise ValueError(\"Only 'saga' solver supports elasticnet penalty,\"\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.79237581        nan        nan 0.7905674\n",
      " 0.76198959 0.79388986        nan        nan]\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=30, solver='saga', tol=0.001)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid search for lr\n",
    "penalty = [\"l1\", \"l2\", \"elasticnet\"]\n",
    "# Tolerance for stopping criteria.\n",
    "tol = [0.00001, 0.001, 0.0000001]\n",
    "# Inverse of regularization strength\n",
    "C = [0,1, 0.5, 1, 10, 20, 30, 50, 100]\n",
    "# Algorithm to use in the optimization problem\n",
    "solver = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "\n",
    "\n",
    "param_distributions = dict(penalty = penalty, tol = tol, C = C, solver = solver)\n",
    "lr = LogisticRegression()\n",
    "grid_lr5 = RandomizedSearchCV(estimator = lr, param_distributions = param_distributions, scoring = \"roc_auc\",\n",
    "                          verbose = 1, n_jobs = -1) \n",
    "grid_result_lr5 = grid_lr5.fit(x_train5, y_train5) \n",
    "\n",
    "grid_result_lr5.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=LogisticRegression(C=30, solver='saga', tol=0.001))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_selection5 = SelectFromModel(grid_result_lr5.best_estimator_)\n",
    "lr_selection5.fit(x_train5, y_train5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_feat_lr5 = x_train5.columns[(lr_selection5.get_support())]\n",
    "len(selected_feat_lr5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cg06098368</th>\n",
       "      <th>cg08690094</th>\n",
       "      <th>cg03124146</th>\n",
       "      <th>cg10933186</th>\n",
       "      <th>cg11108474</th>\n",
       "      <th>cg04838249</th>\n",
       "      <th>cg11430197</th>\n",
       "      <th>cg15509177</th>\n",
       "      <th>cg01418385</th>\n",
       "      <th>cg03203197</th>\n",
       "      <th>...</th>\n",
       "      <th>cg04600077</th>\n",
       "      <th>cg09009380</th>\n",
       "      <th>cg13665998</th>\n",
       "      <th>cg07903023</th>\n",
       "      <th>cg19418458</th>\n",
       "      <th>cg21808635</th>\n",
       "      <th>cg08641118</th>\n",
       "      <th>cg09166085</th>\n",
       "      <th>cg17805624</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.269499</td>\n",
       "      <td>0.516672</td>\n",
       "      <td>0.224724</td>\n",
       "      <td>0.796146</td>\n",
       "      <td>0.345466</td>\n",
       "      <td>0.812046</td>\n",
       "      <td>0.313824</td>\n",
       "      <td>0.418919</td>\n",
       "      <td>0.351138</td>\n",
       "      <td>0.594579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.636356</td>\n",
       "      <td>0.323738</td>\n",
       "      <td>0.110296</td>\n",
       "      <td>0.560400</td>\n",
       "      <td>0.448765</td>\n",
       "      <td>0.810244</td>\n",
       "      <td>0.113030</td>\n",
       "      <td>0.141956</td>\n",
       "      <td>0.243362</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.219685</td>\n",
       "      <td>0.441304</td>\n",
       "      <td>0.169521</td>\n",
       "      <td>0.737150</td>\n",
       "      <td>0.322264</td>\n",
       "      <td>0.539041</td>\n",
       "      <td>0.188154</td>\n",
       "      <td>0.407961</td>\n",
       "      <td>0.269543</td>\n",
       "      <td>0.476736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.724144</td>\n",
       "      <td>0.357260</td>\n",
       "      <td>0.355201</td>\n",
       "      <td>0.627009</td>\n",
       "      <td>0.458319</td>\n",
       "      <td>0.790811</td>\n",
       "      <td>0.182175</td>\n",
       "      <td>0.357434</td>\n",
       "      <td>0.286040</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.314880</td>\n",
       "      <td>0.489395</td>\n",
       "      <td>0.206316</td>\n",
       "      <td>0.822910</td>\n",
       "      <td>0.369445</td>\n",
       "      <td>0.724093</td>\n",
       "      <td>0.286106</td>\n",
       "      <td>0.481554</td>\n",
       "      <td>0.353168</td>\n",
       "      <td>0.484432</td>\n",
       "      <td>...</td>\n",
       "      <td>0.728159</td>\n",
       "      <td>0.368134</td>\n",
       "      <td>0.082696</td>\n",
       "      <td>0.635566</td>\n",
       "      <td>0.275012</td>\n",
       "      <td>0.837924</td>\n",
       "      <td>0.121501</td>\n",
       "      <td>0.141186</td>\n",
       "      <td>0.277873</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.276872</td>\n",
       "      <td>0.506406</td>\n",
       "      <td>0.310907</td>\n",
       "      <td>0.845854</td>\n",
       "      <td>0.363694</td>\n",
       "      <td>0.783814</td>\n",
       "      <td>0.372262</td>\n",
       "      <td>0.493866</td>\n",
       "      <td>0.302054</td>\n",
       "      <td>0.487117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.741501</td>\n",
       "      <td>0.394190</td>\n",
       "      <td>0.084307</td>\n",
       "      <td>0.630104</td>\n",
       "      <td>0.463502</td>\n",
       "      <td>0.834654</td>\n",
       "      <td>0.215362</td>\n",
       "      <td>0.117529</td>\n",
       "      <td>0.308074</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.137395</td>\n",
       "      <td>0.380327</td>\n",
       "      <td>0.170393</td>\n",
       "      <td>0.737851</td>\n",
       "      <td>0.324882</td>\n",
       "      <td>0.581293</td>\n",
       "      <td>0.305695</td>\n",
       "      <td>0.416754</td>\n",
       "      <td>0.264194</td>\n",
       "      <td>0.419088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.783412</td>\n",
       "      <td>0.375530</td>\n",
       "      <td>0.298719</td>\n",
       "      <td>0.619235</td>\n",
       "      <td>0.225151</td>\n",
       "      <td>0.808565</td>\n",
       "      <td>0.150539</td>\n",
       "      <td>0.321008</td>\n",
       "      <td>0.351189</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>0.274042</td>\n",
       "      <td>0.525204</td>\n",
       "      <td>0.387231</td>\n",
       "      <td>0.799908</td>\n",
       "      <td>0.401122</td>\n",
       "      <td>0.805374</td>\n",
       "      <td>0.427378</td>\n",
       "      <td>0.505403</td>\n",
       "      <td>0.493238</td>\n",
       "      <td>0.402383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718748</td>\n",
       "      <td>0.389640</td>\n",
       "      <td>0.088521</td>\n",
       "      <td>0.699565</td>\n",
       "      <td>0.542180</td>\n",
       "      <td>0.869079</td>\n",
       "      <td>0.176504</td>\n",
       "      <td>0.168747</td>\n",
       "      <td>0.275265</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>0.127649</td>\n",
       "      <td>0.458996</td>\n",
       "      <td>0.116075</td>\n",
       "      <td>0.679567</td>\n",
       "      <td>0.298102</td>\n",
       "      <td>0.670181</td>\n",
       "      <td>0.358344</td>\n",
       "      <td>0.404168</td>\n",
       "      <td>0.257487</td>\n",
       "      <td>0.525479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712071</td>\n",
       "      <td>0.337200</td>\n",
       "      <td>0.091538</td>\n",
       "      <td>0.598367</td>\n",
       "      <td>0.361169</td>\n",
       "      <td>0.766231</td>\n",
       "      <td>0.143384</td>\n",
       "      <td>0.172315</td>\n",
       "      <td>0.230949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0.141059</td>\n",
       "      <td>0.522190</td>\n",
       "      <td>0.141652</td>\n",
       "      <td>0.703902</td>\n",
       "      <td>0.322749</td>\n",
       "      <td>0.624104</td>\n",
       "      <td>0.327654</td>\n",
       "      <td>0.492028</td>\n",
       "      <td>0.322215</td>\n",
       "      <td>0.444669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.748548</td>\n",
       "      <td>0.410973</td>\n",
       "      <td>0.078325</td>\n",
       "      <td>0.744434</td>\n",
       "      <td>0.609414</td>\n",
       "      <td>0.827691</td>\n",
       "      <td>0.077538</td>\n",
       "      <td>0.129992</td>\n",
       "      <td>0.242766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0.303791</td>\n",
       "      <td>0.403003</td>\n",
       "      <td>0.278751</td>\n",
       "      <td>0.921208</td>\n",
       "      <td>0.445176</td>\n",
       "      <td>0.663237</td>\n",
       "      <td>0.503726</td>\n",
       "      <td>0.609263</td>\n",
       "      <td>0.517012</td>\n",
       "      <td>0.516163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.830315</td>\n",
       "      <td>0.340245</td>\n",
       "      <td>0.329829</td>\n",
       "      <td>0.637866</td>\n",
       "      <td>0.357777</td>\n",
       "      <td>0.776557</td>\n",
       "      <td>0.108415</td>\n",
       "      <td>0.338011</td>\n",
       "      <td>0.302313</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0.301901</td>\n",
       "      <td>0.552029</td>\n",
       "      <td>0.374958</td>\n",
       "      <td>0.782587</td>\n",
       "      <td>0.463919</td>\n",
       "      <td>0.780577</td>\n",
       "      <td>0.468265</td>\n",
       "      <td>0.658174</td>\n",
       "      <td>0.200803</td>\n",
       "      <td>0.424240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.804106</td>\n",
       "      <td>0.378964</td>\n",
       "      <td>0.072343</td>\n",
       "      <td>0.654000</td>\n",
       "      <td>0.734586</td>\n",
       "      <td>0.737711</td>\n",
       "      <td>0.164567</td>\n",
       "      <td>0.167376</td>\n",
       "      <td>0.223064</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1430 rows × 344 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cg06098368  cg08690094  cg03124146  cg10933186  cg11108474  cg04838249  \\\n",
       "0      0.269499    0.516672    0.224724    0.796146    0.345466    0.812046   \n",
       "2      0.219685    0.441304    0.169521    0.737150    0.322264    0.539041   \n",
       "4      0.314880    0.489395    0.206316    0.822910    0.369445    0.724093   \n",
       "7      0.276872    0.506406    0.310907    0.845854    0.363694    0.783814   \n",
       "10     0.137395    0.380327    0.170393    0.737851    0.324882    0.581293   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "471    0.274042    0.525204    0.387231    0.799908    0.401122    0.805374   \n",
       "472    0.127649    0.458996    0.116075    0.679567    0.298102    0.670181   \n",
       "475    0.141059    0.522190    0.141652    0.703902    0.322749    0.624104   \n",
       "477    0.303791    0.403003    0.278751    0.921208    0.445176    0.663237   \n",
       "478    0.301901    0.552029    0.374958    0.782587    0.463919    0.780577   \n",
       "\n",
       "     cg11430197  cg15509177  cg01418385  cg03203197  ...  cg04600077  \\\n",
       "0      0.313824    0.418919    0.351138    0.594579  ...    0.636356   \n",
       "2      0.188154    0.407961    0.269543    0.476736  ...    0.724144   \n",
       "4      0.286106    0.481554    0.353168    0.484432  ...    0.728159   \n",
       "7      0.372262    0.493866    0.302054    0.487117  ...    0.741501   \n",
       "10     0.305695    0.416754    0.264194    0.419088  ...    0.783412   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "471    0.427378    0.505403    0.493238    0.402383  ...    0.718748   \n",
       "472    0.358344    0.404168    0.257487    0.525479  ...    0.712071   \n",
       "475    0.327654    0.492028    0.322215    0.444669  ...    0.748548   \n",
       "477    0.503726    0.609263    0.517012    0.516163  ...    0.830315   \n",
       "478    0.468265    0.658174    0.200803    0.424240  ...    0.804106   \n",
       "\n",
       "     cg09009380  cg13665998  cg07903023  cg19418458  cg21808635  cg08641118  \\\n",
       "0      0.323738    0.110296    0.560400    0.448765    0.810244    0.113030   \n",
       "2      0.357260    0.355201    0.627009    0.458319    0.790811    0.182175   \n",
       "4      0.368134    0.082696    0.635566    0.275012    0.837924    0.121501   \n",
       "7      0.394190    0.084307    0.630104    0.463502    0.834654    0.215362   \n",
       "10     0.375530    0.298719    0.619235    0.225151    0.808565    0.150539   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "471    0.389640    0.088521    0.699565    0.542180    0.869079    0.176504   \n",
       "472    0.337200    0.091538    0.598367    0.361169    0.766231    0.143384   \n",
       "475    0.410973    0.078325    0.744434    0.609414    0.827691    0.077538   \n",
       "477    0.340245    0.329829    0.637866    0.357777    0.776557    0.108415   \n",
       "478    0.378964    0.072343    0.654000    0.734586    0.737711    0.164567   \n",
       "\n",
       "     cg09166085  cg17805624  label  \n",
       "0      0.141956    0.243362      0  \n",
       "2      0.357434    0.286040      0  \n",
       "4      0.141186    0.277873      1  \n",
       "7      0.117529    0.308074      0  \n",
       "10     0.321008    0.351189      0  \n",
       "..          ...         ...    ...  \n",
       "471    0.168747    0.275265      0  \n",
       "472    0.172315    0.230949      1  \n",
       "475    0.129992    0.242766      1  \n",
       "477    0.338011    0.302313      0  \n",
       "478    0.167376    0.223064      1  \n",
       "\n",
       "[1430 rows x 344 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_selected_train_data5 = train_data5.loc[:,[i for i in selected_feat_lr5]]\n",
    "lr_selected_train_data5['label'] = train_data5['label'] \n",
    "lr_selected_train_data5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1072, 343), (358, 343), (1072,), (358,))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training 75% developing 25%\n",
    "x_train_lr5, x_dev_lr5, y_train_lr5, y_dev_lr5 = train_test_split(lr_selected_train_data5.drop(columns=['label']), lr_selected_train_data5['label'])\n",
    "x_train_lr5.shape, x_dev_lr5.shape, y_train_lr5.shape, y_dev_lr5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8351923076923077"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_selected5 = grid_result_lr5.best_estimator_\n",
    "lr_selected5.fit(x_train_lr5, y_train_lr5)\n",
    "lr_selected_auc5 = roc_auc_score(y_dev_lr5, lr_selected5.predict_proba(x_dev_lr5)[:, 1])\n",
    "lr_selected_auc5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ERISK_lr_selected = data_ERISK['label']\n",
    "x_ERISK_lr_selected = data_ERISK.loc[:,[i for i in selected_feat_lr5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7152796956028109"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_selected_auc_ERISK = roc_auc_score(y_ERISK_lr_selected, lr_selected5.predict_proba(x_ERISK_lr_selected)[:, 1])\n",
    "lr_selected_auc_ERISK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7148385958452238"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking_lr_ERISK = get_stacking()\n",
    "stacking_lr_ERISK.fit(x_train_lr5, y_train_lr5)\n",
    "stacking_lr_ERISK_auc = roc_auc_score(y_ERISK_lr_selected, stacking_lr_ERISK.predict_proba(x_ERISK_lr_selected)[:, 1])\n",
    "stacking_lr_ERISK_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7158665500629047"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_lr_ERISK = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gb', clf3), ('svc', clf4)], voting='soft')\n",
    "voting_lr_ERISK.fit(x_train_lr5, y_train_lr5)\n",
    "voting_lr_ERISK_auc = roc_auc_score(y_ERISK_lr_selected, voting_lr_ERISK.predict_proba(x_ERISK_lr_selected)[:, 1])\n",
    "voting_lr_ERISK_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b8bdd4e700647ba2b08c59e5df8b7da1dcf50a218bcd4c1bcd9b3dc92e8788e5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
